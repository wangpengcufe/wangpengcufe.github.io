<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>机器学习 (六) 决策树 | 机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="机器学习,决策树 PDF电子书下载,电子书，PDF下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:title" content="机器学习  (六) 决策树"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-ml-ml6" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-09-02T09:20:39.000Z"><a href="/machinelearning/ml-ml6/">2019-09-02</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="p-name title" itemprop="headline name">机器学习 (六) 决策树</h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml6-1.png" alt="决策树"></p><a id="more"></a><h1 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a><strong>一、概念</strong></h1><p>决策树及其集合是分类和回归的机器学习任务的流行方法。 决策树被广泛使用，因为它们易于解释，处理分类特征，扩展到多类分类设置，不需要特征缩放，并且能够捕获非线性和特征交互。 诸如随机森林和增强的树集合算法是分类和回归任务的最佳表现者。</p><p>决策树（decision tree）是一种基本的分类与回归方法，这里主要介绍用于分类的决策树。决策树模式呈树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。学习时利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。</p><h1 id="二、基本原理"><a href="#二、基本原理" class="headerlink" title="二、基本原理"></a><strong>二、基本原理</strong></h1><p>决策树学习通常包含三个方面：特征选择、决策树生成和决策树剪枝。决策树学习思想主要来源于：Quinlan在1986年提出的ID算法、在1993年提出的C4.5算法和Breiman等人在1984年提出的CART算法。</p><h2 id="2-1、特征选择"><a href="#2-1、特征选择" class="headerlink" title="2.1、特征选择"></a><strong>2.1、特征选择</strong></h2><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。通常特征选择的准则是信息增益（或信息增益比、基尼指数等），每次计算每个特征的信息增益，并比较它们的大小，选择信息增益最大（信息增益比最大、基尼指数最小）的特征。</p><ul><li>那么问题来了：怎么找到这样的最优划分特征呢？如何来衡量最优？</li></ul><p>什么是最优特征，通俗的理解是对训练数据具有很强的分类能力的特征，比如要看相亲的男女是否合适，他们的年龄差这个特征就远比他们的出生地重要，因为年龄差能更好得对相亲是否成功这个分类问题具有更强的分类能力。</p><p>但是计算机并不知道哪些特征是最优的，因此，就要找一个衡量特征是不是最优的指标，使得决策树在每一个分支上的数据尽可能属于同一类别的数据，即样本纯度最高。</p><ul><li>我们用熵来衡量样本集合的纯度。</li></ul><p><strong>熵</strong></p><p>这是概率统计与信息论中的一个概念，定义为：</p><p><img src="http://wangpengcufe.com/ml6-2.png" alt="熵"></p><p>其中p(x)=pi表示随机变量X发生概率。</p><p>我们可以从两个角度理解这个概念。</p><p>第一就是不确定度的一个度量，我们的目标是为了找到一颗树，使得每个分枝上都代表一个分类，也就是说我们希望这个分枝上的不确定性最小，即确定性最大，也就是这些数据都是同一个类别的。熵越小，代表这些数据是同一类别的越多。</p><p>第二个角度就是从纯度理解。因为熵是不确定度的度量，如果他们不确定度越小，意味着这个群体的差异很小，也就是它的纯度很高。比如，在明大的某富翁聚会上，来的人大多是某总，普通工薪白领就会很少，如果新来了一个刘总，他是富翁的确定性就很大，不确定性就很小，同时这个群体的纯度很大。总结来说就是熵越小，纯度越大，而我们希望的就是纯度越大越好。</p><p><strong>信息增益</strong></p><p>我们用信息熵来衡量一个分支的纯度，以及哪个特征是最优的特征<br>在决策树学习中应用信息增益准则来选择最优特征。信息增益定义如下:</p><p><img src="http://wangpengcufe.com/ml6-3.png" alt="信息增益"></p><p>特征A对训练数据集D的信息增益g(D,A) 等于D的不确定度H（D） 减去给定条件A下D的不确定度H（D|A），可以理解为由于特征A使得对数据集D的分类的不确定性减少的程度,信息增益大的特征具有更强的分类能力。</p><p><strong>信息增益率</strong></p><p>信息增益选择特征倾向于选择取值较多的特征，假设某个属性存在大量的不同值，决策树在选择属性时，将偏向于选择该属性，但这肯定是不正确（导致过拟合）的。因此有必要使用一种更好的方法，那就是信息增益率（Info Gain Ratio）来矫正这一问题。<br>其公式为:</p><p><img src="http://wangpengcufe.com/ml6-4.png" alt="信息增益率"></p><p>其中</p><p><img src="http://wangpengcufe.com/ml6-5.png" alt="HA(D)"></p><p>,n为特征A取值的个数</p><p><strong>基尼指数</strong></p><p>概率分布的基尼指数定义为</p><p><img src="http://wangpengcufe.com/ml6-6.png" alt="基尼指数"></p><p>其中K表示分类问题中类别的个数</p><h2 id="2-2、决策树的生成"><a href="#2-2、决策树的生成" class="headerlink" title="2.2、决策树的生成"></a><strong>2.2、决策树的生成</strong></h2><p>从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增均很小或没有特征可以选择为止，最后得到一个决策树。</p><p>决策树需要有停止条件来终止其生长的过程。一般来说最低的条件是：当该节点下面的所有记录都属于同一类，或者当所有的记录属性都具有相同的值时。这两种条件是停止决策树的必要条件，也是最低的条件。在实际运用中一般希望决策树提前停止生长，限定叶节点包含的最低数据量，以防止由于过度生长造成的过拟合问题。</p><h2 id="2-3、决策树的剪枝"><a href="#2-3、决策树的剪枝" class="headerlink" title="2.3、决策树的剪枝"></a><strong>2.3、决策树的剪枝</strong></h2><p>决策树生成只考虑了通过信息增益或信息增益比来对训练数据更好的拟合，但没有考虑到如果模型过于复杂，会导致过拟合的产生。而剪枝就是缓解过拟合的一种手段，单纯的决策树生成学习局部的模型，而剪枝后的决策树会生成学习整体的模型，因为剪枝的过程中，通过最小化损失函数，可以平衡决策树的对训练数据的拟合程度和整个模型的复杂度。</p><p>决策树的损失函数定义如下：</p><p><img src="http://wangpengcufe.com/ml6-7.png" alt="损失函数"></p><p>其中，</p><p><img src="http://wangpengcufe.com/ml6-8.png" alt="Ht(T)"></p><h1 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a><strong>三、代码实现</strong></h1><p>我们以iris数据集（<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data）" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data）</a> 为例进行分析。iris以鸢尾花的特征作为数据来源，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性，是在数据挖掘、数据分类中非常常用的测试集、训练集。</p><h2 id="3-1、读取数据"><a href="#3-1、读取数据" class="headerlink" title="3.1、读取数据"></a><strong>3.1、读取数据</strong></h2><p>首先，读取文本文件；然后，通过map将每行的数据用“,”隔开，在我们的数据集中，每行被分成了5部分，前4部分是鸢尾花的4个特征，最后一部分是鸢尾花的分类。把这里我们用LabeledPoint来存储标签列和特征列。LabeledPoint在监督学习中常用来存储标签和特征，其中要求标签的类型是double，特征的类型是Vector。所以，我们把莺尾花的分类进行了一下改变，”Iris-setosa”对应分类0，”Iris-versicolor”对应分类1，其余对应分类2；然后获取莺尾花的4个特征，存储在Vector中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.mllib.linalg.Vectors;</span><br><span class="line">import org.apache.spark.mllib.regression.LabeledPoint;</span><br><span class="line">import org.apache.spark.mllib.tree.DecisionTree;</span><br><span class="line">import org.apache.spark.mllib.tree.model.DecisionTreeModel;</span><br><span class="line">import scala.Tuple2;</span><br><span class="line">SparkConf conf = new  SparkConf().setAppName(&quot;decisionTree&quot;).setMaster(&quot;local&quot;);</span><br><span class="line">JavaSparkContext sc = new  JavaSparkContext(conf);</span><br><span class="line">        </span><br><span class="line">/**</span><br><span class="line">   * 读取数据</span><br><span class="line">   * 转化成 LabeledPoint类型</span><br><span class="line">*/</span><br><span class="line">JavaRDD&lt;String&gt; source =  sc.textFile(&quot;data/mllib/iris.data&quot;);</span><br><span class="line">JavaRDD&lt;LabeledPoint&gt; data =  source.map(line-&gt;&#123;</span><br><span class="line">    String[] parts = line.split(&quot;,&quot;);</span><br><span class="line">    double label = 0.0;</span><br><span class="line">    if(parts[4].equals(&quot;Iris-setosa&quot;)) &#123;</span><br><span class="line">                label = 0.0;</span><br><span class="line">     &#125;else  if(parts[4].equals(&quot;Iris-versicolor&quot;)) &#123;</span><br><span class="line">                label = 1.0;</span><br><span class="line">     &#125;else &#123;</span><br><span class="line">                label = 2.0;</span><br><span class="line">     &#125;</span><br><span class="line">     return new  LabeledPoint(label,Vectors.dense(Double.parseDouble(parts[0]),</span><br><span class="line">                              Double.parseDouble(parts[1]),</span><br><span class="line">                              Double.parseDouble(parts[2]),</span><br><span class="line">                              Double.parseDouble(parts[3])));</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="3-2、划分数据集"><a href="#3-2、划分数据集" class="headerlink" title="3.2、划分数据集"></a><strong>3.2、划分数据集</strong></h2><p>接下来，首先进行数据集的划分，这里划分70%的训练集和30%的测试集：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;LabeledPoint&gt;[] splits =  data.randomSplit(new double[] &#123;0.7,0.3&#125;);</span><br><span class="line">JavaRDD&lt;LabeledPoint&gt; trainingData =  splits[0];</span><br><span class="line">JavaRDD&lt;LabeledPoint&gt; testData =  splits[1];</span><br></pre></td></tr></table></figure><h2 id="3-3、构建模型"><a href="#3-3、构建模型" class="headerlink" title="3.3、构建模型"></a><strong>3.3、构建模型</strong></h2><p>调用决策树的trainClassifier方法构建决策树模型，设置参数，比如分类数、信息增益的选择、树的最大深度等：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int numClasses = 3;//分类数</span><br><span class="line">int maxDepth = 5; //树的最大深度</span><br><span class="line">int maxBins = 30;//离散连续特征时使用的bin数。增加maxBins允许算法考虑更多的分割候选者并进行细粒度的分割决策。</span><br><span class="line">String impurity = &quot;gini&quot;;</span><br><span class="line">Map&lt;Integer,Integer&gt;  categoricalFeaturesInfo = new  HashMap&lt;Integer,Integer&gt;();//空的categoricalFeaturesInfo表示所有功能都是连续的。</span><br><span class="line">DecisionTreeModel model =  DecisionTree.trainClassifier(trainingData,  numClasses, categoricalFeaturesInfo, impurity,  maxDepth, maxBins);</span><br></pre></td></tr></table></figure><h2 id="3-4、模型预测"><a href="#3-4、模型预测" class="headerlink" title="3.4、模型预测"></a><strong>3.4、模型预测</strong></h2><p>接下来我们调用决策树模型的predict方法对测试数据集进行预测，并把模型结构打印出来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;Double, Double&gt; predictionAndLabel =  testData.mapToPair(point-&gt;&#123;</span><br><span class="line">      return new Tuple2&lt;&gt;(model.predict(point.features()),point.label());</span><br><span class="line">&#125;);</span><br><span class="line">//打印预测和实际结果</span><br><span class="line">predictionAndLabel.foreach(x-&gt;&#123;</span><br><span class="line">      System.out.println(&quot;predictionAndLabel:&quot;+x);</span><br><span class="line">&#125;);</span><br><span class="line">System.out.println(&quot;Learned  classification tree  model:&quot;+model.toDebugString());</span><br><span class="line">/**</span><br><span class="line">*控制台输出结果：</span><br><span class="line">-----------------------</span><br><span class="line">Learned classification tree model:DecisionTreeModel classifier of depth 5 with 15  nodes</span><br><span class="line">  If (feature 2 &lt;= 2.45)</span><br><span class="line">   Predict: 0.0</span><br><span class="line">  Else (feature 2 &gt; 2.45)</span><br><span class="line">   If (feature 2 &lt;= 4.75)</span><br><span class="line">    Predict: 1.0</span><br><span class="line">   Else (feature 2 &gt; 4.75)</span><br><span class="line">    If (feature 2 &lt;= 4.95)</span><br><span class="line">     If (feature 0 &lt;= 6.25)</span><br><span class="line">      If (feature 1 &lt;= 3.05)</span><br><span class="line">       Predict: 2.0</span><br><span class="line">      Else (feature 1 &gt; 3.05)</span><br><span class="line">       Predict: 1.0</span><br><span class="line">     Else (feature 0 &gt; 6.25)</span><br><span class="line">      Predict: 1.0</span><br><span class="line">    Else (feature 2 &gt; 4.95)</span><br><span class="line">     If (feature 3 &lt;= 1.7000000000000002)</span><br><span class="line">      If (feature 0 &lt;= 6.05)</span><br><span class="line">       Predict: 1.0</span><br><span class="line">      Else (feature 0 &gt; 6.05)</span><br><span class="line">       Predict: 2.0</span><br><span class="line">     Else (feature 3 &gt; 1.7000000000000002)</span><br><span class="line">      Predict: 2.0</span><br><span class="line">------------------------</span><br><span class="line">**/</span><br></pre></td></tr></table></figure><h2 id="3-5、准确性评估"><a href="#3-5、准确性评估" class="headerlink" title="3.5、准确性评估"></a><strong>3.5、准确性评估</strong></h2><p>最后，我们把模型预测的准确性打印出来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">double testErr = predictionAndLabel.filter(pl -&gt;  !pl._1().equals(pl._2())).count() / (double)  testData.count();</span><br><span class="line">System.out.println(&quot;Test  Error:&quot;+testErr);</span><br><span class="line">/**</span><br><span class="line">*控制台输出结果：</span><br><span class="line">------------------------------</span><br><span class="line">Test Error:0.06976744186046512</span><br><span class="line">------------------------------</span><br><span class="line">**/</span><br></pre></td></tr></table></figure><p><strong>参考资料：</strong></p><ul><li>1、<a href="https://baike.baidu.com/reference/10377049/bb04ENljXDjtMUAno2syAKw6fOK9Wwt8IgoEHiw7zx809xYpOuSz0cHJ6AgNYnF_agishqitVsD4fxoiNM3_E2i1-7RQJSign1RZ0BwOXBvNMppTspJQhmpCGyA" title="决策树" target="_blank" rel="noopener">决策树</a> ．云南大学[引用日期2013-01-22]</li></ul><ul><li><a href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" title="sparkml 决策树" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a></li></ul></div><footer><div class="categories"><a href="/categories/machinelearning/">机器学习</a></div><div class="tags"><a href="/tags/content/">content</a>, <a href="/tags/machine-learning/">machine learning</a></div><div class="addthis addthis_toolbox addthis_default_style"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a> <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a> <a class="addthis_counter addthis_pill_style"></a></div><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script><div class="clearfix"></div></footer></div></article><section id="comment"><h1 class="title">留言</h1><div id="fb-root"></div><script>!function(e,t,n){var a,c=e.getElementsByTagName(t)[0];e.getElementById(n)||((a=e.createElement(t)).id=n,a.src="//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345",c.parentNode.insertBefore(a,c))}(document,"script","facebook-jssdk")</script><div class="fb-comments" data-href="http://www.wangpengcufe.com/machinelearning/ml-ml6/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div></section></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>63</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>17</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>81</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>17</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔（简书）</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">机器学习（csdn博客）</a></li><li><a href="https://zhuanlan.zhihu.com/c_1182309165824901120" target="_blank">机器学习（知乎）</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li><li><a href="https://baippt.com/" target="_blank">ppt模板免费下载</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2019 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>