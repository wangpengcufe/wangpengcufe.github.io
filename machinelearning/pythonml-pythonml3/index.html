<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>python机器学习（三）分类算法-朴素贝叶斯 | 机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="机器学习,分类算法,朴素贝叶斯 PDF电子书下载,电子书，PDF下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:title" content="python机器学习（三）分类算法-朴素贝叶斯"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-pythonml-pythonml3" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-02-04T16:22:16.000Z"><a href="/machinelearning/pythonml-pythonml3/">2020-02-05</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="p-name title" itemprop="headline name">python机器学习（三）分类算法-朴素贝叶斯</h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/pythonml3.1.png" alt="朴素贝叶斯"></p><h1 id="一、概率基础"><a href="#一、概率基础" class="headerlink" title="一、概率基础"></a><strong>一、概率基础</strong></h1><p><strong>概率定义：</strong><br>概率定义为一件事情发生的可能性，例如，随机抛硬币，正面朝上的概率。</p><p><strong>联合概率：</strong><br>包含多个条件，且所有条件同时成立的概率，记作：𝑃(𝐴,𝐵) 。</p><p><strong>条件概率：</strong><br>事件A在另外一个事件B已经发生条件下的发生概率，记作：𝑃(𝐴|𝐵) 。P(A1,A2|B) = P(A1|B)P(A2|B)，需要注意的是：此条件概率的成立，是由于A1,A2相互独立的结果。</p><h1 id="二、朴素贝叶斯介绍"><a href="#二、朴素贝叶斯介绍" class="headerlink" title="二、朴素贝叶斯介绍"></a><strong>二、朴素贝叶斯介绍</strong></h1><p><strong>公式：</strong><br><img src="http://wangpengcufe.com/pythonml3.2.png" alt="朴素贝叶斯公式"><br>其中，w为给定文档的特征值(频数统计,预测文档提供)，c为文档类别。<br>公式可以理解为：</p><p><img src="http://wangpengcufe.com/pythonml3.3.png" alt="朴素贝叶斯公式的理解"><br>其中c可以是不同类别。</p><p>公式分为三个部分：</p><p><strong>𝑃(𝐶)</strong>：每个文档类别的概率(某文档类别词数／总文档词数)<br><strong>𝑃(𝑊│𝐶)</strong>：给定类别下特征（被预测文档中出现的词）的概率<br>计算方法：𝑃(𝐹1│𝐶)=𝑁𝑖/𝑁 （训练文档中去计算）<br>𝑁𝑖为该𝐹1词在C类别所有文档中出现的次数<br>N为所属类别C下的文档所有词出现的次数和<br><strong>𝑃(𝐹1,𝐹2,…)</strong>: 预测文档中每个词的概率</p><p><strong>举个栗子：</strong></p><p>现有一篇被预测文档：出现了都江宴，武汉，武松，计算属于历史，地理的类别概率？<br><img src="http://wangpengcufe.com/pythonml3.4.png" alt="image"><br>历史：𝑃(都江宴，武汉，武松│历史)∗P(历史)=（10/108）∗（22/108）∗（65/108）∗(108/235) =0.00563435<br>地理：𝑃(都江宴，武汉，武松│地理)∗P(地理)=（58/127）∗（17/127）∗（0/127）∗(127/235)=0</p><p><strong>拉普拉斯平滑：</strong><br>思考：属于某个类别为0，合适吗？<br>从上面的例子我们得到地理概率为0，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零。<br>解决方法：拉普拉斯平滑系数。<br><img src="http://wangpengcufe.com/pythonml3.5.png" alt="image"><br>𝛼为指定的系数一般为1，m为训练文档中统计出的特征词个数</p><p><strong>sklearn朴素贝叶斯实现API：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</span><br><span class="line">alpha：拉普拉斯平滑系数</span><br></pre></td></tr></table></figure><p><strong>案例：新闻分类</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=&apos;all&apos;)</span><br><span class="line"># 进行数据分割</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)</span><br><span class="line"># 对数据集进行特征抽取</span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line"># 以训练集当中的词的列表进行每篇文章重要性统计[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;]</span><br><span class="line">x_train = tf.fit_transform(x_train)</span><br><span class="line">x_test = tf.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 进行朴素贝叶斯算法的预测</span><br><span class="line">mlt = MultinomialNB(alpha=1.0)</span><br><span class="line">print(x_train)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(0, 120993)	0.0838226531816039</span><br><span class="line">(0, 36277)	0.028728297074726128</span><br><span class="line">(0, 118261)	0.051733692584494416</span><br><span class="line">(0, 118605)	0.08660213360333731</span><br><span class="line">(0, 78914)	0.10725171098177662</span><br><span class="line">(0, 120174)	0.07226288195761017</span><br><span class="line">(0, 146730)	0.03649798864200877</span><br><span class="line">(0, 49960)	0.09535813190987927</span><br><span class="line">(0, 108029)	0.10406938034117505</span><br><span class="line">(0, 151947)	0.1081016719923428</span><br><span class="line">(0, 120110)	0.13513684031456163</span><br><span class="line">(0, 34588)	0.06453595223748614</span><br><span class="line">(0, 133893)	0.04993313285348771</span><br><span class="line">(0, 31218)	0.07845873103784344</span><br><span class="line">(0, 108032)	0.08430822316250115</span><br><span class="line">(0, 30921)	0.11806736198114927</span><br><span class="line">(0, 33267)	0.030864914635712264</span><br><span class="line">(0, 36137)	0.0714722249527062</span><br><span class="line">(0, 57776)	0.07110907374703304</span><br><span class="line">(0, 77937)	0.026514922107534245</span><br><span class="line">(0, 90944)	0.09746338158610199</span><br><span class="line">(0, 135824)	0.09394365947415394</span><br><span class="line">(0, 49956)	0.09183375914922258</span><br><span class="line">(0, 151957)	0.07203295034824395</span><br><span class="line">(0, 33356)	0.07203295034824395</span><br><span class="line">:	:</span><br><span class="line">(14133, 45099)	0.030803124311834594</span><br><span class="line">(14133, 135309)	0.02305588722190138</span><br><span class="line">(14133, 135472)	0.06570104508511963</span><br><span class="line">(14133, 52014)	0.05222321951090842</span><br><span class="line">(14133, 108029)	0.05584161408783517</span><br><span class="line">(14133, 36137)	0.07670122356304401</span><br><span class="line">(14133, 34063)	0.12187079805145053</span><br><span class="line">(14133, 106978)	0.0851182715752145</span><br><span class="line">(14133, 106534)	0.03378056586331488</span><br><span class="line">(14133, 105921)	0.09707364301640503</span><br><span class="line">(14133, 103839)	0.07144955527096918</span><br><span class="line">(14133, 136535)	0.03801377630817533</span><br><span class="line">(14133, 42966)	0.028558472354146207</span><br><span class="line">(14133, 81075)	0.02180715538325887</span><br><span class="line">(14133, 135641)	0.025875408277197205</span><br><span class="line">(14133, 148185)	0.028450089379106706</span><br><span class="line">(14133, 78894)	0.020030955308174968</span><br><span class="line">(14133, 147914)	0.047259202253661425</span><br><span class="line">(14133, 90152)	0.017166154294786778</span><br><span class="line">(14133, 45598)	0.05645818387150284</span><br><span class="line">(14133, 135325)	0.03667700550640032</span><br><span class="line">(14133, 118218)	0.02343357701502816</span><br><span class="line">(14133, 131632)	0.01710795977554328</span><br><span class="line">(14133, 59957)	0.0485327006460036</span><br><span class="line">(14133, 67480)	0.01710795977554328</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mlt.fit(x_train, y_train)   #MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span><br><span class="line">y_predict = mlt.predict(x_test)</span><br><span class="line">print(&quot;预测的文章类别为：&quot;, y_predict)</span><br><span class="line">#预测的文章类别为： [ 3 16  5 ...  0  5  8]</span><br><span class="line"># 得出准确率</span><br><span class="line">print(&quot;准确率为：&quot;, mlt.score(x_test, y_test))</span><br><span class="line">#准确率为： 0.8414685908319185</span><br><span class="line">print(&quot;每个类别的精确率和召回率：&quot;, classification_report(y_test, y_predict, target_names=news.target_names))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">每个类别的精确率和召回率：                           precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.89      0.75      0.81       210</span><br><span class="line">           comp.graphics       0.87      0.81      0.84       225</span><br><span class="line"> comp.os.ms-windows.misc       0.77      0.90      0.83       209</span><br><span class="line">comp.sys.ibm.pc.hardware       0.77      0.78      0.78       258</span><br><span class="line">   comp.sys.mac.hardware       0.86      0.88      0.87       223</span><br><span class="line">          comp.windows.x       0.97      0.76      0.85       260</span><br><span class="line">            misc.forsale       0.92      0.68      0.78       233</span><br><span class="line">               rec.autos       0.91      0.89      0.90       263</span><br><span class="line">         rec.motorcycles       0.94      0.96      0.95       260</span><br><span class="line">      rec.sport.baseball       0.93      0.92      0.92       230</span><br><span class="line">        rec.sport.hockey       0.89      0.97      0.93       234</span><br><span class="line">               sci.crypt       0.64      0.99      0.78       235</span><br><span class="line">         sci.electronics       0.94      0.68      0.79       275</span><br><span class="line">                 sci.med       0.96      0.89      0.93       241</span><br><span class="line">               sci.space       0.89      0.97      0.93       246</span><br><span class="line">  soc.religion.christian       0.56      0.99      0.72       257</span><br><span class="line">      talk.politics.guns       0.84      0.94      0.89       256</span><br><span class="line">   talk.politics.mideast       0.92      0.98      0.94       245</span><br><span class="line">      talk.politics.misc       0.98      0.67      0.80       182</span><br><span class="line">      talk.religion.misc       1.00      0.17      0.29       170</span><br><span class="line"></span><br><span class="line">                accuracy                           0.84      4712</span><br><span class="line">               macro avg       0.87      0.83      0.83      4712</span><br><span class="line">            weighted avg       0.87      0.84      0.84      4712</span><br></pre></td></tr></table></figure><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a><strong>三、总结</strong></h1><p><strong>优点：</strong></p><ul><li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li><li>分类准确度高，速度快</li></ul><p><strong>缺点：</strong></p><ul><li>需要知道先验概率P(F1,F2,…|C)，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。</li></ul></div><footer><div class="categories"><a href="/categories/machinelearning/">机器学习</a></div><div class="tags"><a href="/tags/content/">content</a>, <a href="/tags/machine-learning/">machine learning</a></div><div class="addthis addthis_toolbox addthis_default_style"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a> <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a> <a class="addthis_counter addthis_pill_style"></a></div><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script><div class="clearfix"></div></footer></div></article><section id="comment"><h1 class="title">留言</h1><div id="fb-root"></div><script>!function(e,t,n){var a,c=e.getElementsByTagName(t)[0];e.getElementById(n)||((a=e.createElement(t)).id=n,a.src="//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345",c.parentNode.insertBefore(a,c))}(document,"script","facebook-jssdk")</script><div class="fb-comments" data-href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml3/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div></section></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>63</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>22</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>86</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>22</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔（简书）</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">机器学习（csdn博客）</a></li><li><a href="https://zhuanlan.zhihu.com/c_1182309165824901120" target="_blank">机器学习（知乎）</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li><li><a href="https://baippt.com/" target="_blank">ppt模板免费下载</a></li><li><a href="http://www.youneedcn.com/" target="_blank">你要的资源</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2020 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>