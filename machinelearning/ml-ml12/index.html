<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>机器学习（十二） 特征提取 TF-IDF | 机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="机器学习,特征提取 TF-IDF PDF电子书下载,电子书，PDF下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:title" content="机器学习（十二） 特征提取 TF-IDF"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-ml-ml12" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-16T14:35:05.000Z"><a href="/machinelearning/ml-ml12/">2019-11-16</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="p-name title" itemprop="headline name">机器学习（十二） 特征提取 TF-IDF</h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml12-0.png" alt="目录"></p><a id="more"></a><h1 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a><strong>一、概念</strong></h1><p><strong>“词频－逆向文件频率”（TF-IDF）</strong>是一种在文本挖掘中广泛使用的特征向量化方法，它可以体现一个文档中词语在语料库中的重要程度。<br>词语由t表示，文档由d表示，语料库由D表示。词频TF(t,d)是词语t在文档d中出现的次数。文件频率DF(t,D)是包含词语的文档的个数。如果我们只使用词频来衡量重要性，很容易过度强调在文档中经常出现，却没有太多实际信息的词语，比如“a”，“the”以及“of”。如果一个词语经常出现在语料库中，意味着它并不能很好的对文档进行区分。TF-IDF就是在数值化文档信息，衡量词语能提供多少信息以区分文档。其定义如下：</p><p><img src="http://wangpengcufe.com/ml12-1.png" alt="目录"></p><p>此处|D| 是语料库中总的文档数。公式中使用log函数，当词出现在所有文档中时，它的IDF值变为0。加1是为了避免分母为0的情况。TF-IDF 度量值表示如下：</p><p><img src="http://wangpengcufe.com/ml12-2.png" alt="目录"></p><p>在Spark ML库中，TF-IDF被分成两部分：TF (+hashing) 和 IDF。</p><p><strong>TF</strong>: HashingTF 是一个Transformer，在文本处理中，接收词条的集合然后把这些集合转化成固定长度的特征向量。这个算法在哈希的同时会统计各个词条的词频。</p><p><strong>IDF</strong>: IDF是一个Estimator，在一个数据集上应用它的fit（）方法，产生一个IDFModel。 该IDFModel 接收特征向量（由HashingTF产生），然后计算每一个词在文档中出现的频次。IDF会减少那些在语料库中出现频率较高的词的权重。</p><p>Spark.mllib 中实现词频率统计使用特征hash的方式，原始特征通过hash函数，映射到一个索引值。后面只需要统计这些索引值的频率，就可以知道对应词的频率。这种方式避免设计一个全局1对1的词到索引的映射，这个映射在映射大量语料库时需要花费更长的时间。但需要注意，通过hash的方式可能会映射到同一个值的情况，即不同的原始特征通过Hash映射后是同一个值。为了降低这种情况出现的概率，我们只能对特征向量升维。i.e., 提高hash表的桶数，默认特征维度是 2^20 = 1,048,576.</p><p>在下面的代码段中，我们以一组句子开始。首先使用分解器Tokenizer把句子划分为单个词语。对每一个句子（词袋），我们使用HashingTF将句子转换为特征向量，最后使用IDF重新调整特征向量。这种转换通常可以提高使用文本特征的性能。</p><h1 id="二、代码实现"><a href="#二、代码实现" class="headerlink" title="二、代码实现"></a><strong>二、代码实现</strong></h1><h3 id="2-1、构造文档集合"><a href="#2-1、构造文档集合" class="headerlink" title="2.1、构造文档集合"></a><strong>2.1、构造文档集合</strong></h3><p>导入TFIDF所需要的包，创建一个简单的DataFrame，每一个句子代表一个文档。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line">import org.apache.spark.ml.feature.HashingTF;</span><br><span class="line">import org.apache.spark.ml.feature.IDF;</span><br><span class="line">import org.apache.spark.ml.feature.IDFModel;</span><br><span class="line">import org.apache.spark.ml.feature.Tokenizer;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.Metadata;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line">//获取spark</span><br><span class="line">SparkSession spark = SparkSession.builder().appName(&quot;FeatureExtractors&quot;).master(&quot;local&quot;).getOrCreate();</span><br><span class="line"></span><br><span class="line">//构造数据</span><br><span class="line">List&lt;Row&gt; rawData = Arrays.asList(RowFactory.create(0, &quot;I heard about Spark and I love Spark&quot;),</span><br><span class="line">        RowFactory.create(0, &quot;I wish Java could use case classes&quot;),</span><br><span class="line">        RowFactory.create(1, &quot;Logistic regression models are neat&quot;)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">StructType schema = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;label&quot;,DataTypes.IntegerType,false,Metadata.empty()),</span><br><span class="line">        new StructField(&quot;sentence&quot;,DataTypes.StringType,false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line">Dataset&lt;Row&gt; sentenceData = spark.createDataFrame(rawData,schema);</span><br><span class="line">sentenceData.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----+-------------------------------------+</span><br><span class="line">|label|sentence                             |</span><br><span class="line">+-----+-------------------------------------+</span><br><span class="line">|0    |I heard about Spark and I love  Spark|</span><br><span class="line">|0    |I wish Java could use case  classes  |</span><br><span class="line">|1    |Logistic regression models are  neat |</span><br><span class="line">+-----+-------------------------------------+</span><br></pre></td></tr></table></figure><h3 id="2-2、tokenizer对句子进行分词"><a href="#2-2、tokenizer对句子进行分词" class="headerlink" title="2.2、tokenizer对句子进行分词"></a><strong>2.2、tokenizer对句子进行分词</strong></h3><p>得到文档集合后，即可用tokenizer对句子进行分词。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tokenizer tokenizer = new Tokenizer().setInputCol(&quot;sentence&quot;).setOutputCol(&quot;words&quot;);</span><br><span class="line">Dataset&lt;Row&gt; wordsData = tokenizer.transform(sentenceData);</span><br><span class="line">wordsData.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----+------------------------------------+---------------------------------------------+</span><br><span class="line">|label|sentence                            |words                                        |</span><br><span class="line">+-----+------------------------------------+---------------------------------------------+</span><br><span class="line">|0    |I heard about Spark and I love Spark|[i, heard, about, spark, and, i, love, spark]|</span><br><span class="line">|0    |I wish Java could use case classes  |[i, wish, java, could, use, case, classes]   |</span><br><span class="line">|1    |Logistic regression models are neat |[logistic, regression, models, are, neat]    |</span><br><span class="line">+-----+------------------------------------+---------------------------------------------+</span><br></pre></td></tr></table></figure><h3 id="2-3、TF把句子哈希成特征向量"><a href="#2-3、TF把句子哈希成特征向量" class="headerlink" title="2.3、TF把句子哈希成特征向量"></a><strong>2.3、TF把句子哈希成特征向量</strong></h3><p>得到分词后的文档序列后，即可使用HashingTF的transform()方法把句子哈希成特征向量，这里设置哈希表的桶数为2000。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HashingTF hashingTF = new HashingTF().setInputCol(&quot;words&quot;).setOutputCol(&quot;rawFeatures&quot;).setNumFeatures(2000);</span><br><span class="line">Dataset&lt;Row&gt; featurizedData = hashingTF.transform(wordsData);</span><br><span class="line">featurizedData.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----+------------------------------------+---------------------------------------------+---------------------------------------------------------------------+</span><br><span class="line">|label|sentence                            |words                                        |rawFeatures                                                          |</span><br><span class="line">+-----+------------------------------------+---------------------------------------------+---------------------------------------------------------------------+</span><br><span class="line">|0    |I heard about Spark and I love Spark|[i, heard, about, spark, and, i, love, spark]|(2000,[240,333,1105,1329,1357,1777],[1.0,1.0,2.0,2.0,1.0,1.0])       |</span><br><span class="line">|0    |I wish Java could use case classes  |[i, wish, java, could, use, case, classes]   |(2000,[213,342,489,495,1329,1809,1967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|</span><br><span class="line">|1    |Logistic regression models are neat |[logistic, regression, models, are, neat]    |(2000,[286,695,1138,1193,1604],[1.0,1.0,1.0,1.0,1.0])                |</span><br><span class="line">+-----+------------------------------------+---------------------------------------------+---------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>每一个单词被哈希成了一个不同的索引值。以”I heard about Spark and I love Spark”为例，输出结果中2000代表哈希表的桶数，“[240,333,1105,1329,1357,1777]”分别代表着“heard, about, i, spark, and, love”的哈希值，“[1.0,1.0,2.0,2.0,1.0,1.0]”为对应单词的出现次数，无序。</p><h3 id="2-4、IDF修正词频特征向量"><a href="#2-4、IDF修正词频特征向量" class="headerlink" title="2.4、IDF修正词频特征向量"></a><strong>2.4、IDF修正词频特征向量</strong></h3><p>可以看到，分词序列被变换成一个稀疏特征向量，其中每个单词都被散列成了一个不同的索引值，特征向量在某一维度上的值即该词汇在文档中出现的次数。<br>最后，使用IDF来对单纯的词频特征向量进行修正，使其更能体现不同词汇对文本的区别能力，IDF是一个Estimator，调用fit()方法并将词频向量传入，即产生一个IDFModel。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IDF idf = new IDF().setInputCol(&quot;rawFeatures&quot;).setOutputCol(&quot;features&quot;);</span><br><span class="line">IDFModel idfModel = idf.fit(featurizedData);</span><br></pre></td></tr></table></figure><h3 id="2-5、得到单词对应的TF-IDF度量值"><a href="#2-5、得到单词对应的TF-IDF度量值" class="headerlink" title="2.5、得到单词对应的TF-IDF度量值"></a><strong>2.5、得到单词对应的TF-IDF度量值</strong></h3><p>很显然，IDFModel是一个Transformer，调用它的transform()方法，即可得到每一个单词对应的TF-IDF度量值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; rescaledData = idfModel.transform(featurizedData);</span><br><span class="line">rescaledData.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----+------------------------------------+---------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">|label|sentence                            |words                                        |rawFeatures                                                          |features                                                                                                                                                                       |</span><br><span class="line">+-----+------------------------------------+---------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">|0    |I heard about Spark and I love Spark|[i, heard, about, spark, and, i, love, spark]|(2000,[240,333,1105,1329,1357,1777],[1.0,1.0,2.0,2.0,1.0,1.0])       |(2000,[240,333,1105,1329,1357,1777],[0.6931471805599453,0.6931471805599453,1.3862943611198906,0.5753641449035617,0.6931471805599453,0.6931471805599453])                       |</span><br><span class="line">|0    |I wish Java could use case classes  |[i, wish, java, could, use, case, classes]   |(2000,[213,342,489,495,1329,1809,1967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(2000,[213,342,489,495,1329,1809,1967],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453])|</span><br><span class="line">|1    |Logistic regression models are neat |[logistic, regression, models, are, neat]    |(2000,[286,695,1138,1193,1604],[1.0,1.0,1.0,1.0,1.0])                |(2000,[286,695,1138,1193,1604],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                               |</span><br><span class="line">+-----+------------------------------------+---------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; data = rescaledData.select(&quot;features&quot;,&quot;label&quot;);</span><br><span class="line">data.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+</span><br><span class="line">|features                                                                                                                                                                        |label|</span><br><span class="line">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+</span><br><span class="line">|(2000,[240,333,1105,1329,1357,1777],[0.6931471805599453,0.6931471805599453,1.3862943611198906,0.5753641449035617,0.6931471805599453,0.6931471805599453])                        |0    |</span><br><span class="line">|(2000,[213,342,489,495,1329,1809,1967],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453]) |0    |</span><br><span class="line">|(2000,[286,695,1138,1193,1604],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                |1    |</span><br><span class="line">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+</span><br></pre></td></tr></table></figure><p>可以看到，特征向量已经被其在语料库中出现的总次数进行了修正，通过TF-IDF得到的特征向量，在接下来可以被应用到相关的机器学习方法中。</p><p><strong>参考资料：</strong><a href="http://spark.apache.org/docs/latest/ml-features.html#tf-idf" title="http://spark.apache.org/docs/latest/ml-features.html#tf-idf" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/ml-features.html#tf-idf</a></p></div><footer><div class="categories"><a href="/categories/machinelearning/">机器学习</a></div><div class="tags"><a href="/tags/content/">content</a>, <a href="/tags/machine-learning/">machine learning</a></div><div class="addthis addthis_toolbox addthis_default_style"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a> <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a> <a class="addthis_counter addthis_pill_style"></a></div><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script><div class="clearfix"></div></footer></div></article><section id="comment"><h1 class="title">留言</h1><div id="fb-root"></div><script>!function(e,t,n){var a,c=e.getElementsByTagName(t)[0];e.getElementById(n)||((a=e.createElement(t)).id=n,a.src="//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345",c.parentNode.insertBefore(a,c))}(document,"script","facebook-jssdk")</script><div class="fb-comments" data-href="http://www.wangpengcufe.com/machinelearning/ml-ml12/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div></section></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>63</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>21</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>85</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>21</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔（简书）</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">机器学习（csdn博客）</a></li><li><a href="https://zhuanlan.zhihu.com/c_1182309165824901120" target="_blank">机器学习（知乎）</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li><li><a href="https://baippt.com/" target="_blank">ppt模板免费下载</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2020 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>