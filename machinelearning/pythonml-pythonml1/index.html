<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>python 机器学习（一）机器学习概述与特征工程 | 机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="机器学习,机器学习工作流,特征工程,机器学习开发流程 PDF电子书下载,电子书，PDF下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:title" content="python 机器学习（一）机器学习概述与特征工程"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-pythonml-pythonml1" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-19T02:06:06.000Z"><a href="/machinelearning/pythonml-pythonml1/">2020-01-19</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="p-name title" itemprop="headline name">python 机器学习（一）机器学习概述与特征工程</h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80.png" alt="机器学习概述与特征工程"></p><h1 id="一、机器学习概述"><a href="#一、机器学习概述" class="headerlink" title="一、机器学习概述"></a>一、机器学习概述</h1><h5 id="1-1、什么是机器学习？"><a href="#1-1、什么是机器学习？" class="headerlink" title="1.1、什么是机器学习？"></a><strong>1.1、什么是机器学习</strong>？</h5><p>机器学习是从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测</p><h5 id="1-2、为什么需要机器学习？"><a href="#1-2、为什么需要机器学习？" class="headerlink" title="1.2、为什么需要机器学习？"></a><strong>1.2、为什么需要机器学习</strong>？</h5><ul><li>解放生产力，智能客服，可以不知疲倦的24小时作业</li><li>解决专业问题，ET医疗，帮助看病</li><li>提供社会便利，例如杭州的城市大脑<h5 id="1-3、机器学习应用场景"><a href="#1-3、机器学习应用场景" class="headerlink" title="1.3、机器学习应用场景"></a><strong>1.3、机器学习应用场景</strong></h5></li><li>自然语言处理</li><li>无人驾驶</li><li>计算机视觉</li><li>推荐系统<h1 id="二、数据来源与类型"><a href="#二、数据来源与类型" class="headerlink" title="二、数据来源与类型"></a>二、数据来源与类型</h1><h5 id="2-1、数据的来源"><a href="#2-1、数据的来源" class="headerlink" title="2.1、数据的来源"></a><strong>2.1、数据的来源</strong></h5></li><li>企业日益积累的大量数据（互联网公司更为显著）</li><li>政府掌握的各种数据</li><li>科研机构的实验数据<h5 id="2-2、数据的类型"><a href="#2-2、数据的类型" class="headerlink" title="2.2、数据的类型"></a><strong>2.2、数据的类型</strong></h5>数据的类型将是机器学习模型不同问题不同处理的依据。数据的类型包括：</li></ul><p><strong>离散型数据</strong>：由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。</p><p><strong>连续型数据</strong>：变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。</p><p><strong>注意</strong>：</p><ul><li>只要记住一点，离散型是区间内不可分，连续型是区间内可分</li></ul><h5 id="2-3、可用的数据集"><a href="#2-3、可用的数据集" class="headerlink" title="2.3、可用的数据集"></a><strong>2.3、可用的数据集</strong></h5><p><strong>scikit-learn</strong>：数据量较小 ，方便学习。<br><strong>UCI</strong>：收录了360个数据集，覆盖科学、生活、经济等领域 ，数据量几十万。<br><strong>Kaggle</strong>：大数据竞赛平台，80万科学家，真实数据，数据量巨大。</p><p>常用数据集数据的结构组成：特征值+目标值，如下图：<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.1.png" alt="常用数据集结构"></p><h1 id="三、数据的特征工程"><a href="#三、数据的特征工程" class="headerlink" title="三、数据的特征工程"></a>三、数据的特征工程</h1><h5 id="3-1、特征工程是什么？"><a href="#3-1、特征工程是什么？" class="headerlink" title="3.1、特征工程是什么？"></a><strong>3.1、特征工程是什么</strong>？</h5><p>特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。</p><h5 id="3-2、特征工程的意义"><a href="#3-2、特征工程的意义" class="headerlink" title="3.2、特征工程的意义"></a><strong>3.2、特征工程的意义</strong></h5><p><strong>意义</strong>：直接影响模型的预测结果。</p><h5 id="3-3、scikit-learn"><a href="#3-3、scikit-learn" class="headerlink" title="3.3、scikit-learn"></a><strong>3.3、scikit-learn</strong></h5><ul><li>Python语言的机器学习工具</li><li>Scikit-learn包括许多知名的机器学习算法的实现</li><li>Scikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎。</li><li>目前稳定版本0.18</li><li>安装：pip3 install Scikit-learn</li><li>引用：import sklearn</li></ul><h5 id="3-4、数据的特征抽取"><a href="#3-4、数据的特征抽取" class="headerlink" title="3.4、数据的特征抽取"></a><strong>3.4、数据的特征抽取</strong></h5><h6 id="3-4-1、特点："><a href="#3-4-1、特点：" class="headerlink" title="3.4.1、特点："></a><strong>3.4.1、特点</strong>：</h6><ul><li>特征抽取针对非连续型数据</li><li>特征抽取对文本等进行特征值化</li></ul><h6 id="3-4-2、sklearn特征抽取API"><a href="#3-4-2、sklearn特征抽取API" class="headerlink" title="3.4.2、sklearn特征抽取API :"></a><strong>3.4.2、sklearn特征抽取API</strong> :</h6><p>sklearn.feature_extraction</p><h6 id="3-4-3、字典特征抽取"><a href="#3-4-3、字典特征抽取" class="headerlink" title="3.4.3、字典特征抽取 :"></a><strong>3.4.3、字典特征抽取</strong> :</h6><p><strong>作用</strong>：对字典数据进行特征值化<br><strong>类</strong>：sklearn.feature_extraction.DictVectorizer<br><strong>DictVectorizer语法</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DictVectorizer(sparse=True,…)</span><br><span class="line"></span><br><span class="line">DictVectorizer.fit_transform(X)       </span><br><span class="line">    X:字典或者包含字典的迭代器</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">DictVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">DictVectorizer.get_feature_names()</span><br><span class="line">    返回类别名称</span><br><span class="line"></span><br><span class="line">DictVectorizer.transform(X)</span><br><span class="line">    按照原先的标准转换</span><br></pre></td></tr></table></figure><p><strong>流程</strong>：<br>1、实例化类DictVectorizer<br>2、调用fit_transform方法输入数据并转换</p><p><strong>举一个栗子</strong>：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction import DictVectorizer</span><br><span class="line">dict = DictVectorizer(sparse=False)</span><br><span class="line">data = dict.fit_transform([&#123;&apos;name&apos;: &apos;张飞&apos;,&apos;score&apos;: 70&#125;, &#123;&apos;name&apos;: &apos;赵云&apos;,&apos;score&apos;:100&#125;, &#123;&apos;name&apos;: &apos;刘备&apos;,&apos;score&apos;: 98&#125;])</span><br><span class="line">print(dict.get_feature_names())</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p></p><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[&apos;name=刘备&apos;, &apos;name=张飞&apos;, &apos;name=赵云&apos;, &apos;score&apos;]</span><br><span class="line">[[  0.   1.   0.  70.]</span><br><span class="line"> [  0.   0.   1. 100.]</span><br><span class="line"> [  1.   0.   0.  98.]]</span><br></pre></td></tr></table></figure><p>从中，我们可以看出：对于字典 [{‘name’: ‘张飞’,’score’: 70}, {‘name’: ‘赵云’,’score’:100}, {‘name’: ‘刘备’,’score’: 98}] ，DictVectorizer类将汉字（张飞，赵云，刘备）转成了one-hot编码（0,1,0），而数值类型的数据（70,100,98）是不做处理的。</p><p>什么是one-hot编码？<br>One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。</p><h6 id="3-4-4、文本特征抽取"><a href="#3-4-4、文本特征抽取" class="headerlink" title="3.4.4、文本特征抽取"></a><strong>3.4.4、文本特征抽取</strong></h6><p><strong>作用</strong>：对文本数据进行特征值化<br><strong>类</strong>：sklearn.feature_extraction.text.CountVectorizer<br><strong>CountVectorizer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CountVectorizer(max_df=1.0,min_df=1,…)</span><br><span class="line">    返回词频矩阵</span><br><span class="line"></span><br><span class="line">CountVectorizer.fit_transform(X,y)       </span><br><span class="line">    X:文本或者包含文本字符串的可迭代对象</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">CountVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">CountVectorizer.get_feature_names()</span><br><span class="line">    返回值:单词列表</span><br></pre></td></tr></table></figure><p><strong>流程</strong>:<br>1、实例化类CountVectorizer<br>2、调用fit_transform方法输入数据并转换</p><p><strong>举一个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;我爱学习，学习使我进步&quot;, &quot;我爱work，work 使我快乐&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;work&apos;, &apos;使我快乐&apos;, &apos;学习使我进步&apos;, &apos;我爱work&apos;, &apos;我爱学习&apos;]</span><br><span class="line">[[0 0 1 0 1]</span><br><span class="line"> [1 1 0 1 0]]</span><br></pre></td></tr></table></figure><p>可以看到，API中的CountVectorizer类将中文转换成了单个词语，并给每个词语的出现个数进行了统计。有一点要注意的是，程序并不会给中文分词，所以，例子中，‘学习使我进步’程序认为是一个词语，这种情况下，可以用空格，或者逗号，将中文进行分割。还有一点要注意的是，<strong>如果是英文的话，是不会统计单个字母的，因为字母的统计是没有意义的，同理，CountVectorizer也不支持单个中文字</strong>。</p><p>我们可以验证一下栗子：<br><strong>英文栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;I like study , study makes me happy&quot;, &quot;I am a good student&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;am&apos;, &apos;good&apos;, &apos;happy&apos;, &apos;like&apos;, &apos;makes&apos;, &apos;me&apos;, &apos;student&apos;, &apos;study&apos;]</span><br><span class="line">[[0 0 1 1 1 1 0 2]</span><br><span class="line"> [1 1 0 0 0 0 1 0]]</span><br></pre></td></tr></table></figure><p><strong>中文栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;我 热爱 学习， 学习 使我 进步&quot;, &quot;我 是 一个 好学生&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;一个&apos;, &apos;使我&apos;, &apos;好学生&apos;, &apos;学习&apos;, &apos;热爱&apos;, &apos;进步&apos;]</span><br><span class="line">[[0 1 0 2 1 1]</span><br><span class="line"> [1 0 1 0 0 0]]</span><br></pre></td></tr></table></figure><p>从中文栗子和英文栗子中，我们可以看到单个英文和单个中文是不会统计数量的，因为统计单个中文或者英文是没有意义的。</p><p>手动加空格，或者加逗号分隔始终是指标不治本，如果给我们一篇文章，让我们去处理的话，那要累到手瘫了。那么，有没有好的办法呢？是有的，那就是用 python 里提供的 jieba 分词类库。<br><strong>我们再来举一个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">con1 = jieba.cut(&quot;我热爱学习，学习使我感到进步。&quot;)</span><br><span class="line">con2 = jieba.cut(&quot;我热爱工作，工作可以让我感到快乐。&quot;)</span><br><span class="line">con3 = jieba.cut(&quot;如果不让我学习，也不让我工作，我会觉得浑身不舒服。&quot;)</span><br><span class="line"></span><br><span class="line"># 转换成列表</span><br><span class="line">content1 = list(con1)</span><br><span class="line">content2 = list(con2)</span><br><span class="line">content3 = list(con3)</span><br><span class="line"></span><br><span class="line"># 把列表转换成字符串</span><br><span class="line">c1 = &apos; &apos;.join(content1)</span><br><span class="line">c2 = &apos; &apos;.join(content2)</span><br><span class="line">c3 = &apos; &apos;.join(content3)</span><br><span class="line"></span><br><span class="line">print(c1, c2, c3)</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([c1, c2, c3])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我 热爱 学习 ， 学习 使 我 感到 进步 。 我 热爱工作 ， 工作 可以 让 我 感到 快乐 。 如果 不让 我 学习 ， 也 不让 我 工作 ， 我会 觉得 浑身 不 舒服 。</span><br><span class="line">[&apos;不让&apos;, &apos;可以&apos;, &apos;如果&apos;, &apos;学习&apos;, &apos;工作&apos;, &apos;快乐&apos;, &apos;感到&apos;, &apos;我会&apos;, &apos;浑身&apos;, &apos;热爱&apos;, &apos;热爱工作&apos;, &apos;舒服&apos;, &apos;觉得&apos;, &apos;进步&apos;]</span><br><span class="line">[[0 0 0 2 0 0 1 0 0 1 0 0 0 1]</span><br><span class="line"> [0 1 0 0 1 1 1 0 0 0 1 0 0 0]</span><br><span class="line"> [2 0 1 1 1 0 0 1 1 0 0 1 1 0]]</span><br></pre></td></tr></table></figure><p>从栗子中可以看到，jieba分词包把句子进行了分词，然后对每个词语的个数进行了统计，但是对于 ‘我’、 ‘也’ 这样的单个中文，并没有统计个数，因为这样的单个中文统计没有意义。</p><p>至此，我们学会了统计文章中英文和中文的词语的个数，那么，单纯统计一个词语出现的个数越多就表示这个词语在文章中越重要吗？那比如，“我们”，“你们”，“他们”，“你的”，这样的指示代词出现的频率应该是最高的，能说明代词是文章的重点吗？显然不是。怎么过滤掉这种出现很多，但是并不是重点的词语呢？我们就要开始学习一种 TF-IDF 的处理方法了。</p><h6 id="3-4-5、TF-IDF"><a href="#3-4-5、TF-IDF" class="headerlink" title="3.4.5、TF-IDF"></a><strong>3.4.5、TF-IDF</strong></h6><p><strong>主要思想</strong>：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。<br><strong>作用</strong>：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。<br><strong>类</strong>：sklearn.feature_extraction.text.TfidfVectorizer<br><strong>TfidfVectorizer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TfidfVectorizer(stop_words=None,…)</span><br><span class="line">    返回词的权重矩阵</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.fit_transform(X,y)       </span><br><span class="line">    X:文本或者包含文本字符串的可迭代对象</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.get_feature_names()</span><br><span class="line">    返回值:单词列表</span><br></pre></td></tr></table></figure><p>我们开始举个栗子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</span><br><span class="line"></span><br><span class="line">con1 = jieba.cut(&quot;我们热爱学习，学习使我们感到进步。&quot;)</span><br><span class="line">con2 = jieba.cut(&quot;我们热爱工作，工作可以让我们感到快乐。&quot;)</span><br><span class="line">con3 = jieba.cut(&quot;如果不让我们学习，不让我们工作，就会感到浑身不舒服。&quot;)</span><br><span class="line"></span><br><span class="line"># 转换成列表</span><br><span class="line">content1 = list(con1)</span><br><span class="line">content2 = list(con2)</span><br><span class="line">content3 = list(con3)</span><br><span class="line"></span><br><span class="line"># 把列表转换成字符串</span><br><span class="line">c1 = &apos; &apos;.join(content1)</span><br><span class="line">c2 = &apos; &apos;.join(content2)</span><br><span class="line">c3 = &apos; &apos;.join(content3)</span><br><span class="line"></span><br><span class="line">print(c1, c2, c3)</span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line">data = tf.fit_transform([c1, c2, c3])</span><br><span class="line">print(tf.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">我们 热爱 学习 ， 学习 使 我们 感到 进步 。 我们 热爱工作 ， 工作 可以 让 我们 感到 快乐 。 如果 不让 我们 学习 ， 不让 我们 工作 ， 就 会 感到 浑身 不 舒服 。</span><br><span class="line">[&apos;不让&apos;, &apos;可以&apos;, &apos;如果&apos;, &apos;学习&apos;, &apos;工作&apos;, &apos;快乐&apos;, &apos;感到&apos;, &apos;我们&apos;, &apos;浑身&apos;, &apos;热爱&apos;, &apos;热爱工作&apos;, &apos;舒服&apos;, &apos;进步&apos;]</span><br><span class="line">[[0.         0.         0.         0.61800047 0.         0.</span><br><span class="line">  0.23996625 0.4799325  0.         0.40629818 0.         0.</span><br><span class="line">  0.40629818]</span><br><span class="line"> [0.         0.43345167 0.         0.         0.32965117 0.43345167</span><br><span class="line">  0.25600354 0.51200708 0.         0.         0.43345167 0.</span><br><span class="line">  0.        ]</span><br><span class="line"> [0.63561168 0.         0.31780584 0.24169953 0.24169953 0.</span><br><span class="line">  0.18770125 0.3754025  0.31780584 0.         0.         0.31780584</span><br><span class="line">  0.        ]]</span><br></pre></td></tr></table></figure><p>我们可以看到，通过 TF-IDF 的处理，把每句话的重点单词找出来了，第一句话 “我们” 和 “学习” 都出现了2次 ，但 “学习”是0.61800047，“我们” 是0.4799325， 重点是 “学习” ， 第三句话“我们” 和 “ 不让 ” 都出现了2次 ，但 “不让”是0.63561168 ，“我们” 是0.3754025 ， 第三句话重点强调 “不让” ，因为 “我们” 在三句话中都频繁出现，并不是每一句话的 “专属” ， 也就不是一句话的重点了。</p><h5 id="3-5、数据的特征处理"><a href="#3-5、数据的特征处理" class="headerlink" title="3.5、数据的特征处理"></a><strong>3.5、数据的特征处理</strong></h5><h6 id="3-5-1、特征处理是什么？"><a href="#3-5-1、特征处理是什么？" class="headerlink" title="3.5.1、特征处理是什么？"></a><strong>3.5.1、特征处理是什么</strong>？</h6><p>通过特定的统计方法（数学方法）将数据转换成算法要求的数据。</p><h6 id="3-5-2、为什么需要特征处理？"><a href="#3-5-2、为什么需要特征处理？" class="headerlink" title="3.5.2、为什么需要特征处理？"></a><strong>3.5.2、为什么需要特征处理</strong>？</h6><p>每个特征的单位不一样，比如相亲的时候，有乘坐飞机的里程数，人的身高，玩游戏的时间，里程数的数值很大，身高相对里程数值很小，那么在做分析的时候，里程数的数值就会起决定性作用。事实在，在统计分析的时候，分析人员认为每个特征同样重要。所以我们需要把不同单位的数值进行特征处理，不因为数值的相差巨大而造成特征的差别。</p><h6 id="3-5-3、特征处理的方法"><a href="#3-5-3、特征处理的方法" class="headerlink" title="3.5.3、特征处理的方法"></a><strong>3.5.3、特征处理的方法</strong></h6><p>数值型数据：（标准缩放）<br>1、<strong>归一化</strong><br>2、<strong>标准化</strong><br>3、<strong>缺失值</strong><br>类别型数据：one-hot编码<br>时间类型：时间的切分</p><h6 id="3-5-4、归一化："><a href="#3-5-4、归一化：" class="headerlink" title="3.5.4、归一化："></a><strong>3.5.4、归一化</strong>：</h6><p><strong>特点</strong>：通过对原始数据进行变换把数据映射到(默认为[0,1])之间。<br><strong>公式</strong>： &nbsp;&nbsp;𝑋′= (𝑥−𝑚𝑖𝑛)/(𝑚𝑎𝑥−𝑚𝑖𝑛) &nbsp;&nbsp;&nbsp;&nbsp; 𝑋′′=𝑋′∗(𝑚𝑥−𝑚𝑖)+𝑚𝑖<br>其中：作用于每一列，max为一列的最大值，min为一列的最小值，那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0。<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.2.png" alt="归一化计算过程"><br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.3.png" alt="归一化计算过程"></p><p><strong>sklearn归一化API</strong> : sklearn.preprocessing.MinMaxScaler<br><strong>MinMaxScaler语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MinMaxScalar(feature_range=(0,1)…)</span><br><span class="line">    每个特征缩放到给定范围(默认[0,1])</span><br><span class="line"></span><br><span class="line">MinMaxScalar.fit_transform(X)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br></pre></td></tr></table></figure><p><strong>归一化步骤</strong>：<br>1、实例化MinMaxScalar<br>2、通过fit_transform转换</p><p><strong>举一个栗子</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">mm = MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,15,46]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[1.         0.         0.         0.        ]</span><br><span class="line"> [0.         1.         1.         0.83333333]</span><br><span class="line"> [0.5        0.5        1.         1.        ]]</span><br></pre></td></tr></table></figure><p>我们可以看到，之前的数据，特征一（ 90,60,75） 是比特征二（2,4,3）在数值上大很多的，那么，如果不做特征处理，直接带入模型处理的话，特征一显然就占决定性作用了，就没有特征二什么事情了。而进行归一化转换之后，特征一和特征二在数值上就在同一量级了，他们就变得“同等重要”了。</p><p><strong>归一化总结</strong>：注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。</p><p>对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变。那么，我们有没有好的解决办法呢？有的，那就是标准化。</p><h6 id="3-5-5、标准化"><a href="#3-5-5、标准化" class="headerlink" title="3.5.5、标准化"></a><strong>3.5.5、标准化</strong></h6><p><strong>特点</strong>：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内<br><strong>公式</strong> ：𝑋′= (𝑥−mean)/𝜎<br>其中，mean为平均值，𝜎为标准差(考量数据的稳定性)<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.5.png" alt="标准差"></p><p>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。</p><p><strong>sklearn特征处理API</strong> : scikit-learn.preprocessing.StandardScaler</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StandardScaler(…)</span><br><span class="line">    处理之后每列来说所有数据都聚集在均值0附近方差为1</span><br><span class="line"></span><br><span class="line">StandardScaler.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br><span class="line"></span><br><span class="line">StandardScaler.mean_</span><br><span class="line">    原始数据中每列特征的平均值</span><br><span class="line"></span><br><span class="line">StandardScaler.std_</span><br><span class="line">    原始数据每列特征的方差</span><br></pre></td></tr></table></figure><p><strong>标准化步骤</strong>：<br>1、实例化StandardScaler<br>2、通过fit_transform转换</p><p><strong>举个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">std = StandardScaler()</span><br><span class="line">data = std.fit_transform([[ 1., -1., 3.],[ 2., 4., 2.],[ 4., 6., -1.]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[-1.06904497 -1.35873244  0.98058068]</span><br><span class="line"> [-0.26726124  0.33968311  0.39223227]</span><br><span class="line"> [ 1.33630621  1.01904933 -1.37281295]]</span><br></pre></td></tr></table></figure><p><strong>标准化总结</strong>：标准化可以避免最大值，最小值发生异常值的干扰。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</p><p><strong>缺失值处理方法</strong><br><strong>删除</strong>：如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列。</p><p><strong>插补</strong>：可以通过缺失值每行或者每列的平均值、中位数来填充。（主要方法）</p><p><strong>sklearn缺失值API</strong>: sklearn.preprocessing.Imputer<br><strong>Imputer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, axis=0)</span><br><span class="line">    完成缺失值插补</span><br><span class="line"></span><br><span class="line">Imputer.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br></pre></td></tr></table></figure><p><strong>Imputer流程</strong>：<br>1、初始化Imputer,指定”缺失值”，指定填补策略，指定行或列<br>2、调用fit_transform</p><p>关于np.nan(np.NaN)<br>1、 numpy的数组中可以使用np.nan/np.NaN来代替缺失值，属于float类型。<br>2、如果是文件中的一些缺失值，可以替换成nan，通过np.array转化成float型的数组即可。</p><h5 id="3-6、数据的特征选择"><a href="#3-6、数据的特征选择" class="headerlink" title="3.6、数据的特征选择"></a><strong>3.6、数据的特征选择</strong></h5><h6 id="3-6-1、特征选择是什么？"><a href="#3-6-1、特征选择是什么？" class="headerlink" title="3.6.1、特征选择是什么？"></a><strong>3.6.1、特征选择是什么</strong>？</h6><p>特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。</p><h6 id="3-6-2、为什么要做特征选择？"><a href="#3-6-2、为什么要做特征选择？" class="headerlink" title="3.6.2、为什么要做特征选择？"></a><strong>3.6.2、为什么要做特征选择</strong>？</h6><p>冗余：部分特征的相关度高，容易消耗计算性能<br>噪声：部分特征对预测结果有负影响</p><h6 id="3-6-3、特征选择主要方法："><a href="#3-6-3、特征选择主要方法：" class="headerlink" title="3.6.3、特征选择主要方法："></a><strong>3.6.3、特征选择主要方法</strong>：</h6><p>Filter(过滤式):VarianceThreshold<br>Embedded(嵌入式)：正则化、决策树</p><h6 id="3-6-4、sklearn特征选择API"><a href="#3-6-4、sklearn特征选择API" class="headerlink" title="3.6.4、sklearn特征选择API"></a><strong>3.6.4、sklearn特征选择API</strong></h6><p>sklearn.feature_selection.VarianceThreshold<br>VarianceThreshold语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">VarianceThreshold(threshold = 0.0)</span><br><span class="line">    删除所有低方差特征</span><br><span class="line"></span><br><span class="line">Variance.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：训练集差异低于threshold的特征将被删除。</span><br><span class="line">    默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。</span><br></pre></td></tr></table></figure><h6 id="3-6-5、VarianceThreshold流程："><a href="#3-6-5、VarianceThreshold流程：" class="headerlink" title="3.6.5、VarianceThreshold流程："></a><strong>3.6.5、VarianceThreshold流程</strong>：</h6><p>1、初始化VarianceThreshold,指定阀值方差<br>2、调用fit_transform</p><h6 id="3-6-6、举个栗子："><a href="#3-6-6、举个栗子：" class="headerlink" title="3.6.6、举个栗子："></a><strong>3.6.6、举个栗子</strong>：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">var = VarianceThreshold(threshold=1.0)</span><br><span class="line">data = var.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0]</span><br><span class="line"> [4]</span><br><span class="line"> [1]]</span><br></pre></td></tr></table></figure><p>从栗子中，可以看到，把方差是0的第一个特征值（0,0,0），第4个特征值（3,3,3），和方差小于1的第2个特征值（2,1,2）都给删除了，只剩下（0,4,1）这个方差大于1的特征值。默认情况下，threshold 等于1.0 。</p><h5 id="3-7、降维-（PCA）"><a href="#3-7、降维-（PCA）" class="headerlink" title="3.7、降维 （PCA）"></a><strong>3.7、降维 （PCA）</strong></h5><h6 id="3-7-1、sklearn降维API-："><a href="#3-7-1、sklearn降维API-：" class="headerlink" title="3.7.1、sklearn降维API ："></a><strong>3.7.1、sklearn降维API</strong> ：</h6><p>sklearn. decomposition</p><h6 id="3-7-2、本质："><a href="#3-7-2、本质：" class="headerlink" title="3.7.2、本质："></a><strong>3.7.2、本质</strong>：</h6><p>PCA是一种分析、简化数据集的技术。</p><h6 id="3-7-3、目的："><a href="#3-7-3、目的：" class="headerlink" title="3.7.3、目的："></a><strong>3.7.3、目的</strong>：</h6><p>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</p><h6 id="3-7-4、作用："><a href="#3-7-4、作用：" class="headerlink" title="3.7.4、作用："></a><strong>3.7.4、作用</strong>：</h6><p>可以削减回归分析或者聚类分析中特征的数量。</p><h6 id="3-7-5、PCA语法："><a href="#3-7-5、PCA语法：" class="headerlink" title="3.7.5、PCA语法："></a><strong>3.7.5、PCA语法</strong>：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PCA(n_components=None)</span><br><span class="line">    将数据分解为较低维数空间</span><br><span class="line"></span><br><span class="line">PCA.fit_transform(X)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后指定维度的array</span><br></pre></td></tr></table></figure><h6 id="3-7-6、PCA流程："><a href="#3-7-6、PCA流程：" class="headerlink" title="3.7.6、PCA流程："></a><strong>3.7.6、PCA流程</strong>：</h6><p>1、初始化PCA,指定减少后的维度<br>2、调用fit_transform</p><h6 id="3-7-7、举个栗子"><a href="#3-7-7、举个栗子" class="headerlink" title="3.7.7、举个栗子"></a><strong>3.7.7、举个栗子</strong></h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">pca = PCA(n_components=0.9)</span><br><span class="line">data = pca.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,15,46]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 15.77507261]</span><br><span class="line"> [-15.11129418]</span><br><span class="line"> [ -0.66377843]]</span><br></pre></td></tr></table></figure><p>该栗子中，原本有4个特征的数据，变成了一个特征，并且该特征保留了原来90%的信息，n_components=0.9 。</p><h1 id="四、机器学习基础"><a href="#四、机器学习基础" class="headerlink" title="四、机器学习基础"></a>四、机器学习基础</h1><h5 id="4-1、机器学习开发流程"><a href="#4-1、机器学习开发流程" class="headerlink" title="4.1、机器学习开发流程"></a><strong>4.1、机器学习开发流程</strong></h5><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.6.png" alt="机器学习开发流程"><br>首先要根据原始数据明确问题做什么，建立模型： 根据数据类型划分应用种类；然后做数据的基本处理：（缺失值，合并表等）和特征工程（特征进行处理） （重要）；其次，找到合适的算法进行预测： 最后，对模型评估，根据模型的准确率，判定效果，如果合格 ，则上线使用，以API形式提供，如果不合格，则要换算法 或者重新提取特征工程，如此循环往复继续下去，直到得到满意的模型。</p><h5 id="4-2、机器学习模型是什么？"><a href="#4-2、机器学习模型是什么？" class="headerlink" title="4.2、机器学习模型是什么？"></a><strong>4.2、机器学习模型是什么</strong>？</h5><p><strong>定义</strong>：通过一种映射关系将输入值到输出值。<br>简单来讲，模型 = 算法 + 数据。<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.7.png" alt="模型"></p><h5 id="4-3、机器学习算法分类"><a href="#4-3、机器学习算法分类" class="headerlink" title="4.3、机器学习算法分类"></a><strong>4.3、机器学习算法分类</strong></h5><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.8.png" alt="算法分类"></p></div><footer><div class="categories"><a href="/categories/machinelearning/">机器学习</a></div><div class="tags"><a href="/tags/content/">content</a>, <a href="/tags/machine-learning/">machine learning</a></div><div class="addthis addthis_toolbox addthis_default_style"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a> <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a> <a class="addthis_counter addthis_pill_style"></a></div><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script><div class="clearfix"></div></footer></div></article><section id="comment"><h1 class="title">留言</h1><div id="fb-root"></div><script>!function(e,t,n){var a,c=e.getElementsByTagName(t)[0];e.getElementById(n)||((a=e.createElement(t)).id=n,a.src="//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345",c.parentNode.insertBefore(a,c))}(document,"script","facebook-jssdk")</script><div class="fb-comments" data-href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml1/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div></section></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>63</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>21</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>85</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>21</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔（简书）</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">机器学习（csdn博客）</a></li><li><a href="https://zhuanlan.zhihu.com/c_1182309165824901120" target="_blank">机器学习（知乎）</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li><li><a href="https://baippt.com/" target="_blank">ppt模板免费下载</a></li><li><a href="http://www.youneedcn.com/" target="_blank">你要的资源</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2020 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>