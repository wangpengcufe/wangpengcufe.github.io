<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>机器学习（四） 分类 | 机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="PDF电子书下载,电子书，PDF下载，电子书下载，PDF电子书免费下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载，电子书下载，PDF电子书免费下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:title" content="机器学习（四） 分类"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-ml-ml4" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-08-31T07:32:39.000Z"><a href="/machinelearning/ml-ml4/">2019-08-31</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="p-name title" itemprop="headline name">机器学习（四） 分类</h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="/machinelearning/ml-ml4/1.png" alt="图1"></p><a id="more"></a><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a><strong>分类</strong></h1><p>分类旨在将项目分为不同类别。 最常见的分类类型是二元分类，其中有两类，通常分别为正数和负数。 如果有两个以上的类别，则称为多类分类。 spark.mllib支持两种线性分类方法：线性支持向量机（SVM）和逻辑回归。 线性SVM仅支持二进制分类，而逻辑回归支持二进制和多类分类问题。 对于这两种方法，spark.mllib支持L1和L2正则化变体。 训练数据集由MLlib中LabeledPoint的RDD表示，其中标签是从零开始的类索引：0,1,2，….</p><h1 id="一、基本思想"><a href="#一、基本思想" class="headerlink" title="一、基本思想"></a><strong>一、基本思想</strong></h1><p>统计学习理论是在传统统计学基础上发展起来的一种机器学习方法 。SVM 的基本思想可由图 1说明 ，在二维两类线性可分情况下，有很多可能的线性分类器可以把这组数据分割开，但是只有一个使两类的分类间隔 margin最大，即图中的 H，这个线性分类器就是最优分类超平面，与其它分类器相比 ，具有更好的泛化性 。</p><p><img src="/machinelearning/ml-ml4/2.png" alt="最优分类超平面"></p><h1 id="二、计算公式"><a href="#二、计算公式" class="headerlink" title="二、计算公式"></a><strong>二、计算公式</strong></h1><p>假设超平面可描述为：<br><img src="/machinelearning/ml-ml4/3.png" alt="假设超平面公式"></p><p>线性SVM是大规模分类任务的标准方法。 其学习策略是使数据间的间隔最大化，最终可转化为一个凸二次规划问题的求解。</p><p>分类器的损失函数（hinge loss铰链损失）：</p><p>L(w;x,y):=max{0,1−ywTx}.</p><p>默认情况下，线性SVM使用L2正则化进行训练。 我们还支持替代L1正则化。 在这种情况下，问题变成线性程序。线性SVM算法输出SVM模型。 给定一个新的数据点，用x表示，该模型根据wTx的值进行预测。 默认情况下，如果wTx≥0则结果为正，否则为负。</p><h1 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a><strong>三、代码实现</strong></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import  org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import  org.apache.spark.mllib.classification.SVMModel;</span><br><span class="line">import  org.apache.spark.mllib.classification.SVMWithSGD;</span><br><span class="line">import  org.apache.spark.mllib.evaluation.BinaryClassificationMetrics;</span><br><span class="line">import org.apache.spark.mllib.linalg.Vectors;</span><br><span class="line">import  org.apache.spark.mllib.regression.LabeledPoint;</span><br></pre></td></tr></table></figure><h2 id="3-1、读取数据："><a href="#3-1、读取数据：" class="headerlink" title="3.1、读取数据："></a><strong>3.1、读取数据：</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new  SparkConf().setAppName(&quot;SVM&quot;).setMaster(&quot;local&quot;);</span><br><span class="line">JavaSparkContext sc = new  JavaSparkContext(conf);             </span><br><span class="line">JavaRDD&lt;String&gt; source =  sc.textFile(&quot;data/mllib/iris.data&quot;);</span><br></pre></td></tr></table></figure><p>用LabeledPoint来存储标签列和特征列。 LabeledPoint在监督学习中常用来存储标签和特征，其中要求标签的类型是double，特征的类型是Vector</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;LabeledPoint&gt; data =  source.map(line-&gt;&#123;            </span><br><span class="line">String[] parts = line.split(&quot;,&quot;);           </span><br><span class="line"> double label = 0.0;           </span><br><span class="line"> if(parts[4].equals(&quot;Iris-setosa&quot;)) &#123;                </span><br><span class="line">            label = 0.0;            </span><br><span class="line">&#125;else  if(parts[4].equals(&quot;Iris-versicolor&quot;)) &#123;        </span><br><span class="line">            label = 1.0;           </span><br><span class="line"> &#125;else &#123;              </span><br><span class="line">            label = 2.0;         </span><br><span class="line">&#125;           </span><br><span class="line"> return new  LabeledPoint(label,Vectors.dense(Double.parseDouble(parts[0]),                 </span><br><span class="line">                                              Double.parseDouble(parts[1]),                   </span><br><span class="line">                                              Double.parseDouble(parts[2]),                  </span><br><span class="line">                                              Double.parseDouble(parts[3])));       </span><br><span class="line"> &#125;);</span><br></pre></td></tr></table></figure><h2 id="3-2、-构建模型"><a href="#3-2、-构建模型" class="headerlink" title="3.2、 构建模型"></a><strong>3.2、 构建模型</strong></h2><p>因为SVM只支持2分类，所以我们要进行一下数据抽取，这里我们通过filter过滤掉第2类的数据，只选取第0类和第1类的数据。然后，我们把数据集划分成两部分，其中训练集占60%，测试集占40%</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;LabeledPoint&gt;[] filters =  data.filter(line-&gt;&#123; </span><br><span class="line">        return line.label()!=2; </span><br><span class="line">&#125;).randomSplit(new  double[]&#123;0.6,0.4&#125;,11L);</span><br><span class="line">JavaRDD&lt;LabeledPoint&gt; training =  filters[0].cache();</span><br><span class="line">JavaRDD&lt;LabeledPoint&gt; test = filters[1];</span><br></pre></td></tr></table></figure><p>接下来，通过训练集构建模型SVMWithSGD。这里的SGD即著名的随机梯度下降算法（Stochastic Gradient Descent）。设置迭代次数为1000，除此之外还有stepSize（迭代步伐大小），regParam（regularization正则化控制参数），miniBatchFraction（每次迭代参与计算的样本比例），initialWeights（weight向量初始值）等参数可以进行设置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//构建训练集 SVMWithSGD  </span><br><span class="line">// SGD即著名的随机梯度下降算法（Stochastic  Gradient Descent）  </span><br><span class="line">// 设置迭代次数为1000，  </span><br><span class="line">// 除此之外还有stepSize（迭代步伐大小），   </span><br><span class="line">// regParam（regularization正则化控制参数），  </span><br><span class="line">// miniBatchFraction（每次迭代参与计算的样本比例），  </span><br><span class="line">//initialWeights（weight向量初始值）等参数可以进行设置*/</span><br><span class="line">SVMModel model =  SVMWithSGD.train(training.rdd(), 1000);</span><br></pre></td></tr></table></figure><h2 id="3-3、-模型评估"><a href="#3-3、-模型评估" class="headerlink" title="3.3、 模型评估"></a><strong>3.3、 模型评估</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">//清除默认阈值，这样会输出原始的预测评分，即带有确信度的结果</span><br><span class="line">model.clearThreshold();   </span><br><span class="line">JavaRDD&lt;Tuple2&lt;Object,Object&gt;&gt;  scoreAndLabels = test.map(point-&gt;   </span><br><span class="line">    new  Tuple2&lt;&gt;(model.predict(point.features()),point.label()));</span><br><span class="line">    scoreAndLabels.foreach(x-&gt;&#123;          </span><br><span class="line">    System.out.println(x);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//输出结果:</span><br><span class="line">(-2.627551665051128,0.0)(-2.145161194882099,0.0)</span><br><span class="line">(-2.3068829871403618,0.0)(-3.0554378212130096,0.0)</span><br><span class="line">(-2.3698036980710446,0.0)(-2.335545287277434,0.0)</span><br><span class="line">(-2.6962358412306786,0.0)(-2.8222115665081975,0.0)</span><br><span class="line">(-3.5549967121975508,0.0)(-1.963540537080021,0.0)</span><br><span class="line">(-2.8307953180240637,0.0)(-3.5132621172293095,0.0)</span><br><span class="line">(-3.8139420880575643,0.0)(-2.6303719513181254,0.0)</span><br><span class="line">(-1.4913566958139257,0.0)(-2.5373343352394144,0.0)</span><br><span class="line">(-2.4271282983451896,0.0)(-2.6590342514551977,0.0)</span><br><span class="line">(3.2420043610860385,1.0)(3.5440500131703354,1.0)</span><br><span class="line">(3.067344577412759,1.0)(3.269179005035978,1.0)</span><br><span class="line">(2.141265211522379,1.0)(3.705816267306055,1.0)</span><br><span class="line">(4.418311047904414,1.0)(2.955773777046275,1.0)</span><br><span class="line">(4.117932735084642,1.0)(3.904874870539733,1.0)</span><br><span class="line">(2.061176997559964,1.0)(2.685256091027288,1.0)</span><br><span class="line">(3.210566236559426,1.0)(3.963262576277656,1.0)</span><br><span class="line">(3.299206068645311,1.0)(3.7974891199125067,1.0)...  ...</span><br></pre></td></tr></table></figure><p>那如果设置了阈值，则会把大于阈值的结果当成正预测，小于阈值的结果当成负预测。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.setThreshold(0.0); </span><br><span class="line">scoreAndLabels.foreach(x-&gt;&#123;   </span><br><span class="line">     System.out.println(x);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(0.0,0.0)(0.0,0.0)(0.0,0.0)</span><br><span class="line">(0.0,0.0)(0.0,0.0)(0.0,0.0)</span><br><span class="line">(0.0,0.0)(0.0,0.0)(0.0,0.0)</span><br><span class="line">(0.0,0.0)(0.0,0.0)(0.0,0.0)</span><br><span class="line">(0.0,0.0)(0.0,0.0)(0.0,0.0)</span><br><span class="line">(0.0,0.0)(0.0,0.0)(0.0,0.0)</span><br><span class="line">(1.0,1.0)(1.0,1.0)(1.0,1.0)</span><br><span class="line">(1.0,1.0)(1.0,1.0)(1.0,1.0)</span><br><span class="line">(1.0,1.0)(1.0,1.0)...  ...</span><br></pre></td></tr></table></figure><p>构建评估矩阵，把模型预测的准确性打印出来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BinaryClassificationMetrics metrics = new  BinaryClassificationMetrics(JavaRDD.toRDD(scoreAndLabels));</span><br><span class="line">System.out.println(&quot;Area under ROC =  &quot;+metrics.areaUnderROC());</span><br><span class="line">//结果打印：</span><br><span class="line">Area under ROC = 1.0</span><br></pre></td></tr></table></figure><p>其中， SVMWithSGD.train() 方法默认的通过把正则化参数设为1来执行来范数。如果我们想配置这个算法，可以通过创建一个新的 SVMWithSGD对象然后调用他的setter方法来进行重新配置。下面这个例子，我们构建了一个正则化参数为0.1的L1正则化SVM方法 ，然后迭代这个训练算法2000次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SVMWithSGD sgd = new SVMWithSGD();</span><br><span class="line">sgd.optimizer().setRegParam(0.1).setNumIterations(2000).setUpdater(new L1Updater());</span><br><span class="line">SVMModel modelL1 =  sgd.run(training.rdd());</span><br><span class="line">System.out.println(&quot;modelL1:&quot;+modelL1);</span><br><span class="line">//打印结果：</span><br><span class="line">modelL1:org.apache.spark.mllib.classification.SVMModel:</span><br><span class="line">intercept = 0.0,  numFeatures = 4, numClasses = 2, threshold = 0.0</span><br></pre></td></tr></table></figure><p>模型保存和加载：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.save(sc.sc(), &quot;data/mllib/FiveSVM&quot;);</span><br><span class="line">SVMModel sameModel =  SVMModel.load(sc.sc(), &quot;data/mllib/FiveSVM&quot;);</span><br></pre></td></tr></table></figure><h1 id="四、性质"><a href="#四、性质" class="headerlink" title="四、性质"></a><strong>四、性质</strong></h1><p>稳健性与稀疏性：SVM的优化问题同时考虑了经验风险和结构风险最小化，因此具有稳定性。从几何观点，SVM的稳定性体现在其构建超平面决策边界时要求边距最大，因此间隔边界之间有充裕的空间包容测试样本 。SVM使用铰链损失函数作为代理损失，铰链损失函数的取值特点使SVM具有稀疏性，即其决策边界仅由支持向量决定，其余的样本点不参与经验风险最小化 。在使用核方法的非线性学习中，SVM的稳健性和稀疏性在确保了可靠求解结果的同时降低了核矩阵的计算量和内存开销。</p><h1 id="五、应用"><a href="#五、应用" class="headerlink" title="五、应用"></a><strong>五、应用</strong></h1><p>SVM在各领域的模式识别问题中有广泛应用，包括人像识别（face recognition） 、文本分类（text categorization） 、笔迹识别（handwriting recognition） 、生物信息学 等。</p><h1 id="六、参考资料："><a href="#六、参考资料：" class="headerlink" title="六、参考资料："></a><strong>六、参考资料：</strong></h1><ul><li>1、张健沛[1], 徐华[1]. 支持向量机（SVM）主动学习方法研究与应用[J]. 计算机应用, 2004(1).</li><li>2、<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fmllib-linear-methods.html%23classification" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/mllib-linear-methods.html#classification</a></li><li>3、数据下载地址：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Farchive.ics.uci.edu%2Fml%2Fmachine-learning-databases%2Firis%2Firis.data" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a></li></ul></div><footer><div class="categories"><a href="/categories/machinelearning/">机器学习</a></div><div class="tags"><a href="/tags/content/">content</a>, <a href="/tags/machine-learning/">machine learning</a></div><div class="addthis addthis_toolbox addthis_default_style"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a> <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a> <a class="addthis_counter addthis_pill_style"></a></div><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script><div class="clearfix"></div></footer></div></article><section id="comment"><h1 class="title">留言</h1><div id="fb-root"></div><script>!function(e,t,n){var a,c=e.getElementsByTagName(t)[0];e.getElementById(n)||((a=e.createElement(t)).id=n,a.src="//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345",c.parentNode.insertBefore(a,c))}(document,"script","facebook-jssdk")</script><div class="fb-comments" data-href="http://www.wangpengcufe.com/machinelearning/ml-ml4/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div></section></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>63</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>13</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>77</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>13</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">wangpengcufe的csdn博客</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2019 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>