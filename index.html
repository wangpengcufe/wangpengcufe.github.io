<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="PDF电子书下载,电子书，PDF下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-main" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T06:42:05.000Z"><a href="/navigate/main/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/navigate/main/">文章汇总</a></h1></header><div class="e-content entry" itemprop="articleBody"><ul><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml1/">机器学习 (一) Spark MLlib介绍</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml2/">机器学习 (二) 基本数据类型</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml3/">机器学习 (三) 基本的统计工具</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml4/">机器学习 (四) 分类</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml5/">机器学习 (五) 逻辑斯蒂回归</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml6/">机器学习 (六) 决策树</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml7/">机器学习 (七) 奇异值分解-SVD</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml8/">机器学习 (八) 主成分分析-PCA</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml9/">机器学习 (九) 协同过滤算法</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml10/">机器学习 (十) 聚类</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml11/">机器学习 (十一) 机器学习工作流</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml12/">机器学习 (十二) 特征提取 TF-IDF</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml13/">机器学习 (十三) 特征抽取–CountVectorizer</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml14/">机器学习 (十四) 特征抽取–Word2Vec</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml15/">机器学习 (十五) 特征选择-卡方选择器</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml16/">机器学习 (十六) 特征变换-标签和索引的转化</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/python-python1/">案例（一） 利用RFM模型做用户价值分析</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/python-python2/">案例（二） 如何把python项目部署到linux服务器上</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml1/">python 机器学习（一）机器学习概述与特征工程</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml2/">python 机器学习（二）分类算法-k近邻算法</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml3/">python 机器学习（三）分类算法-朴素贝叶斯</a></p></li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-15T14:47:27.000Z"><a href="/read/ebooks/">2019-11-15</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks/">好书精选</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>掌握已知，探索未知，为日益精进而阅读！</p><p><a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=68bf1f82b97f7e775c9a2bf79650d7dfae359b2ea236dc4298408bcf41f055fb"><img border="0" src="//pub.idqqimg.com/wpa/images/group.png" alt="电子书分享" title="电子书分享"></a></p><p><strong>管理</strong></p><ul><li><a href="https://545c.com/file/21704187-407028472" target="view_window">《定位》 阿尔·里斯，杰克·特劳特</a></li><li><a href="https://545c.com/file/21704187-397370436" target="view_window">《反脆弱：从不确定性中获益》（美）纳西姆·尼古拉斯·塔勒布</a></li><li><a href="https://545c.com/file/21704187-397841620" target="view_window">《舍与得的人生经营课》 赵丽荣</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-hfsxy/">《哈佛商学院管理全书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》（美）德鲁克</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gxq/">《滚雪球：巴菲特和他的财富人生》-艾丽斯·施罗德</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-lj/">《雷军：从金山软件到小米手机》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-sycs/">《一本书读懂商业常识》 (董智轩)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zkl/">《自控力》凯利·麦格尼格尔</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-jycy/">《精益创业》(美)埃里克·莱斯</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bsjdpy/">《把时间当作朋友》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cywj/">《创业维艰 如何完成比难更难的事》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-glsj/">《聪明人是怎样管理时间的》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-jjl/">《拒绝力》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cjjy/">《超级记忆》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zqjd/">《如何做出正确决定》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yzsk/">《优质思考的力量》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-ydybs/">《如何阅读一本书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gxddybs/">《如何高效读懂一本书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-ambjy/">《稻盛和夫_阿米巴经营》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-jyxyzx/">《稻盛和夫-经营为什么需要哲学》</a></li></ul><p><strong>理财</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-xzf/">《薛兆丰经济学讲义》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cfxlx/">《财富心理学》(金圣荣)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qbbfbb/">《穷爸爸富爸爸》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qclbd/">《穷查理宝典 查理·芒格智慧箴言录》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》(墨知行)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qmz/">《给穷忙族看的理财书》(比尔李)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qrqkd/">《穷人穷口袋，富人富脑袋》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-ffrs/">《富足人生的八大支柱 别为金钱焦虑丛书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yxzq/">《用心挣钱 用脑花钱》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-dytj/">《第一桶金：改变命运的68个创业传奇》（英涛）</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-glmyfq/">《如何有效管理每一分钱：用会计思维增值你的财富》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-fldz/">《复利打造百万富翁：持续被动收入，通往财务自由》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-lcrms/">《我的第一本理财入门书 畅销珍藏版》(武庆新)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cfdfp/">《财富的分配》（美）克拉克</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bblfh/">《巴比伦富豪 理财的十二条黄金定律》（美）乔治.克拉森</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wzqgz/">《白领理财日记之玩转钱规则》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bftzd/">《巴菲特之道（原书第3版）》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bft/">《一个美国资本家的成长—世界首富沃伦·巴菲特传》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-xzb/">《小资本赚钱100招》</a></li></ul><p><strong>营销</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-gkwsm/">《顾客为什么购买》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bk/">《爆款：如何打造超级IP》</a></li></ul><p><strong>金融</strong></p><ul><li><a href="https://545c.com/file/21704187-407028478" target="view_window">《货币简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-ddcb/">《一本书读懂财报》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-xdjjx/">《小岛经济学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wdgsrs/">《荣辱二十年 我的股市人生》(阚治东)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-hdjjx/">《海盗经济学》（美）彼得·里森</a></li></ul><p><strong>家庭</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》（美）罗兰·米勒</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-fblgt/">《非暴力沟通》（美）卢森堡</a></li><li><a href="https://545c.com/file/21704187-397349671" target="view_window">《正面管教》（美）简·尼尔森</a></li></ul><p><strong>心理</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-xwx/">《行为心理学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-whzz/">《乌合之众：大众心理研究》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-hzxl/">《如何把握孩子心理》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-flyd/">《弗洛伊德，性学与爱情心理学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cjgt/">《超级沟通心理学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yxl/">《影响力》</a></li></ul><p><strong>科技</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-rljs/">《人类简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wwjs/">《万物简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wljs/">《未来简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-sjjs/">《时间简史》</a></li></ul><p><strong>计算机</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-tjjqxx/">《图解机器学习》</a></li></ul><p><strong>历史</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-sj/">《史记》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zgf/">《曾国藩：又笨又慢平天下》</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-download" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T08:00:27.000Z"><a href="/datadownload/download/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/datadownload/download/">电子书</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><strong>阅读Top10</strong></p><ul><li>1、<a href="http://www.wangpengcufe.com/read/ebooks-rljs/">《人类简史》</a></li><li>2、<a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》</a></li><li>3、<a href="https://545c.com/file/21704187-397349671" target="view_window"> 《正面管教》</a></li><li>4、<a href="https://545c.com/file/21704187-398029061" target="view_window"> 《沉思录》</a></li><li>5、<a href="http://www.wangpengcufe.com/read/ebooks-xdjjx/">《小岛经济学》</a></li><li>6、<a href="http://www.wangpengcufe.com/read/ebooks-ddcb/">《一本书读懂财报》</a></li><li>7、<a href="http://www.wangpengcufe.com/read/ebooks-sjjs/">《时间简史》</a></li><li>8、<a href="http://www.wangpengcufe.com/read/ebooks-qbbfbb/">《穷爸爸富爸爸》</a></li><li>9、<a href="http://www.wangpengcufe.com/read/ebooks-bsjdpy/">《把时间当作朋友》</a></li><li>10、<a href="http://www.wangpengcufe.com/read/ebooks-xwx/">《行为心理学》</a></li></ul><p><strong>更多阅读</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks/">好书精选</a></li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml3" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-02-04T16:22:16.000Z"><a href="/machinelearning/pythonml-pythonml3/">2020-02-05</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml3/">python机器学习（三）分类算法-朴素贝叶斯</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/pythonml3.1.png" alt="朴素贝叶斯"></p><h1 id="一、概率基础"><a href="#一、概率基础" class="headerlink" title="一、概率基础"></a><strong>一、概率基础</strong></h1><p><strong>概率定义：</strong><br>概率定义为一件事情发生的可能性，例如，随机抛硬币，正面朝上的概率。</p><p><strong>联合概率：</strong><br>包含多个条件，且所有条件同时成立的概率，记作：𝑃(𝐴,𝐵) 。</p><p><strong>条件概率：</strong><br>事件A在另外一个事件B已经发生条件下的发生概率，记作：𝑃(𝐴|𝐵) 。P(A1,A2|B) = P(A1|B)P(A2|B)，需要注意的是：此条件概率的成立，是由于A1,A2相互独立的结果。</p><h1 id="二、朴素贝叶斯介绍"><a href="#二、朴素贝叶斯介绍" class="headerlink" title="二、朴素贝叶斯介绍"></a><strong>二、朴素贝叶斯介绍</strong></h1><p><strong>公式：</strong><br><img src="http://wangpengcufe.com/pythonml3.2.png" alt="朴素贝叶斯公式"><br>其中，w为给定文档的特征值(频数统计,预测文档提供)，c为文档类别。<br>公式可以理解为：</p><p><img src="http://wangpengcufe.com/pythonml3.3.png" alt="朴素贝叶斯公式的理解"><br>其中c可以是不同类别。</p><p>公式分为三个部分：</p><p><strong>𝑃(𝐶)</strong>：每个文档类别的概率(某文档类别词数／总文档词数)<br><strong>𝑃(𝑊│𝐶)</strong>：给定类别下特征（被预测文档中出现的词）的概率<br>计算方法：𝑃(𝐹1│𝐶)=𝑁𝑖/𝑁 （训练文档中去计算）<br>𝑁𝑖为该𝐹1词在C类别所有文档中出现的次数<br>N为所属类别C下的文档所有词出现的次数和<br><strong>𝑃(𝐹1,𝐹2,…)</strong>: 预测文档中每个词的概率</p><p><strong>举个栗子：</strong></p><p>现有一篇被预测文档：出现了都江宴，武汉，武松，计算属于历史，地理的类别概率？<br><img src="http://wangpengcufe.com/pythonml3.4.png" alt="image"><br>历史：𝑃(都江宴，武汉，武松│历史)∗P(历史)=（10/108）∗（22/108）∗（65/108）∗(108/235) =0.00563435<br>地理：𝑃(都江宴，武汉，武松│地理)∗P(地理)=（58/127）∗（17/127）∗（0/127）∗(127/235)=0</p><p><strong>拉普拉斯平滑：</strong><br>思考：属于某个类别为0，合适吗？<br>从上面的例子我们得到地理概率为0，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零。<br>解决方法：拉普拉斯平滑系数。<br><img src="http://wangpengcufe.com/pythonml3.5.png" alt="image"><br>𝛼为指定的系数一般为1，m为训练文档中统计出的特征词个数</p><p><strong>sklearn朴素贝叶斯实现API：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</span><br><span class="line">alpha：拉普拉斯平滑系数</span><br></pre></td></tr></table></figure><p><strong>案例：新闻分类</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=&apos;all&apos;)</span><br><span class="line"># 进行数据分割</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)</span><br><span class="line"># 对数据集进行特征抽取</span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line"># 以训练集当中的词的列表进行每篇文章重要性统计[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;]</span><br><span class="line">x_train = tf.fit_transform(x_train)</span><br><span class="line">x_test = tf.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 进行朴素贝叶斯算法的预测</span><br><span class="line">mlt = MultinomialNB(alpha=1.0)</span><br><span class="line">print(x_train)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(0, 120993)	0.0838226531816039</span><br><span class="line">(0, 36277)	0.028728297074726128</span><br><span class="line">(0, 118261)	0.051733692584494416</span><br><span class="line">(0, 118605)	0.08660213360333731</span><br><span class="line">(0, 78914)	0.10725171098177662</span><br><span class="line">(0, 120174)	0.07226288195761017</span><br><span class="line">(0, 146730)	0.03649798864200877</span><br><span class="line">(0, 49960)	0.09535813190987927</span><br><span class="line">(0, 108029)	0.10406938034117505</span><br><span class="line">(0, 151947)	0.1081016719923428</span><br><span class="line">(0, 120110)	0.13513684031456163</span><br><span class="line">(0, 34588)	0.06453595223748614</span><br><span class="line">(0, 133893)	0.04993313285348771</span><br><span class="line">(0, 31218)	0.07845873103784344</span><br><span class="line">(0, 108032)	0.08430822316250115</span><br><span class="line">(0, 30921)	0.11806736198114927</span><br><span class="line">(0, 33267)	0.030864914635712264</span><br><span class="line">(0, 36137)	0.0714722249527062</span><br><span class="line">(0, 57776)	0.07110907374703304</span><br><span class="line">(0, 77937)	0.026514922107534245</span><br><span class="line">(0, 90944)	0.09746338158610199</span><br><span class="line">(0, 135824)	0.09394365947415394</span><br><span class="line">(0, 49956)	0.09183375914922258</span><br><span class="line">(0, 151957)	0.07203295034824395</span><br><span class="line">(0, 33356)	0.07203295034824395</span><br><span class="line">:	:</span><br><span class="line">(14133, 45099)	0.030803124311834594</span><br><span class="line">(14133, 135309)	0.02305588722190138</span><br><span class="line">(14133, 135472)	0.06570104508511963</span><br><span class="line">(14133, 52014)	0.05222321951090842</span><br><span class="line">(14133, 108029)	0.05584161408783517</span><br><span class="line">(14133, 36137)	0.07670122356304401</span><br><span class="line">(14133, 34063)	0.12187079805145053</span><br><span class="line">(14133, 106978)	0.0851182715752145</span><br><span class="line">(14133, 106534)	0.03378056586331488</span><br><span class="line">(14133, 105921)	0.09707364301640503</span><br><span class="line">(14133, 103839)	0.07144955527096918</span><br><span class="line">(14133, 136535)	0.03801377630817533</span><br><span class="line">(14133, 42966)	0.028558472354146207</span><br><span class="line">(14133, 81075)	0.02180715538325887</span><br><span class="line">(14133, 135641)	0.025875408277197205</span><br><span class="line">(14133, 148185)	0.028450089379106706</span><br><span class="line">(14133, 78894)	0.020030955308174968</span><br><span class="line">(14133, 147914)	0.047259202253661425</span><br><span class="line">(14133, 90152)	0.017166154294786778</span><br><span class="line">(14133, 45598)	0.05645818387150284</span><br><span class="line">(14133, 135325)	0.03667700550640032</span><br><span class="line">(14133, 118218)	0.02343357701502816</span><br><span class="line">(14133, 131632)	0.01710795977554328</span><br><span class="line">(14133, 59957)	0.0485327006460036</span><br><span class="line">(14133, 67480)	0.01710795977554328</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mlt.fit(x_train, y_train)   #MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span><br><span class="line">y_predict = mlt.predict(x_test)</span><br><span class="line">print(&quot;预测的文章类别为：&quot;, y_predict)</span><br><span class="line">#预测的文章类别为： [ 3 16  5 ...  0  5  8]</span><br><span class="line"># 得出准确率</span><br><span class="line">print(&quot;准确率为：&quot;, mlt.score(x_test, y_test))</span><br><span class="line">#准确率为： 0.8414685908319185</span><br><span class="line">print(&quot;每个类别的精确率和召回率：&quot;, classification_report(y_test, y_predict, target_names=news.target_names))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">每个类别的精确率和召回率：                           precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.89      0.75      0.81       210</span><br><span class="line">           comp.graphics       0.87      0.81      0.84       225</span><br><span class="line"> comp.os.ms-windows.misc       0.77      0.90      0.83       209</span><br><span class="line">comp.sys.ibm.pc.hardware       0.77      0.78      0.78       258</span><br><span class="line">   comp.sys.mac.hardware       0.86      0.88      0.87       223</span><br><span class="line">          comp.windows.x       0.97      0.76      0.85       260</span><br><span class="line">            misc.forsale       0.92      0.68      0.78       233</span><br><span class="line">               rec.autos       0.91      0.89      0.90       263</span><br><span class="line">         rec.motorcycles       0.94      0.96      0.95       260</span><br><span class="line">      rec.sport.baseball       0.93      0.92      0.92       230</span><br><span class="line">        rec.sport.hockey       0.89      0.97      0.93       234</span><br><span class="line">               sci.crypt       0.64      0.99      0.78       235</span><br><span class="line">         sci.electronics       0.94      0.68      0.79       275</span><br><span class="line">                 sci.med       0.96      0.89      0.93       241</span><br><span class="line">               sci.space       0.89      0.97      0.93       246</span><br><span class="line">  soc.religion.christian       0.56      0.99      0.72       257</span><br><span class="line">      talk.politics.guns       0.84      0.94      0.89       256</span><br><span class="line">   talk.politics.mideast       0.92      0.98      0.94       245</span><br><span class="line">      talk.politics.misc       0.98      0.67      0.80       182</span><br><span class="line">      talk.religion.misc       1.00      0.17      0.29       170</span><br><span class="line"></span><br><span class="line">                accuracy                           0.84      4712</span><br><span class="line">               macro avg       0.87      0.83      0.83      4712</span><br><span class="line">            weighted avg       0.87      0.84      0.84      4712</span><br></pre></td></tr></table></figure><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a><strong>三、总结</strong></h1><p><strong>优点：</strong></p><ul><li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li><li>分类准确度高，速度快</li></ul><p><strong>缺点：</strong></p><ul><li>需要知道先验概率P(F1,F2,…|C)，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。</li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml2" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-21T09:02:06.000Z"><a href="/machinelearning/pythonml-pythonml2/">2020-01-21</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml2/">python 机器学习（二）分类算法-k近邻算法</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/pythonml2.1.png" alt="分类算法-K近邻算法"></p><h1 id="一、什么是K近邻算法？"><a href="#一、什么是K近邻算法？" class="headerlink" title="一、什么是K近邻算法？"></a><strong>一、什么是K近邻算法</strong>？</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义:"></a><strong>定义:</strong></h2><p>如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><h2 id="来源"><a href="#来源" class="headerlink" title="来源:"></a><strong>来源:</strong></h2><p>KNN算法最早是由Cover和Hart提出的一种分类算法.</p><h2 id="计算距离公式"><a href="#计算距离公式" class="headerlink" title="计算距离公式:"></a><strong>计算距离公式:</strong></h2><p>两个样本的距离可以通过如下公式计算，又叫欧式距离。<br>比如说，a(a1,a2,a3),b(b1,b2,b3)<br><img src="http://wangpengcufe.com/pythonml2.2.png" alt="欧式距离"></p><h1 id="二、K近邻算法的实现"><a href="#二、K近邻算法的实现" class="headerlink" title="二、K近邻算法的实现"></a><strong>二、K近邻算法的实现</strong></h1><h2 id="sk-learn近邻算法API"><a href="#sk-learn近邻算法API" class="headerlink" title="sk-learn近邻算法API"></a><strong>sk-learn近邻算法API</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=&apos;auto&apos;)</span><br><span class="line">n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数 </span><br><span class="line">algorithm：&#123;‘auto’，‘ball_tree’，‘kd_tree’，‘brute’&#125;，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)</span><br></pre></td></tr></table></figure><h2 id="近邻算法实例"><a href="#近邻算法实例" class="headerlink" title="近邻算法实例"></a><strong>近邻算法实例</strong></h2><p>案例背景：（kaggle地址：<a href="https://www.kaggle.com/c/facebook-v-predicting-check-ins/overview）" target="_blank" rel="noopener">https://www.kaggle.com/c/facebook-v-predicting-check-ins/overview）</a><br><img src="http://wangpengcufe.com/pythonml2.3.png" alt="预测入住"></p><p>数据下载地址：<a href="https://storage.googleapis.com/kagglesdsdata/competitions/5186/37497/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1579794270&Signature=dUw4i0K1VJJ7UfiSVvXmPiMRhx9v2aPh1eJagNdOQIpdM%2F7CjfhO7K3VLg5Oxd8%2BD%2B9XJqggolwF63CsmOLEPEyBb5BL7g6YRriltjCf1gwJUFx3u2ax6dfjfsyr%2FY4x5hKrrRSpqFPwd4SN9TzUrwcMf7erFxreIpWrO8peG7T%2Fw1EyxNkbH0NGBgYEZ20n0TgYstGGS30fjdoB8mus%2B747tNaWsudQXv5MCr9XRdC8IT95klZ8R5pqNb5tMoIKYaZRwmM5N2clianWQ1knAALW%2Fmpa536gQM%2BbLDzGCUX48rLbwHiZYGE45EEAEGtNdnwM0CUOBojJ2c7UfoeK8g%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv.zip" target="_blank" rel="noopener">train.csv</a></p><p>数据格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">            row_id       x       y  accuracy    time    place_id</span><br><span class="line">0                0  0.7941  9.0809        54  470702  8523065625</span><br><span class="line">1                1  5.9567  4.7968        13  186555  1757726713</span><br><span class="line">2                2  8.3078  7.0407        74  322648  1137537235</span><br><span class="line">3                3  7.3665  2.5165        65  704587  6567393236</span><br><span class="line">4                4  4.0961  1.1307        31  472130  7440663949</span><br><span class="line">...            ...     ...     ...       ...     ...         ...</span><br><span class="line">29118016  29118016  6.5133  1.1435        67  399740  8671361106</span><br><span class="line">29118017  29118017  5.9186  4.4134        67  125480  9077887898</span><br><span class="line">29118018  29118018  2.9993  6.3680        67  737758  2838334300</span><br><span class="line">29118019  29118019  4.0637  8.0061        70  764975  1007355847</span><br><span class="line">29118020  29118020  7.4523  2.0871        17  102842  7028698129</span><br><span class="line"></span><br><span class="line">[29118021 rows x 6 columns]</span><br></pre></td></tr></table></figure><p>实现思路：<br>1、数据集的处理(缩小数据集范围，处理日期数据，增加分割的日期数据，删除没用的日期数据，将签到位置少于n个用户的删除)<br>2、分割数据集<br>3、对数据集进行标准化<br>4、estimator流程进行分类预测</p><p>具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"># 读取数据</span><br><span class="line">data = pd.read_csv(&quot;./data/FBlocation/train.csv&quot;)</span><br><span class="line"># 处理数据</span><br><span class="line"># 1、缩小数据,查询数据集范围</span><br><span class="line">data = data.query(&quot;x &gt; 1.0 &amp;  x &lt; 1.25 &amp; y &gt; 2.5 &amp; y &lt; 2.75&quot;)</span><br><span class="line"># 处理时间的数据</span><br><span class="line">time_value = pd.to_datetime(data[&apos;time&apos;], unit=&apos;s&apos;)</span><br><span class="line"> # 把日期格式转换成 字典格式</span><br><span class="line">time_value = pd.DatetimeIndex(time_value)</span><br><span class="line"># 构造一些特征</span><br><span class="line">data[&apos;day&apos;] = time_value.day</span><br><span class="line">data[&apos;hour&apos;] = time_value.hour</span><br><span class="line">data[&apos;weekday&apos;] = time_value.weekday</span><br><span class="line"># 把时间戳特征删除</span><br><span class="line">data = data.drop([&apos;time&apos;], axis=1)</span><br><span class="line"># 把签到数量少于n个目标位置删除</span><br><span class="line">place_count = data.groupby(&apos;place_id&apos;).count()</span><br><span class="line">tf = place_count[place_count.row_id &gt; 3].reset_index()</span><br><span class="line">data = data[data[&apos;place_id&apos;].isin(tf.place_id)]</span><br><span class="line"></span><br><span class="line"># 取出数据当中的特征值和目标值</span><br><span class="line">y = data[&apos;place_id&apos;]</span><br><span class="line">x = data.drop([&apos;place_id&apos;], axis=1)</span><br><span class="line"># 进行数据的分割训练集合测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)</span><br><span class="line"># 特征工程（标准化）</span><br><span class="line">std = StandardScaler()</span><br><span class="line"># 对测试集和训练集的特征值进行标准化</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test)</span><br><span class="line"># 进行算法流程 # 超参数</span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line">knn.fit(x_train, y_train)</span><br><span class="line">y_predict = knn.predict(x_test)</span><br><span class="line">print(&quot;预测的目标签到位置为：&quot;, y_predict)</span><br><span class="line">print(&quot;预测的准确率:&quot;, knn.score(x_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">预测的目标签到位置为： [8258328058 2355236719 6683426742 ... 5606572086 4932578245 9237487147]</span><br><span class="line">预测的准确率: 0.3959810874704492</span><br></pre></td></tr></table></figure><h2 id="思考问题"><a href="#思考问题" class="headerlink" title="思考问题"></a><strong>思考问题</strong></h2><p>1、k值取多大？有什么影响？<br>2、性能问题？</p><h1 id="三、K近邻算法总结"><a href="#三、K近邻算法总结" class="headerlink" title="三、K近邻算法总结"></a><strong>三、K近邻算法总结</strong></h1><h2 id="K近邻算法优缺点"><a href="#K近邻算法优缺点" class="headerlink" title="K近邻算法优缺点"></a><strong>K近邻算法优缺点</strong></h2><p><strong>优点</strong>：<br>简单，易于理解，易于实现，无需估计参数，无需训练</p><p><strong>缺点</strong></p><ul><li>懒惰算法，对测试样本分类时的计算量大，内存开销大</li><li>必须指定K值，K值选择不当则分类精度不能保证</li></ul><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h2><p>小数据场景，几千～几万样本，具体场景具体业务去测试</p><h1 id="四、分类模型的评估"><a href="#四、分类模型的评估" class="headerlink" title="四、分类模型的评估"></a><strong>四、分类模型的评估</strong></h1><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a><strong>评估方法</strong></h2><p>estimator.score()<br>一般最常见使用的是准确率，即预测结果正确的百分比</p><p><strong>混淆矩阵</strong><br>在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)<br><img src="http://wangpengcufe.com/pythonml2.4.png" alt="混淆矩阵"><br><strong>精确率（Precision）</strong>：预测结果为正例样本中真实为正例的比例（查得准）<br><img src="http://wangpengcufe.com/pythonml2.5.png" alt="精确率（Precision）"><br><strong>召回率(Recall)</strong>：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）<br><img src="http://wangpengcufe.com/pythonml2.6.png" alt="召回率(Recall)"><br>其他分类标准，<strong>F1-score</strong>，反映了模型的稳健型<br><img src="http://wangpengcufe.com/pythonml2.7.png" alt="F1-score"></p><h2 id="分类模型评估API"><a href="#分类模型评估API" class="headerlink" title="分类模型评估API"></a><strong>分类模型评估API</strong></h2><p>评估API ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report</span><br></pre></td></tr></table></figure><p>用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report(y_true, y_pred, target_names=None)</span><br><span class="line"></span><br><span class="line">y_true：真实目标值</span><br><span class="line"></span><br><span class="line">y_pred：估计器预测目标值</span><br><span class="line"></span><br><span class="line">target_names：目标类别名称</span><br><span class="line"></span><br><span class="line">return：每个类别精确率与召回率</span><br></pre></td></tr></table></figure></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml1" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-19T02:06:06.000Z"><a href="/machinelearning/pythonml-pythonml1/">2020-01-19</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml1/">python 机器学习（一）机器学习概述与特征工程</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80.png" alt="机器学习概述与特征工程"></p><h1 id="一、机器学习概述"><a href="#一、机器学习概述" class="headerlink" title="一、机器学习概述"></a>一、机器学习概述</h1><h5 id="1-1、什么是机器学习？"><a href="#1-1、什么是机器学习？" class="headerlink" title="1.1、什么是机器学习？"></a><strong>1.1、什么是机器学习</strong>？</h5><p>机器学习是从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测</p><h5 id="1-2、为什么需要机器学习？"><a href="#1-2、为什么需要机器学习？" class="headerlink" title="1.2、为什么需要机器学习？"></a><strong>1.2、为什么需要机器学习</strong>？</h5><ul><li>解放生产力，智能客服，可以不知疲倦的24小时作业</li><li>解决专业问题，ET医疗，帮助看病</li><li>提供社会便利，例如杭州的城市大脑<h5 id="1-3、机器学习应用场景"><a href="#1-3、机器学习应用场景" class="headerlink" title="1.3、机器学习应用场景"></a><strong>1.3、机器学习应用场景</strong></h5></li><li>自然语言处理</li><li>无人驾驶</li><li>计算机视觉</li><li>推荐系统<h1 id="二、数据来源与类型"><a href="#二、数据来源与类型" class="headerlink" title="二、数据来源与类型"></a>二、数据来源与类型</h1><h5 id="2-1、数据的来源"><a href="#2-1、数据的来源" class="headerlink" title="2.1、数据的来源"></a><strong>2.1、数据的来源</strong></h5></li><li>企业日益积累的大量数据（互联网公司更为显著）</li><li>政府掌握的各种数据</li><li>科研机构的实验数据<h5 id="2-2、数据的类型"><a href="#2-2、数据的类型" class="headerlink" title="2.2、数据的类型"></a><strong>2.2、数据的类型</strong></h5>数据的类型将是机器学习模型不同问题不同处理的依据。数据的类型包括：</li></ul><p><strong>离散型数据</strong>：由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。</p><p><strong>连续型数据</strong>：变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。</p><p><strong>注意</strong>：</p><ul><li>只要记住一点，离散型是区间内不可分，连续型是区间内可分</li></ul><h5 id="2-3、可用的数据集"><a href="#2-3、可用的数据集" class="headerlink" title="2.3、可用的数据集"></a><strong>2.3、可用的数据集</strong></h5><p><strong>scikit-learn</strong>：数据量较小 ，方便学习。<br><strong>UCI</strong>：收录了360个数据集，覆盖科学、生活、经济等领域 ，数据量几十万。<br><strong>Kaggle</strong>：大数据竞赛平台，80万科学家，真实数据，数据量巨大。</p><p>常用数据集数据的结构组成：特征值+目标值，如下图：<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.1.png" alt="常用数据集结构"></p><h1 id="三、数据的特征工程"><a href="#三、数据的特征工程" class="headerlink" title="三、数据的特征工程"></a>三、数据的特征工程</h1><h5 id="3-1、特征工程是什么？"><a href="#3-1、特征工程是什么？" class="headerlink" title="3.1、特征工程是什么？"></a><strong>3.1、特征工程是什么</strong>？</h5><p>特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。</p><h5 id="3-2、特征工程的意义"><a href="#3-2、特征工程的意义" class="headerlink" title="3.2、特征工程的意义"></a><strong>3.2、特征工程的意义</strong></h5><p><strong>意义</strong>：直接影响模型的预测结果。</p><h5 id="3-3、scikit-learn"><a href="#3-3、scikit-learn" class="headerlink" title="3.3、scikit-learn"></a><strong>3.3、scikit-learn</strong></h5><ul><li>Python语言的机器学习工具</li><li>Scikit-learn包括许多知名的机器学习算法的实现</li><li>Scikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎。</li><li>目前稳定版本0.18</li><li>安装：pip3 install Scikit-learn</li><li>引用：import sklearn</li></ul><h5 id="3-4、数据的特征抽取"><a href="#3-4、数据的特征抽取" class="headerlink" title="3.4、数据的特征抽取"></a><strong>3.4、数据的特征抽取</strong></h5><h6 id="3-4-1、特点："><a href="#3-4-1、特点：" class="headerlink" title="3.4.1、特点："></a><strong>3.4.1、特点</strong>：</h6><ul><li>特征抽取针对非连续型数据</li><li>特征抽取对文本等进行特征值化</li></ul><h6 id="3-4-2、sklearn特征抽取API"><a href="#3-4-2、sklearn特征抽取API" class="headerlink" title="3.4.2、sklearn特征抽取API :"></a><strong>3.4.2、sklearn特征抽取API</strong> :</h6><p>sklearn.feature_extraction</p><h6 id="3-4-3、字典特征抽取"><a href="#3-4-3、字典特征抽取" class="headerlink" title="3.4.3、字典特征抽取 :"></a><strong>3.4.3、字典特征抽取</strong> :</h6><p><strong>作用</strong>：对字典数据进行特征值化<br><strong>类</strong>：sklearn.feature_extraction.DictVectorizer<br><strong>DictVectorizer语法</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DictVectorizer(sparse=True,…)</span><br><span class="line"></span><br><span class="line">DictVectorizer.fit_transform(X)       </span><br><span class="line">    X:字典或者包含字典的迭代器</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">DictVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">DictVectorizer.get_feature_names()</span><br><span class="line">    返回类别名称</span><br><span class="line"></span><br><span class="line">DictVectorizer.transform(X)</span><br><span class="line">    按照原先的标准转换</span><br></pre></td></tr></table></figure><p><strong>流程</strong>：<br>1、实例化类DictVectorizer<br>2、调用fit_transform方法输入数据并转换</p><p><strong>举一个栗子</strong>：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction import DictVectorizer</span><br><span class="line">dict = DictVectorizer(sparse=False)</span><br><span class="line">data = dict.fit_transform([&#123;&apos;name&apos;: &apos;张飞&apos;,&apos;score&apos;: 70&#125;, &#123;&apos;name&apos;: &apos;赵云&apos;,&apos;score&apos;:100&#125;, &#123;&apos;name&apos;: &apos;刘备&apos;,&apos;score&apos;: 98&#125;])</span><br><span class="line">print(dict.get_feature_names())</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p></p><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[&apos;name=刘备&apos;, &apos;name=张飞&apos;, &apos;name=赵云&apos;, &apos;score&apos;]</span><br><span class="line">[[  0.   1.   0.  70.]</span><br><span class="line"> [  0.   0.   1. 100.]</span><br><span class="line"> [  1.   0.   0.  98.]]</span><br></pre></td></tr></table></figure><p>从中，我们可以看出：对于字典 [{‘name’: ‘张飞’,’score’: 70}, {‘name’: ‘赵云’,’score’:100}, {‘name’: ‘刘备’,’score’: 98}] ，DictVectorizer类将汉字（张飞，赵云，刘备）转成了one-hot编码（0,1,0），而数值类型的数据（70,100,98）是不做处理的。</p><p>什么是one-hot编码？<br>One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。</p><h6 id="3-4-4、文本特征抽取"><a href="#3-4-4、文本特征抽取" class="headerlink" title="3.4.4、文本特征抽取"></a><strong>3.4.4、文本特征抽取</strong></h6><p><strong>作用</strong>：对文本数据进行特征值化<br><strong>类</strong>：sklearn.feature_extraction.text.CountVectorizer<br><strong>CountVectorizer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CountVectorizer(max_df=1.0,min_df=1,…)</span><br><span class="line">    返回词频矩阵</span><br><span class="line"></span><br><span class="line">CountVectorizer.fit_transform(X,y)       </span><br><span class="line">    X:文本或者包含文本字符串的可迭代对象</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">CountVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">CountVectorizer.get_feature_names()</span><br><span class="line">    返回值:单词列表</span><br></pre></td></tr></table></figure><p><strong>流程</strong>:<br>1、实例化类CountVectorizer<br>2、调用fit_transform方法输入数据并转换</p><p><strong>举一个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;我爱学习，学习使我进步&quot;, &quot;我爱work，work 使我快乐&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;work&apos;, &apos;使我快乐&apos;, &apos;学习使我进步&apos;, &apos;我爱work&apos;, &apos;我爱学习&apos;]</span><br><span class="line">[[0 0 1 0 1]</span><br><span class="line"> [1 1 0 1 0]]</span><br></pre></td></tr></table></figure><p>可以看到，API中的CountVectorizer类将中文转换成了单个词语，并给每个词语的出现个数进行了统计。有一点要注意的是，程序并不会给中文分词，所以，例子中，‘学习使我进步’程序认为是一个词语，这种情况下，可以用空格，或者逗号，将中文进行分割。还有一点要注意的是，<strong>如果是英文的话，是不会统计单个字母的，因为字母的统计是没有意义的，同理，CountVectorizer也不支持单个中文字</strong>。</p><p>我们可以验证一下栗子：<br><strong>英文栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;I like study , study makes me happy&quot;, &quot;I am a good student&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;am&apos;, &apos;good&apos;, &apos;happy&apos;, &apos;like&apos;, &apos;makes&apos;, &apos;me&apos;, &apos;student&apos;, &apos;study&apos;]</span><br><span class="line">[[0 0 1 1 1 1 0 2]</span><br><span class="line"> [1 1 0 0 0 0 1 0]]</span><br></pre></td></tr></table></figure><p><strong>中文栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;我 热爱 学习， 学习 使我 进步&quot;, &quot;我 是 一个 好学生&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;一个&apos;, &apos;使我&apos;, &apos;好学生&apos;, &apos;学习&apos;, &apos;热爱&apos;, &apos;进步&apos;]</span><br><span class="line">[[0 1 0 2 1 1]</span><br><span class="line"> [1 0 1 0 0 0]]</span><br></pre></td></tr></table></figure><p>从中文栗子和英文栗子中，我们可以看到单个英文和单个中文是不会统计数量的，因为统计单个中文或者英文是没有意义的。</p><p>手动加空格，或者加逗号分隔始终是指标不治本，如果给我们一篇文章，让我们去处理的话，那要累到手瘫了。那么，有没有好的办法呢？是有的，那就是用 python 里提供的 jieba 分词类库。<br><strong>我们再来举一个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">con1 = jieba.cut(&quot;我热爱学习，学习使我感到进步。&quot;)</span><br><span class="line">con2 = jieba.cut(&quot;我热爱工作，工作可以让我感到快乐。&quot;)</span><br><span class="line">con3 = jieba.cut(&quot;如果不让我学习，也不让我工作，我会觉得浑身不舒服。&quot;)</span><br><span class="line"></span><br><span class="line"># 转换成列表</span><br><span class="line">content1 = list(con1)</span><br><span class="line">content2 = list(con2)</span><br><span class="line">content3 = list(con3)</span><br><span class="line"></span><br><span class="line"># 把列表转换成字符串</span><br><span class="line">c1 = &apos; &apos;.join(content1)</span><br><span class="line">c2 = &apos; &apos;.join(content2)</span><br><span class="line">c3 = &apos; &apos;.join(content3)</span><br><span class="line"></span><br><span class="line">print(c1, c2, c3)</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([c1, c2, c3])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我 热爱 学习 ， 学习 使 我 感到 进步 。 我 热爱工作 ， 工作 可以 让 我 感到 快乐 。 如果 不让 我 学习 ， 也 不让 我 工作 ， 我会 觉得 浑身 不 舒服 。</span><br><span class="line">[&apos;不让&apos;, &apos;可以&apos;, &apos;如果&apos;, &apos;学习&apos;, &apos;工作&apos;, &apos;快乐&apos;, &apos;感到&apos;, &apos;我会&apos;, &apos;浑身&apos;, &apos;热爱&apos;, &apos;热爱工作&apos;, &apos;舒服&apos;, &apos;觉得&apos;, &apos;进步&apos;]</span><br><span class="line">[[0 0 0 2 0 0 1 0 0 1 0 0 0 1]</span><br><span class="line"> [0 1 0 0 1 1 1 0 0 0 1 0 0 0]</span><br><span class="line"> [2 0 1 1 1 0 0 1 1 0 0 1 1 0]]</span><br></pre></td></tr></table></figure><p>从栗子中可以看到，jieba分词包把句子进行了分词，然后对每个词语的个数进行了统计，但是对于 ‘我’、 ‘也’ 这样的单个中文，并没有统计个数，因为这样的单个中文统计没有意义。</p><p>至此，我们学会了统计文章中英文和中文的词语的个数，那么，单纯统计一个词语出现的个数越多就表示这个词语在文章中越重要吗？那比如，“我们”，“你们”，“他们”，“你的”，这样的指示代词出现的频率应该是最高的，能说明代词是文章的重点吗？显然不是。怎么过滤掉这种出现很多，但是并不是重点的词语呢？我们就要开始学习一种 TF-IDF 的处理方法了。</p><h6 id="3-4-5、TF-IDF"><a href="#3-4-5、TF-IDF" class="headerlink" title="3.4.5、TF-IDF"></a><strong>3.4.5、TF-IDF</strong></h6><p><strong>主要思想</strong>：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。<br><strong>作用</strong>：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。<br><strong>类</strong>：sklearn.feature_extraction.text.TfidfVectorizer<br><strong>TfidfVectorizer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TfidfVectorizer(stop_words=None,…)</span><br><span class="line">    返回词的权重矩阵</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.fit_transform(X,y)       </span><br><span class="line">    X:文本或者包含文本字符串的可迭代对象</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.get_feature_names()</span><br><span class="line">    返回值:单词列表</span><br></pre></td></tr></table></figure><p>我们开始举个栗子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</span><br><span class="line"></span><br><span class="line">con1 = jieba.cut(&quot;我们热爱学习，学习使我们感到进步。&quot;)</span><br><span class="line">con2 = jieba.cut(&quot;我们热爱工作，工作可以让我们感到快乐。&quot;)</span><br><span class="line">con3 = jieba.cut(&quot;如果不让我们学习，不让我们工作，就会感到浑身不舒服。&quot;)</span><br><span class="line"></span><br><span class="line"># 转换成列表</span><br><span class="line">content1 = list(con1)</span><br><span class="line">content2 = list(con2)</span><br><span class="line">content3 = list(con3)</span><br><span class="line"></span><br><span class="line"># 把列表转换成字符串</span><br><span class="line">c1 = &apos; &apos;.join(content1)</span><br><span class="line">c2 = &apos; &apos;.join(content2)</span><br><span class="line">c3 = &apos; &apos;.join(content3)</span><br><span class="line"></span><br><span class="line">print(c1, c2, c3)</span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line">data = tf.fit_transform([c1, c2, c3])</span><br><span class="line">print(tf.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">我们 热爱 学习 ， 学习 使 我们 感到 进步 。 我们 热爱工作 ， 工作 可以 让 我们 感到 快乐 。 如果 不让 我们 学习 ， 不让 我们 工作 ， 就 会 感到 浑身 不 舒服 。</span><br><span class="line">[&apos;不让&apos;, &apos;可以&apos;, &apos;如果&apos;, &apos;学习&apos;, &apos;工作&apos;, &apos;快乐&apos;, &apos;感到&apos;, &apos;我们&apos;, &apos;浑身&apos;, &apos;热爱&apos;, &apos;热爱工作&apos;, &apos;舒服&apos;, &apos;进步&apos;]</span><br><span class="line">[[0.         0.         0.         0.61800047 0.         0.</span><br><span class="line">  0.23996625 0.4799325  0.         0.40629818 0.         0.</span><br><span class="line">  0.40629818]</span><br><span class="line"> [0.         0.43345167 0.         0.         0.32965117 0.43345167</span><br><span class="line">  0.25600354 0.51200708 0.         0.         0.43345167 0.</span><br><span class="line">  0.        ]</span><br><span class="line"> [0.63561168 0.         0.31780584 0.24169953 0.24169953 0.</span><br><span class="line">  0.18770125 0.3754025  0.31780584 0.         0.         0.31780584</span><br><span class="line">  0.        ]]</span><br></pre></td></tr></table></figure><p>我们可以看到，通过 TF-IDF 的处理，把每句话的重点单词找出来了，第一句话 “我们” 和 “学习” 都出现了2次 ，但 “学习”是0.61800047，“我们” 是0.4799325， 重点是 “学习” ， 第三句话“我们” 和 “ 不让 ” 都出现了2次 ，但 “不让”是0.63561168 ，“我们” 是0.3754025 ， 第三句话重点强调 “不让” ，因为 “我们” 在三句话中都频繁出现，并不是每一句话的 “专属” ， 也就不是一句话的重点了。</p><h5 id="3-5、数据的特征处理"><a href="#3-5、数据的特征处理" class="headerlink" title="3.5、数据的特征处理"></a><strong>3.5、数据的特征处理</strong></h5><h6 id="3-5-1、特征处理是什么？"><a href="#3-5-1、特征处理是什么？" class="headerlink" title="3.5.1、特征处理是什么？"></a><strong>3.5.1、特征处理是什么</strong>？</h6><p>通过特定的统计方法（数学方法）将数据转换成算法要求的数据。</p><h6 id="3-5-2、为什么需要特征处理？"><a href="#3-5-2、为什么需要特征处理？" class="headerlink" title="3.5.2、为什么需要特征处理？"></a><strong>3.5.2、为什么需要特征处理</strong>？</h6><p>每个特征的单位不一样，比如相亲的时候，有乘坐飞机的里程数，人的身高，玩游戏的时间，里程数的数值很大，身高相对里程数值很小，那么在做分析的时候，里程数的数值就会起决定性作用。事实在，在统计分析的时候，分析人员认为每个特征同样重要。所以我们需要把不同单位的数值进行特征处理，不因为数值的相差巨大而造成特征的差别。</p><h6 id="3-5-3、特征处理的方法"><a href="#3-5-3、特征处理的方法" class="headerlink" title="3.5.3、特征处理的方法"></a><strong>3.5.3、特征处理的方法</strong></h6><p>数值型数据：（标准缩放）<br>1、<strong>归一化</strong><br>2、<strong>标准化</strong><br>3、<strong>缺失值</strong><br>类别型数据：one-hot编码<br>时间类型：时间的切分</p><h6 id="3-5-4、归一化："><a href="#3-5-4、归一化：" class="headerlink" title="3.5.4、归一化："></a><strong>3.5.4、归一化</strong>：</h6><p><strong>特点</strong>：通过对原始数据进行变换把数据映射到(默认为[0,1])之间。<br><strong>公式</strong>： &nbsp;&nbsp;𝑋′= (𝑥−𝑚𝑖𝑛)/(𝑚𝑎𝑥−𝑚𝑖𝑛) &nbsp;&nbsp;&nbsp;&nbsp; 𝑋′′=𝑋′∗(𝑚𝑥−𝑚𝑖)+𝑚𝑖<br>其中：作用于每一列，max为一列的最大值，min为一列的最小值，那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0。<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.2.png" alt="归一化计算过程"><br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.3.png" alt="归一化计算过程"></p><p><strong>sklearn归一化API</strong> : sklearn.preprocessing.MinMaxScaler<br><strong>MinMaxScaler语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MinMaxScalar(feature_range=(0,1)…)</span><br><span class="line">    每个特征缩放到给定范围(默认[0,1])</span><br><span class="line"></span><br><span class="line">MinMaxScalar.fit_transform(X)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br></pre></td></tr></table></figure><p><strong>归一化步骤</strong>：<br>1、实例化MinMaxScalar<br>2、通过fit_transform转换</p><p><strong>举一个栗子</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">mm = MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,15,46]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[1.         0.         0.         0.        ]</span><br><span class="line"> [0.         1.         1.         0.83333333]</span><br><span class="line"> [0.5        0.5        1.         1.        ]]</span><br></pre></td></tr></table></figure><p>我们可以看到，之前的数据，特征一（ 90,60,75） 是比特征二（2,4,3）在数值上大很多的，那么，如果不做特征处理，直接带入模型处理的话，特征一显然就占决定性作用了，就没有特征二什么事情了。而进行归一化转换之后，特征一和特征二在数值上就在同一量级了，他们就变得“同等重要”了。</p><p><strong>归一化总结</strong>：注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。</p><p>对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变。那么，我们有没有好的解决办法呢？有的，那就是标准化。</p><h6 id="3-5-5、标准化"><a href="#3-5-5、标准化" class="headerlink" title="3.5.5、标准化"></a><strong>3.5.5、标准化</strong></h6><p><strong>特点</strong>：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内<br><strong>公式</strong> ：𝑋′= (𝑥−mean)/𝜎<br>其中，mean为平均值，𝜎为标准差(考量数据的稳定性)<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.5.png" alt="标准差"></p><p>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。</p><p><strong>sklearn特征处理API</strong> : scikit-learn.preprocessing.StandardScaler</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StandardScaler(…)</span><br><span class="line">    处理之后每列来说所有数据都聚集在均值0附近方差为1</span><br><span class="line"></span><br><span class="line">StandardScaler.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br><span class="line"></span><br><span class="line">StandardScaler.mean_</span><br><span class="line">    原始数据中每列特征的平均值</span><br><span class="line"></span><br><span class="line">StandardScaler.std_</span><br><span class="line">    原始数据每列特征的方差</span><br></pre></td></tr></table></figure><p><strong>标准化步骤</strong>：<br>1、实例化StandardScaler<br>2、通过fit_transform转换</p><p><strong>举个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">std = StandardScaler()</span><br><span class="line">data = std.fit_transform([[ 1., -1., 3.],[ 2., 4., 2.],[ 4., 6., -1.]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[-1.06904497 -1.35873244  0.98058068]</span><br><span class="line"> [-0.26726124  0.33968311  0.39223227]</span><br><span class="line"> [ 1.33630621  1.01904933 -1.37281295]]</span><br></pre></td></tr></table></figure><p><strong>标准化总结</strong>：标准化可以避免最大值，最小值发生异常值的干扰。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</p><p><strong>缺失值处理方法</strong><br><strong>删除</strong>：如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列。</p><p><strong>插补</strong>：可以通过缺失值每行或者每列的平均值、中位数来填充。（主要方法）</p><p><strong>sklearn缺失值API</strong>: sklearn.preprocessing.Imputer<br><strong>Imputer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, axis=0)</span><br><span class="line">    完成缺失值插补</span><br><span class="line"></span><br><span class="line">Imputer.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br></pre></td></tr></table></figure><p><strong>Imputer流程</strong>：<br>1、初始化Imputer,指定”缺失值”，指定填补策略，指定行或列<br>2、调用fit_transform</p><p>关于np.nan(np.NaN)<br>1、 numpy的数组中可以使用np.nan/np.NaN来代替缺失值，属于float类型。<br>2、如果是文件中的一些缺失值，可以替换成nan，通过np.array转化成float型的数组即可。</p><h5 id="3-6、数据的特征选择"><a href="#3-6、数据的特征选择" class="headerlink" title="3.6、数据的特征选择"></a><strong>3.6、数据的特征选择</strong></h5><h6 id="3-6-1、特征选择是什么？"><a href="#3-6-1、特征选择是什么？" class="headerlink" title="3.6.1、特征选择是什么？"></a><strong>3.6.1、特征选择是什么</strong>？</h6><p>特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。</p><h6 id="3-6-2、为什么要做特征选择？"><a href="#3-6-2、为什么要做特征选择？" class="headerlink" title="3.6.2、为什么要做特征选择？"></a><strong>3.6.2、为什么要做特征选择</strong>？</h6><p>冗余：部分特征的相关度高，容易消耗计算性能<br>噪声：部分特征对预测结果有负影响</p><h6 id="3-6-3、特征选择主要方法："><a href="#3-6-3、特征选择主要方法：" class="headerlink" title="3.6.3、特征选择主要方法："></a><strong>3.6.3、特征选择主要方法</strong>：</h6><p>Filter(过滤式):VarianceThreshold<br>Embedded(嵌入式)：正则化、决策树</p><h6 id="3-6-4、sklearn特征选择API"><a href="#3-6-4、sklearn特征选择API" class="headerlink" title="3.6.4、sklearn特征选择API"></a><strong>3.6.4、sklearn特征选择API</strong></h6><p>sklearn.feature_selection.VarianceThreshold<br>VarianceThreshold语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">VarianceThreshold(threshold = 0.0)</span><br><span class="line">    删除所有低方差特征</span><br><span class="line"></span><br><span class="line">Variance.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：训练集差异低于threshold的特征将被删除。</span><br><span class="line">    默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。</span><br></pre></td></tr></table></figure><h6 id="3-6-5、VarianceThreshold流程："><a href="#3-6-5、VarianceThreshold流程：" class="headerlink" title="3.6.5、VarianceThreshold流程："></a><strong>3.6.5、VarianceThreshold流程</strong>：</h6><p>1、初始化VarianceThreshold,指定阀值方差<br>2、调用fit_transform</p><h6 id="3-6-6、举个栗子："><a href="#3-6-6、举个栗子：" class="headerlink" title="3.6.6、举个栗子："></a><strong>3.6.6、举个栗子</strong>：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">var = VarianceThreshold(threshold=1.0)</span><br><span class="line">data = var.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0]</span><br><span class="line"> [4]</span><br><span class="line"> [1]]</span><br></pre></td></tr></table></figure><p>从栗子中，可以看到，把方差是0的第一个特征值（0,0,0），第4个特征值（3,3,3），和方差小于1的第2个特征值（2,1,2）都给删除了，只剩下（0,4,1）这个方差大于1的特征值。默认情况下，threshold 等于1.0 。</p><h5 id="3-7、降维-（PCA）"><a href="#3-7、降维-（PCA）" class="headerlink" title="3.7、降维 （PCA）"></a><strong>3.7、降维 （PCA）</strong></h5><h6 id="3-7-1、sklearn降维API-："><a href="#3-7-1、sklearn降维API-：" class="headerlink" title="3.7.1、sklearn降维API ："></a><strong>3.7.1、sklearn降维API</strong> ：</h6><p>sklearn. decomposition</p><h6 id="3-7-2、本质："><a href="#3-7-2、本质：" class="headerlink" title="3.7.2、本质："></a><strong>3.7.2、本质</strong>：</h6><p>PCA是一种分析、简化数据集的技术。</p><h6 id="3-7-3、目的："><a href="#3-7-3、目的：" class="headerlink" title="3.7.3、目的："></a><strong>3.7.3、目的</strong>：</h6><p>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</p><h6 id="3-7-4、作用："><a href="#3-7-4、作用：" class="headerlink" title="3.7.4、作用："></a><strong>3.7.4、作用</strong>：</h6><p>可以削减回归分析或者聚类分析中特征的数量。</p><h6 id="3-7-5、PCA语法："><a href="#3-7-5、PCA语法：" class="headerlink" title="3.7.5、PCA语法："></a><strong>3.7.5、PCA语法</strong>：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PCA(n_components=None)</span><br><span class="line">    将数据分解为较低维数空间</span><br><span class="line"></span><br><span class="line">PCA.fit_transform(X)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后指定维度的array</span><br></pre></td></tr></table></figure><h6 id="3-7-6、PCA流程："><a href="#3-7-6、PCA流程：" class="headerlink" title="3.7.6、PCA流程："></a><strong>3.7.6、PCA流程</strong>：</h6><p>1、初始化PCA,指定减少后的维度<br>2、调用fit_transform</p><h6 id="3-7-7、举个栗子"><a href="#3-7-7、举个栗子" class="headerlink" title="3.7.7、举个栗子"></a><strong>3.7.7、举个栗子</strong></h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">pca = PCA(n_components=0.9)</span><br><span class="line">data = pca.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,15,46]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 15.77507261]</span><br><span class="line"> [-15.11129418]</span><br><span class="line"> [ -0.66377843]]</span><br></pre></td></tr></table></figure><p>该栗子中，原本有4个特征的数据，变成了一个特征，并且该特征保留了原来90%的信息，n_components=0.9 。</p><h1 id="四、机器学习基础"><a href="#四、机器学习基础" class="headerlink" title="四、机器学习基础"></a>四、机器学习基础</h1><h5 id="4-1、机器学习开发流程"><a href="#4-1、机器学习开发流程" class="headerlink" title="4.1、机器学习开发流程"></a><strong>4.1、机器学习开发流程</strong></h5><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.6.png" alt="机器学习开发流程"><br>首先要根据原始数据明确问题做什么，建立模型： 根据数据类型划分应用种类；然后做数据的基本处理：（缺失值，合并表等）和特征工程（特征进行处理） （重要）；其次，找到合适的算法进行预测： 最后，对模型评估，根据模型的准确率，判定效果，如果合格 ，则上线使用，以API形式提供，如果不合格，则要换算法 或者重新提取特征工程，如此循环往复继续下去，直到得到满意的模型。</p><h5 id="4-2、机器学习模型是什么？"><a href="#4-2、机器学习模型是什么？" class="headerlink" title="4.2、机器学习模型是什么？"></a><strong>4.2、机器学习模型是什么</strong>？</h5><p><strong>定义</strong>：通过一种映射关系将输入值到输出值。<br>简单来讲，模型 = 算法 + 数据。<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.7.png" alt="模型"></p><h5 id="4-3、机器学习算法分类"><a href="#4-3、机器学习算法分类" class="headerlink" title="4.3、机器学习算法分类"></a><strong>4.3、机器学习算法分类</strong></h5><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.8.png" alt="算法分类"></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-python-python2" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-02T15:28:56.000Z"><a href="/machinelearning/python-python2/">2020-01-02</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/python-python2/">案例（二）如何把python项目部署到linux服务器上</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2linux%E6%9C%8D%E5%8A%A1%E5%99%A8.png" alt="Python项目部署"></p><h1 id="一、案例背景"><a href="#一、案例背景" class="headerlink" title="一、案例背景"></a><strong>一、案例背景</strong></h1><p>用Python写了个脚本，需要部署到Linux环境的服务器上，由于服务器linux系统（centos,redhat等）自带的是python2，现在的python萌新都是从python3开始学习，所以需要给服务器配置一个python3环境，一番摸索过后，踩过一些坑，也总结了一下经验，故留此文。</p><h1 id="二、主要应用技术"><a href="#二、主要应用技术" class="headerlink" title="二、主要应用技术"></a><strong>二、主要应用技术</strong></h1><h2 id="2-1、linux-命令"><a href="#2-1、linux-命令" class="headerlink" title="2.1、linux 命令"></a><strong>2.1、linux 命令</strong></h2><p><img src="http://wangpengcufe.com/python2-1.png" alt="linux命令"></p><h1 id="三、实施步骤"><a href="#三、实施步骤" class="headerlink" title="*三、实施步骤 *"></a>*<em>三、实施步骤 *</em></h1><h3 id="3-1、安装python3"><a href="#3-1、安装python3" class="headerlink" title="3.1、安装python3"></a><strong>3.1、安装python3</strong></h3><p>开始安装之前先看一下机器的环境，主要看一下操作系他的环境和python版本。<br>查看环境<br>操作系统:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]#  cat /etc/redhat-release</span><br><span class="line">CentOS release 6.5 (Final)</span><br></pre></td></tr></table></figure><p>查看python版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# python -V</span><br><span class="line">Python 2.6.6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@reportweb]# python3 -V</span><br><span class="line">-bash: python3: command not found</span><br></pre></td></tr></table></figure><p>可以看到操作系统是CentOS 6.5 ，python此时只有一个python2，没有python 3 。</p><p>下面开始进入正式安装：<br>第1步：下载python安装包，准备编译环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make</span><br></pre></td></tr></table></figure><p>第2步：下载python源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# wget https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz</span><br></pre></td></tr></table></figure><p>第3步：解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# tar -zxvf Python-3.6.6.tgz</span><br></pre></td></tr></table></figure><p>第4步：安装，编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# cd Python-3.6.2</span><br><span class="line">[root@reportweb Python-3.6.2] # mkdir /usr/local/python3.6</span><br><span class="line">[root@reportweb Python-3.6.2] # ./configure --prefix=/usr/local/python3.6</span><br><span class="line">[root@reportweb Python-3.6.2] # make</span><br><span class="line">[root@reportweb Python-3.6.2] # make install</span><br></pre></td></tr></table></figure><p>第5步：建立软链</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb Python-3.6.2] # ln -s /usr/local/python3.6/bin/python3.6  /usr/bin/python3</span><br></pre></td></tr></table></figure><p>第6步：查询python版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# python3 -V</span><br><span class="line">Python 3.6.6</span><br><span class="line"></span><br><span class="line">[root@reportweb]# python -V</span><br><span class="line">Python 2.6.6</span><br></pre></td></tr></table></figure><p>至此，python3安装完毕。</p><h3 id="3-2、安装pip3"><a href="#3-2、安装pip3" class="headerlink" title="3.2、安装pip3"></a><strong>3.2、安装pip3</strong></h3><p>下面开始安装pip3的步骤。<br>第1步：安装setuptools</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# wget --no-check-certificate  https://pypi.python.org/packages/source/s/setuptools/setuptools-19.6.tar.gz#md5=c607dd118eae682c44ed146367a17e26</span><br></pre></td></tr></table></figure><p>第2步：解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# tar -zxvf setuptools-19.6.tar.gz</span><br></pre></td></tr></table></figure><p>第3步：编译,安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src] # cd setuptools-19.6/</span><br><span class="line">[root@reportweb setuptools-19.6] # python3 setup.py build</span><br><span class="line">[root@reportweb setuptools-19.6] # python3 setup.py install</span><br></pre></td></tr></table></figure><p>第4步：建立软链</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb setuptools-19.6] #  ln -s /usr/local/python3.6/bin/pip3 /usr/bin/pip3</span><br></pre></td></tr></table></figure><p>第5步：查看pip3版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb] # pip3 -V</span><br><span class="line">pip 8.0.2 from /usr/local/python3.6/lib/python3.6/site-packages/pip-8.0.2-py3.6.egg (python 3.6)</span><br></pre></td></tr></table></figure><p>第6步：更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb] # pip3 install --upgrade pip</span><br><span class="line">Collecting pip</span><br><span class="line">  Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB)</span><br><span class="line">    100% |████████████████████████████████| 1.4MB 172kB/s </span><br><span class="line">Installing collected packages: pip</span><br><span class="line">  Found existing installation: pip 8.0.2</span><br><span class="line">    Uninstalling pip-8.0.2:</span><br><span class="line">      Successfully uninstalled pip-8.0.2</span><br><span class="line">Successfully installed pip-19.3.1</span><br><span class="line">[root@reportweb]# pip3 -V</span><br><span class="line">pip 19.3.1 from /usr/local/python3.6/lib/python3.6/site-packages/pip (python 3.6)</span><br></pre></td></tr></table></figure><p>至此，pip3安装完毕。</p><h3 id="3-3、启动服务"><a href="#3-3、启动服务" class="headerlink" title="3.3、启动服务"></a><strong>3.3、启动服务</strong></h3><p>启动服务分为前台启动和后台启动。</p><p>前台启动，就是应用直接由窗口运行，能在窗口直接打印出日志信息，如果手动 ctrl + C 打断时，应用停止。当使用前台启动时，如果我们退出终端，服务就会停止。<br>后台启动，就是用nohup等命令，执行应用，在窗口关闭后，或者 执行其他命令时，该应用仍然可以再后台运行。</p><p>使用前台启动python项目：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# python3 test.py</span><br><span class="line">2020-01-02 16:00:47 读取原日志信息：00:00:01.560 [http-nio-8605-exec-5] [DEBUG] Bound request context to thread: org.apache.catalina.connector.RequestFacade@2dc071e6 org.springframework.boot.web.filter.OrderedRequestContextFilter [RID:] [TID:] [MID:]</span><br><span class="line">2020-01-02 16:00:47 转换格式前信息：00:00:01.560 [http-nio-8605-exec-5] [DEBUG] Bound request context to thread: org.apache.catalina.connector.RequestFacade@2dc071e6 org.springframework.boot.web.filter.OrderedRequestContextFilter [RID:] [TID:] [MID:]</span><br><span class="line">^CTraceback (most recent call last):</span><br><span class="line">  File &quot;test.py&quot;, line 174, in &lt;module&gt;</span><br><span class="line">    read_rawlog()</span><br><span class="line">  File &quot;test.py&quot;, line 79, in read_rawlog</span><br><span class="line">    line_json = log_data_to_json(line_temp)</span><br><span class="line">  File &quot;test.py&quot;, line 120, in log_data_to_json</span><br><span class="line">    logger = re.findall(logger_rule, line)</span><br><span class="line">  File &quot;/usr/python3/lib/python3.6/re.py&quot;, line 222, in findall</span><br><span class="line">    return _compile(pattern, flags).findall(string)</span><br><span class="line">KeyboardInterrupt</span><br><span class="line">[root@reportweb data]#</span><br></pre></td></tr></table></figure><p>前台会输出正常的日志信息，直到你按下 ctrl + c 打断它，就会报KeyboardInterrupt。<br>下面演示后台启动python文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb data]# nohup python3 test.py &amp;</span><br></pre></td></tr></table></figure><p>简单介绍一下nohup 命令</p><p>用途：不挂断地运行命令。</p><p>语法：nohup Command [ Arg … ] [　&amp; ]</p><p>描述：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思( no hang up)。nohup 执行 默认会自动生成一个 nohup.out 记录文件。</p><h3 id="3-4、停止服务"><a href="#3-4、停止服务" class="headerlink" title="3.4、停止服务"></a><strong>3.4、停止服务</strong></h3><p>前台启动的服务，直接 ctrl + c 停止服务，或者退出终端，服务就会停止。</p><p>后台启动的服务，用命令查看pid，然后 kill -9 pid 杀死任务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# ps -ef|grep python</span><br><span class="line">root      9233  2598 95 16:22 pts/0    00:00:16 python3 test.py</span><br><span class="line">root      9244  2598  0 16:22 pts/0    00:00:00 grep python</span><br><span class="line">[root@reportweb]# kill -9 9233</span><br><span class="line">[root@reportweb]# ps -ef|grep python</span><br><span class="line">[root@reportweb]# ps -ef|grep python</span><br><span class="line">root      9257  2598  0 16:23 pts/0    00:00:00 grep python</span><br><span class="line">[1]+  已杀死               nohup python3 test.py</span><br></pre></td></tr></table></figure><h3 id="3-5、查看python执行的日志"><a href="#3-5、查看python执行的日志" class="headerlink" title="3.5、查看python执行的日志"></a><strong>3.5、查看python执行的日志</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">查看前10行命令：</span><br><span class="line">[root@reportweb]# head -n 10 nohup.out</span><br><span class="line"></span><br><span class="line">查看后10行命令：</span><br><span class="line">[root@reportweb]# tail -n 10 nohup.out</span><br><span class="line"></span><br><span class="line">显示文件 nohup.log 的内容，从第 20 行至文件末尾:</span><br><span class="line">[root@reportweb]# tail +20 nohup.out</span><br><span class="line"></span><br><span class="line">要跟踪名为 nohup.log 的文件的增长情况，请输入以下命令：</span><br><span class="line">[root@reportweb]# tail -f nohup.out</span><br></pre></td></tr></table></figure><p>tail -f filename 会把 filename 文件里的最尾部的内容显示在屏幕上，并且不断刷新，只要 filename 更新就可以看到最新的文件内容。</p><p>命令格式：<br>tail [参数] [文件]<br>参数：</p><p>-f 循环读取<br>-q 不显示处理信息<br>-v 显示详细的处理信息<br>-c&lt;数目&gt; 显示的字节数<br>-n&lt;行数&gt; 显示文件的尾部 n 行内容<br>–pid=PID 与-f合用,表示在进程ID,PID死掉之后结束<br>-q, –quiet, –silent 从不输出给出文件名的首部<br>-s, –sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-python-python1" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T15:28:56.000Z"><a href="/machinelearning/python-python1/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/python-python1/">案例（一） 利用RFM模型做用户价值分析</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python1-1.png" alt="RFM模型"></p><h1 id="一、案例背景"><a href="#一、案例背景" class="headerlink" title="一、案例背景"></a><strong>一、案例背景</strong></h1><p>在产品迭代过程中，通常需要根据用户的属性进行归类，也就是通过分析数据，对用户进行归类，以便于在推送及转化过程中获得更大的收益。</p><p>本案例是基于某互联网公司的实际用户购票数据为研究对象，对用户购票的时间，购买的金额进行了采集，每个用户用手机号来区别唯一性。数据分析人员根据用户购买的时间和金额，通过建立RFM模型，来计算出用户最近最近一次购买的打分，用户购买频率的打分，用户购买金额的打分，然后根据三个分数进行一个加权打分，和综合打分。业务人员可以根据用户的打分情况，对不同的用户进行个性化营销和精准营销，例如给不同的用户推送定制的营销短信，不同优惠额度的打折券等等。</p><p>通过RFM方法，可以根据用户的属性数据分析，对用户进行了归类。在推送、转化等很多过程中，可以更加精准化，不至于出现用户反感的情景，更重要的是，对产品转化等商业价值也有很大的帮助。</p><h1 id="二、RFM概念"><a href="#二、RFM概念" class="headerlink" title="二、RFM概念"></a><strong>二、RFM概念</strong></h1><p>RFM模型是衡量客户价值和客户创利能力的重要工具和手段。在众多的客户关系管理(CRM)的分析模式中，RFM模型是被广泛提到的。该机械模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱3项指标来描述该客户的价值状况。</p><p><strong>RFM分析</strong> 就是根据客户活跃程度和交易金额的贡献，进行客户价值细分的一种方法。其中：</p><p><strong>R（Recency）</strong>：客户最近一次交易时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。</p><p><strong>F（Frequency）</strong>：客户在最近一段时间内交易的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。</p><p><strong>M（Monetary）</strong>：客户在最近一段时间内交易的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。</p><p><img src="http://wangpengcufe.com/python1-2.png" alt="客户价值"></p><p><strong>R打分：</strong>基于最近一次交易日期计算的得分，距离当前日期越近，得分越高。例如5分制。</p><p><strong>F打分：</strong>基于交易频率计算的得分，交易频率越高，得分越高。如5分制。</p><p><strong>M打分：</strong>基于交易金额计算的得分，交易金额越高，得分越高。如5分制。</p><p><strong>RFM总分值</strong>：RFM=Rx100+Fx10+Mx1</p><p><strong>RFM分析的主要作用：</strong></p><ul><li><p>识别优质客户。可以指定个性化的沟通和营销服务，为更多的营销决策提供有力支持。</p></li><li><p>能够衡量客户价值和客户利润创收能力。</p></li></ul><h1 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a><strong>三、代码实现</strong></h1><h3 id="3-1、引包"><a href="#3-1、引包" class="headerlink" title="3.1、引包"></a><strong>3.1、引包</strong></h3><p>首先我们引入需要用的包，数据分析常用的numpy包，pandas包，等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import mysql.connector</span><br></pre></td></tr></table></figure><h3 id="3-2、读取数据"><a href="#3-2、读取数据" class="headerlink" title="3.2、读取数据"></a><strong>3.2、读取数据</strong></h3><p>接下来我们开始用pd.read_csv方法读取用户的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+&apos;：读取数据...&apos;)</span><br><span class="line"></span><br><span class="line">config = &#123;</span><br><span class="line">    &apos;host&apos; : &apos;127.0.0.1&apos;,</span><br><span class="line">    &apos;user&apos; : &apos;root&apos;,</span><br><span class="line">    &apos;password&apos; : &apos;test123&apos;,</span><br><span class="line">    &apos;port&apos; : 3306,</span><br><span class="line">    &apos;database&apos; : &apos;user&apos;,</span><br><span class="line">    &apos;charset&apos; : &apos;gb2312&apos;</span><br><span class="line">&#125;</span><br><span class="line">cnn = mysql.connector.connect(**config) # 建立MySQL连接</span><br><span class="line">cursor = cnn.cursor() # 获得游标</span><br><span class="line">sql = &quot;SELECT  phoneNo AS PHONENO,create_date AS ORDERDATE,order_no AS ORDERNO,ROUND(pay_amount/100,2) AS PAYAMOUNT &quot; \</span><br><span class="line">      &quot;FROM user.`event_record_order`&quot; # SQL语句</span><br><span class="line">raw_data = pd.read_sql(sql,cnn,index_col=&apos;PHONENO&apos;)</span><br><span class="line">cursor.close() # 关闭游标</span><br><span class="line">cnn.close() # 关闭连接</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+&apos;：读取数据完毕！&apos;)</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+&apos;：开始建立RFM模型...&apos;)</span><br></pre></td></tr></table></figure><p>介绍一下config 里的参数信息：host是数据库的ip信息，本案例用的是本地数据库，实际部署生产服务器时，改成生产的ip地址即可。user 是数据库的用户名，password是密码，port是数据库的端口号，database是连接的数据库名 （schema），charset是字符集编码。</p><p>购票时间（ORDERDATE)，订单号（ORDERID）是object类型，订单金额（AMOUNTINFO）是浮点类型。index_col指定了数据中用户的唯一性用 USERID来表示。</p><p>time.strftime(‘%Y-%m-%d %H:%M:%S’,time.localtime(time.time())打印了当前的系统时间，用来记录日志信息。</p><h3 id="3-3、数据审查"><a href="#3-3、数据审查" class="headerlink" title="3.3、数据审查"></a><strong>3.3、数据审查</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;Data Overview :&apos;)</span><br><span class="line">print(raw_data.head(4)) #打印原始数据前4条</span><br><span class="line">print(&apos;-&apos; * 30)</span><br><span class="line">print(&apos;Data DESC:&apos;)</span><br><span class="line">print(raw_data.describe())  #打印原始数据基本描述性信息</span><br></pre></td></tr></table></figure><p>我们用raw_data.head(n)来指定取出数据的前几条，’-‘*30是用来输出打印分隔线，下文再出现时不再重复解释，用raw_data.describe()来获得数据的基本描述性信息。输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Data Overview:</span><br><span class="line">                      ORDERDATE               ORDERNO  PAYAMOUNT</span><br><span class="line">PHONENO                                                         </span><br><span class="line">135****0930 2019-10-02 13:37:36  01201910021336227979        7.0</span><br><span class="line">183****1153 2019-09-30 06:22:29  0120190930062149F9AF        4.5</span><br><span class="line">150****6073 2019-10-30 18:21:45  01201910301821065CFD        2.0</span><br><span class="line">173****7295 2019-10-21 15:13:23  01201910211512498153        7.0</span><br><span class="line">------------------------------</span><br><span class="line">Data DESC:</span><br><span class="line">          PAYAMOUNT</span><br><span class="line">count  96323.000000</span><br><span class="line">mean       4.212409</span><br><span class="line">std        3.049499</span><br><span class="line">min        0.000000</span><br><span class="line">25%        2.600000</span><br><span class="line">50%        3.600000</span><br><span class="line">75%        5.000000</span><br><span class="line">max       80.000000</span><br></pre></td></tr></table></figure><p>我们看到结果中的 count表示总共的记录条数，mean表示了均值，std表示标准差，min表示最小值，25%表示下四分位，也叫第一四分位，50%表示中位值，也叫第二四分位，75%表示上四分位，也叫第三四分位。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">na_cols = raw_data.isnull().any(axis=0) #查看每一列是否具有缺失值</span><br><span class="line">print(&apos;NA Cols:&apos;)</span><br><span class="line">print(na_cols)</span><br><span class="line">print(&apos;-&apos; * 30)</span><br><span class="line">na_lines = raw_data.isnull().any(axis=1) #查看每一行是否具有缺失值</span><br><span class="line">print(&apos;NA Records:&apos;)</span><br><span class="line">print(&apos;Total number of NA lines is :&#123;0&#125;&apos;.format(na_lines.sum()))  #查看具有缺失值的行总记录数</span><br><span class="line">print(raw_data[na_lines])  #只查看具有缺失值的行信息</span><br></pre></td></tr></table></figure><p>我们用raw_data.isnull()来判断是否有缺失值，其中参数axis=0表示的是列，axis=1表示的是行，用:{0}’.format()的方式在字符串中传入参数。输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NA Cols:</span><br><span class="line">ORDERDATE    False</span><br><span class="line">ORDERNO      False</span><br><span class="line">PAYAMOUNT    False</span><br><span class="line">dtype: bool</span><br><span class="line">------------------------------</span><br><span class="line">NA Records:</span><br><span class="line">Total number of NA lines is :0</span><br><span class="line">Empty DataFrame</span><br><span class="line">Columns: [ORDERDATE, ORDERNO, PAYAMOUNT]</span><br><span class="line">Index: []</span><br></pre></td></tr></table></figure><p>通过结果可以看到，实际的交易用户数据还是比较完整的，没有缺失数据的情况，可能这批数据被技术人员采集过来已经处理过了，不讨论了。如果数据有缺失的情况怎么办？那就要对缺失的数据进行一个预处理。</p><h3 id="3-4、数据预处理"><a href="#3-4、数据预处理" class="headerlink" title="3.4、数据预处理"></a><strong>3.4、数据预处理</strong></h3><p>数据预处理，包括数据异常，格式转换，单位转化（如果有单位不统一的情况）等。</p><p>我们先来看<strong>异常值处理：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales_data = raw_data.dropna() #丢弃带有缺失值的行记录</span><br><span class="line">sales_data = sales_data[sales_data[&apos;PAYAMOUNT&apos;] &gt; 1]</span><br></pre></td></tr></table></figure><p>这里，我用代码去除了小于1元的订单，正常出行连1块钱都不用，那应该是测试数据了，现在谁出门做个公交还不得1元起步。对于用户有缺失值的记录进行了丢弃，当然也可以用其他的方法，例如平均值补全法。</p><p>然后看<strong>日期格式转换：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales_data[&apos;ORDERDATE&apos;] = pd.to_datetime(sales_data[&apos;ORDERDATE&apos;])</span><br><span class="line">print(&apos;Raw Dtype:&apos;)</span><br><span class="line">print(sales_data.dtypes)</span><br></pre></td></tr></table></figure><p>用pd.to_datetime()方法对用户的订单日期进行了格式化转换。输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Raw Dtype:</span><br><span class="line">ORDERDATE    datetime64[ns]</span><br><span class="line">ORDERNO              object</span><br><span class="line">PAYAMOUNT           float64</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><p>最后看<strong>数据转换：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recency_value = sales_data[&apos;ORDERDATE&apos;].groupby(sales_data.index).max()  #计算原始最近一次购买时间</span><br><span class="line">frequency_value = sales_data[&apos;ORDERDATE&apos;].groupby(sales_data.index).count()    #计算原始订单数</span><br><span class="line">monetray_value = sales_data[&apos;PAYAMOUNT&apos;].groupby(sales_data.index).sum()  #计算原始订单总金额</span><br></pre></td></tr></table></figure><p>这里根据订单日期的聚合运算得到了用户的最近一次购买时间，用户总的购买数，和购买金额，max()得到了购买时间，count()得到了购买数量，sum()得到了购买金额。</p><h3 id="3-5、计算RFM得分"><a href="#3-5、计算RFM得分" class="headerlink" title="3.5、计算RFM得分"></a><strong>3.5、计算RFM得分</strong></h3><p>得到了最近的购买时间，购买数，和购买金额，下面就可以开始计算RFM得分了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deadline_date = pd.datetime(2019,11,15)</span><br><span class="line">r_interval = (deadline_date - recency_value).dt.days</span><br><span class="line">r_score = pd.cut(r_interval,5,labels=[5,4,3,2,1])</span><br><span class="line">f_score = pd.cut(frequency_value,5,labels=[1,2,3,4,5])</span><br><span class="line">m_score = pd.cut(monetray_value,5,labels=[1,2,3,4,5])</span><br></pre></td></tr></table></figure><p>我们又把客户分成五等分，这个五等分分析相当于是一个“忠诚度的阶梯”(loyalty ladder)，如购买一次的客户为新客户，购买两次的客户为潜力客户，购买三次的客户为老客户，购买四次的客户为成熟客户，购买五次及以上则为忠实客户。其诀窍在于让消费者一直顺着阶梯往上爬，把销售想象成是要将两次购买的顾客往上推成三次购买的顾客，把一次购买者变成两次的。</p><p>我们用deadline_date来表示分析的截止日期，那么统计用户的时间范围就是从数据中最早开始的购买时间到deadline_date。</p><p>用pandas.series.dt.days可以对操作后的datatime直接进行取数。pandas.cut用来把一组数据分割成离散的区间。</p><p>简单介绍一下pandas.cut的用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates=&apos;raise&apos;)</span><br></pre></td></tr></table></figure><ul><li><strong>x</strong>：被切分的类数组（array-like）数据，必须是1维的（不能用DataFrame）；</li><li><strong>bins</strong>：bins是被切割后的区间（或者叫“桶”、“箱”、“面元”），有3中形式：一个int型的标量、标量序列（数组）或者pandas.IntervalIndex 。<ul><li><strong>一个int型的标量</strong>，当bins为一个int型的标量时，代表将x平分成bins份。x的范围在每侧扩展0.1%，以包括x的最大值和最小值。</li><li>标量序列，标量序列定义了被分割后每一个bin的区间边缘，此时x没有扩展。</li><li>pandas.IntervalIndex，定义要使用的精确区间。</li></ul></li><li>right：bool型参数，默认为True，表示是否包含区间右部。比如如果bins=[1,2,3]，right=True，则区间为(1,2]，(2,3]；right=False，则区间为(1,2),(2,3)。</li><li><strong>labels</strong>：给分割后的bins打标签，比如把年龄x分割成年龄段bins后，可以给年龄段打上诸如青年、中年的标签。labels的长度必须和划分后的区间长度相等，比如bins=[1,2,3]，划分后有2个区间(1,2]，(2,3]，则labels的长度必须为2。如果指定labels=False，则返回x中的数据在第几个bin中（从0开始）。</li><li>retbins：bool型的参数，表示是否将分割后的bins返回，当bins为一个int型的标量时比较有用，这样可以得到划分后的区间，默认为False。</li><li>precision：保留区间小数点的位数，默认为3.</li><li>include_lowest：bool型的参数，表示区间的左边是开还是闭的，默认为false，也就是不包含区间左部（闭）。</li><li>duplicates：是否允许重复区间。有两种选择：raise：不允许，drop：允许。</li></ul><p>重点理解我标粗的几个参数，其他参数有需要用到时查阅。</p><p><strong>RFM数据合并</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rfm_list = [r_score,f_score,m_score]  #将r、f、m三个维度组成列表</span><br><span class="line">rfm_cols = [&apos;r_score&apos;,&apos;f_score&apos;,&apos;m_score&apos;] #设置r、f、m 三个维度列名</span><br><span class="line">rfm_pd = pd.DataFrame(np.array(rfm_list).transpose(),dtype=np.int32,columns=rfm_cols,index=frequency_value.index) #建立r、f、m数据框</span><br></pre></td></tr></table></figure><p>我们把RFM的数据进行了合并，首先是将r、f、m三个维度组成一个列表，然后取了三个列名，把数据，列名组装成一个数据框DataFrame.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;RFM Score Overview:&apos;)</span><br><span class="line">print(rfm_pd.head(4))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RFM Score Overview:</span><br><span class="line">             r_score  f_score  m_score</span><br><span class="line">PHONENO                               </span><br><span class="line">13001055088        4        1        1</span><br><span class="line">13001061903        4        1        1</span><br><span class="line">13001066446        5        1        1</span><br><span class="line">13001123218        4        1        1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rfm_pd[&apos;rfm_wscore&apos;] = rfm_pd[&apos;r_score&apos;] * 0.6 + rfm_pd[&apos;f_score&apos;] * 0.3 + rfm_pd[&apos;m_score&apos;] * 0.1</span><br><span class="line">rfm_pd_tmp = rfm_pd.copy()</span><br><span class="line">rfm_pd_tmp[&apos;r_score&apos;] = rfm_pd_tmp[&apos;r_score&apos;].astype(&apos;str&apos;)</span><br><span class="line">rfm_pd_tmp[&apos;f_score&apos;] = rfm_pd_tmp[&apos;f_score&apos;].astype(&apos;str&apos;)</span><br><span class="line">rfm_pd_tmp[&apos;m_score&apos;] = rfm_pd_tmp[&apos;m_score&apos;].astype(&apos;str&apos;)</span><br><span class="line">rfm_pd[&apos;rfm_comb&apos;] = rfm_pd_tmp[&apos;r_score&apos;].str.cat(rfm_pd_tmp[&apos;f_score&apos;]).str.cat(rfm_pd_tmp[&apos;m_score&apos;])</span><br></pre></td></tr></table></figure><p>理论上，上一次消费时间越近的顾客应该是比较好的顾客，对提供即时的商品或是服务也最有可能会有反应。营销人员若想业绩有所成长，只能靠偷取竞争对手的市场占有率，而如果要密切地注意消费者的购买行为，那么最近的一次消费就是营销人员第一个要利用的工具。历史显示，如果我们能让消费者购买，他们就会持续购买。这也就是为什么，0至3个月的顾客收到营销人员的沟通信息多于3至6个月的顾客。</p><p>这里，对RFM进行了加权打分，R占60%，F占30%，M占10%，当然也可以根据业务的实际情况进行相应的权重调整。综合打分是根据RFM=R<em>100+F</em>10+M*1。</p><h3 id="3-6、保存结果"><a href="#3-6、保存结果" class="headerlink" title="3.6、保存结果"></a><strong>3.6、保存结果</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;Final RFM Score Overview:&apos;)</span><br><span class="line">print(rfm_pd.head(4))</span><br><span class="line">print(&apos;-&apos;*30)</span><br><span class="line">print(&apos;Final RFM Score DESC:&apos;)</span><br><span class="line">print(rfm_pd.describe())</span><br><span class="line"></span><br><span class="line">rfm_pd.to_csv(&apos;sales_rfm_score.csv&apos;)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Final RFM Score Overview:</span><br><span class="line">             r_score  f_score  m_score  rfm_wscore rfm_comb</span><br><span class="line">PHONENO                                                    </span><br><span class="line">13001055088        4        1        1         2.8      411</span><br><span class="line">13001061903        4        1        1         2.8      411</span><br><span class="line">13001066446        5        1        1         3.4      511</span><br><span class="line">13001123218        4        1        1         2.8      411</span><br><span class="line">------------------------------</span><br><span class="line">Final RFM Score DESC:</span><br><span class="line">            r_score       f_score       m_score    rfm_wscore</span><br><span class="line">count  53064.000000  53064.000000  53064.000000  53064.000000</span><br><span class="line">mean       3.732172      1.006407      1.002148      2.641441</span><br><span class="line">std        0.944452      0.113022      0.055212      0.570417</span><br><span class="line">min        1.000000      1.000000      1.000000      1.000000</span><br><span class="line">25%        3.000000      1.000000      1.000000      2.200000</span><br><span class="line">50%        4.000000      1.000000      1.000000      2.800000</span><br><span class="line">75%        5.000000      1.000000      1.000000      3.400000</span><br></pre></td></tr></table></figure><h3 id="3-7、写入数据库"><a href="#3-7、写入数据库" class="headerlink" title="3.7、写入数据库"></a><strong>3.7、写入数据库</strong></h3><p><strong>建立数据库连接</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">table_name = &apos;sale_rfm_score&apos;</span><br><span class="line">#数据框基本信息</span><br><span class="line">config = &#123;</span><br><span class="line">    &apos;host&apos; : &apos;172.0.0.1&apos;,</span><br><span class="line">    &apos;user&apos; : &apos;root&apos;,</span><br><span class="line">    &apos;password&apos; : &apos;test123&apos;,</span><br><span class="line">    &apos;port&apos; : 3306,</span><br><span class="line">    &apos;database&apos; : &apos;skpda&apos;,</span><br><span class="line">    &apos;charset&apos; : &apos;gb2312&apos;</span><br><span class="line">&#125;</span><br><span class="line">con = mysql.connector.connect(**config)</span><br><span class="line">cursor = con.cursor()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;show tables&quot;)  #</span><br><span class="line">table_object = cursor.fetchall()  # 通过fetchall方法获得所有数据</span><br><span class="line">table_list = []  # 创建库列表</span><br><span class="line">for t in table_object:  # 循环读出所有库</span><br><span class="line">    table_list.append(t[0])  # 每个每个库追加到列表</span><br><span class="line">if not table_name in table_list:  # 如果目标表没有创建</span><br><span class="line">    cursor.execute(&apos;&apos;&apos;</span><br><span class="line">    CREATE TABLE %s (</span><br><span class="line">    phone_no               VARCHAR(20),</span><br><span class="line">    r_score               int(2),</span><br><span class="line">    f_score              int(2),</span><br><span class="line">    m_score              int(2),</span><br><span class="line">    rfm_wscore              DECIMAL(10,2),</span><br><span class="line">    rfm_comb              VARCHAR(10),</span><br><span class="line">    create_date              VARCHAR(20)</span><br><span class="line">    )ENGINE=InnoDB DEFAULT CHARSET=gb2312</span><br><span class="line">    &apos;&apos;&apos; % table_name)  # 创建新表</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+ &apos;：开始清除 table &#123;0&#125;的历史数据...&apos;.format(table_name)) # 输出开始清历史数据的提示信息</span><br><span class="line">delete_sql = &apos;truncate table &#123;0&#125;&apos;.format(table_name)</span><br><span class="line">cursor.execute(delete_sql)</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+ &apos;：清除 table &#123;0&#125;的历史数据完毕！&apos;.format(table_name)) # 输出清除历史数据完毕的提示信息</span><br></pre></td></tr></table></figure><p>连接的参数不再介绍，上文已经介绍过。通过fetchall方法获得所有数据,读出所有的表，如果没有表则创建。用cursor.execute先执行truncate语句，把表中的信息先清除，然后重新写入数据。</p><p><strong>将数据写入数据库</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">phone_no = rfm_pd.index # 索引列</span><br><span class="line">rfm_wscore = rfm_pd[&apos;rfm_wscore&apos;]  #RFM 加权得分列</span><br><span class="line">rfm_comb = rfm_pd[&apos;rfm_comb&apos;]  #RFM组合得分列</span><br><span class="line">timestamp = time.strftime(&apos;%Y-%m-%d&apos;,time.localtime(time.time())) # 写库日期</span><br><span class="line">print(&apos;开始写入数据库表 &#123;0&#125;&apos;.format(table_name)) # 输出开始写库的提示信息</span><br><span class="line">for i in range(rfm_pd.shape[0]):</span><br><span class="line">    insert_sql = &quot;INSERT INTO `%s` VALUES (&apos;%s&apos;,%s,%s,%s,%s,%s,&apos;%s&apos;)&quot; % \</span><br><span class="line">                 (table_name, phone_no[i], r_score.iloc[i], f_score.iloc[i], m_score.iloc[i], rfm_wscore.iloc[i],</span><br><span class="line">                  rfm_comb.iloc[i], timestamp)  # 写库SQL依据</span><br><span class="line">    cursor.execute(insert_sql)</span><br><span class="line">    con.commit()</span><br><span class="line">cursor.close()</span><br><span class="line">con.close()</span><br><span class="line">print(&apos;写入数据库结束，总记录条数为: %d&apos; %(i+1))</span><br></pre></td></tr></table></figure><p>先从数据集合 rfm_pd （rfm_pd 是一个DataFrame）中获取到rfm的每个字段， ’….{0}’.format(table_name)表示的是在字符串中拼接参数，{0}代表一个字符串占位符。</p><h1 id="四、案例结果分析"><a href="#四、案例结果分析" class="headerlink" title="四、案例结果分析"></a><strong>四、案例结果分析</strong></h1><p>根据RFM模型的建立，我们在数据库里生成了数据。</p><p><img src="http://wangpengcufe.com/python1-3.png" alt="数据库表生成"></p><p>然后前段工程师根据数据库里的数据得到了用户RFM的价值打分页面，如图（后台展示页面）。</p><p>运营人员根据页面的打分情况来衡量客户价值和客户创利能力，了解客户差异。将客户分别按照R、F、M参数分组后，假设某个客户同时属于R5、F4、M3三个组，则可以得到该客户的RFM代码543。同理，我们可以推测，有一些客户刚刚成功交易、且交易频率高、总采购金额大，其RFM代码是555，还有一些客户的RFM代码是554、545……每一个RFM代码都对应着一小组客户，开展市场营销活动的时候可以从中挑选出若干组进行。<br><img src="http://wangpengcufe.com/python1-4.png" alt="后台展示页面"></p><p>用户是根据RFM的打分倒序排列，可以直接找到重点客户的信息，点开手机号，查看客户的详细信息（这一步由前端开发人员实现），针对重点客户展开各种个性化营销。</p><p><img src="http://wangpengcufe.com/python1-5.png" alt="重点客户详细信息"></p><p>RFM三个指标每个维度再细分出5份，这样就能够细分出5x5x5=125类用户，再根据每类用户精准营销……显然125类用户已超出普通人脑的计算范畴了，更别说针对125类用户量体定制营销策略。实际运用上，我们只需要把每个维度做一次两分即可，这样在3个维度上我们依然得到了8组用户。</p><p>这样，就可以得到以下解读（编号次序RFM,1代表高，0代表低）<br>重要价值客户（111）：最近消费时间近、消费频次和消费金额都很高，必须是VIP啊！<br>重要保持客户（011）：最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠诚客户，我们需要主动和他保持联系。<br>重要发展客户（101）：最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展。<br>重要挽留客户（001）：最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。</p><p><strong>案例结论：</strong></p><ul><li>表现处于一般水平以上的用户的比例太小，低于1%（R、F、M三个维度得分均在3以上的用户数），VIP客户太少。</li><li>会员中99%以上的客户消费状态都不容乐观，主要体现在消费频率低R、消费总金额低M。这可能跟公司的地铁出行的业务有关系，公司的业务分布在全国中小城市，大部分用户都是使用一次的用户。</li><li>低价值客户有262个，占总比例的 0.4%，运营人员可以导出下载这批用户。</li></ul><p>下一节，讲一下在linux服务器上部署python应用。</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ml-ml16" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T15:28:56.000Z"><a href="/machinelearning/ml-ml16/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml16/">机器学习（十六） 特征变换-标签和索引的转化</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/ml16.png" alt="卡方选择器"></p><h1 id="一、原理"><a href="#一、原理" class="headerlink" title="一、原理"></a><strong>一、原理</strong></h1><p>在机器学习处理过程中,为了方便相关算法的实现,经常需要把标签数据(一般是字符串)转化成整数索引,或是在计算结束后将整数索引还原为相应的标签.<br>Spark ML 包中提供了几个相关的转换器:<br>StringIndexer,IndexToString,OneHotEncoder,VectorIndexer,他们提供了十分方便的特征转换功能,这些转换器都位于org.apache.spark.ml.feature包下。</p><p>值得注意的是,用于特征转换的转换器和其他的机器学习算法一样,也属于Ml Pipeline模型的一部分,可以用来构成机器学习流水线,以StringIndexer为例。</p><p>StringIndexer（字符串-索引变换）将字符串的标签编码成标签索引。标签索引序列的取值范围是[0，numLabels（字符串中所有出现的单词去掉重复的词后的总和）]，按照标签出现频率排序，出现最多的标签索引为0。如果输入是数值型，我们先将数值映射到字符串，再对字符串进行索引化。如果下游的pipeline（例如：Estimator或者Transformer）需要用到索引化后的标签序列，则需要将这个pipeline的输入列名字指定为索引化序列的名字。大部分情况下，通过setInputCol设置输入的列名。</p><p>下面来具体介绍StringIndexer、IndexToString、OneHotEncoder、VectorIndexer。</p><h1 id="二、StringIndexer（字符串-索引变换）"><a href="#二、StringIndexer（字符串-索引变换）" class="headerlink" title="二、StringIndexer（字符串-索引变换）"></a><strong>二、StringIndexer（字符串-索引变换）</strong></h1><h3 id="2-1、原理"><a href="#2-1、原理" class="headerlink" title="2.1、原理"></a><strong>2.1、原理</strong></h3><p>StringIndexer将标签的字符串列编码为标签索引的列。 索引位于[0，numLabels）中，并支持四个排序选项：“frequencyDesc”：按标签频率的降序（最频繁的标签分配为0），“frequencyAsc”：按标签频率的升序（最不频繁的标签分配为0） ，“alphabetDesc”：降序字母顺序和“alphabetAsc”：升序字母顺序（默认=“frequencyDesc”）。 如果用户选择保留，则看不见的标签将放置在索引numLabels处。 如果输入列为数字，则将其强制转换为字符串并为字符串值编制索引。 当下游管道组件（例如Estimator或Transformer）使用此字符串索引标签时，必须将组件的输入列设置为此字符串索引列名称。 在许多情况下，可以使用setInputCol设置输入列。</p><h3 id="2-2、代码实现"><a href="#2-2、代码实现" class="headerlink" title="2.2、代码实现"></a><strong>2.2、代码实现</strong></h3><p>首先引入需要用的包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line">import org.apache.spark.ml.feature.IndexToString;</span><br><span class="line">import org.apache.spark.ml.feature.OneHotEncoderEstimator;</span><br><span class="line">import org.apache.spark.ml.feature.StringIndexer;</span><br><span class="line">import org.apache.spark.ml.feature.StringIndexerModel;</span><br><span class="line">import org.apache.spark.ml.feature.VectorIndexer;</span><br><span class="line">import org.apache.spark.ml.feature.VectorIndexerModel;</span><br><span class="line">import org.apache.spark.ml.linalg.VectorUDT;</span><br><span class="line">import org.apache.spark.ml.linalg.Vectors;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.Metadata;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line">import scala.collection.immutable.Set;</span><br></pre></td></tr></table></figure><p>获取spark:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = SparkSession.builder().appName(&quot;StringIndexerTest&quot;).master(&quot;local&quot;).getOrCreate();</span><br></pre></td></tr></table></figure><p>构造一些简单数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rowRDD = Arrays.asList(RowFactory.create(0,&quot;a&quot;),</span><br><span class="line">                        RowFactory.create(1,&quot;b&quot;),</span><br><span class="line">                        RowFactory.create(2,&quot;c&quot;),</span><br><span class="line">                        RowFactory.create(3,&quot;a&quot;),</span><br><span class="line">                        RowFactory.create(4,&quot;a&quot;),</span><br><span class="line">                        RowFactory.create(5,&quot;c&quot;));</span><br><span class="line"></span><br><span class="line">StructType schema = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;id&quot;,DataTypes.IntegerType,false,Metadata.empty()),</span><br><span class="line">        new StructField(&quot;category&quot;,DataTypes.StringType,false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD, schema);</span><br><span class="line">df.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+</span><br><span class="line">|id |category|</span><br><span class="line">+---+--------+</span><br><span class="line">|0  |a       |</span><br><span class="line">|1  |b       |</span><br><span class="line">|2  |c       |</span><br><span class="line">|3  |a       |</span><br><span class="line">|4  |a       |</span><br><span class="line">|5  |c       |</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure><p>然后构建StringIndexer模型，我们创建一个StringIndexer对象，设定输入输出列名，其余参数采用默认值，并对这个DataFrame进行训练，产生StringIndexerModel对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StringIndexer indexer = new StringIndexer().setInputCol(&quot;category&quot;).setOutputCol(&quot;categoryIndex&quot;);</span><br><span class="line">StringIndexerModel model = indexer.fit(df);</span><br></pre></td></tr></table></figure><p>之后我们即可利用StringIndexerModel对象对DataFrame数据进行转换操作，可以看到，默认情况下，StringIndexerModel依次按照出现频率的高低，把字符标签进行了排序，即出现最多的“a”被编号成0，“c”为1，出现最少的“b”为0。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; indexed1 = model.transform(df);</span><br><span class="line">indexed1.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+</span><br><span class="line">|id |category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|0  |a       |0.0          |</span><br><span class="line">|1  |b       |2.0          |</span><br><span class="line">|2  |c       |1.0          |</span><br><span class="line">|3  |a       |0.0          |</span><br><span class="line">|4  |a       |0.0          |</span><br><span class="line">|5  |c       |1.0          |</span><br><span class="line">+---+--------+-------------+</span><br></pre></td></tr></table></figure><p>如果我们使用已有的数据构建了一个StringIndexerModel，然后再构建一个新的DataFrame，这个DataFrame中有着模型内未曾出现的标签“d”，用已有的模型去转换这一DataFrame会有什么效果？<br>实际上，如果直接转换的话，Spark会抛出异常，报出“Unseen label: d”的错误。<br>为了处理这种情况，在模型训练后，可以通过设置setHandleInvalid(“skip”)来忽略掉那些未出现的标签，这样，带有未出现标签的行将直接被过滤掉，所下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rowRDD2 = Arrays.asList(RowFactory.create(0,&quot;a&quot;),</span><br><span class="line">        RowFactory.create(1,&quot;b&quot;),</span><br><span class="line">        RowFactory.create(2,&quot;c&quot;),</span><br><span class="line">        RowFactory.create(3,&quot;a&quot;),</span><br><span class="line">        RowFactory.create(4,&quot;a&quot;),</span><br><span class="line">        RowFactory.create(5,&quot;d&quot;));</span><br><span class="line">Dataset&lt;Row&gt; df2 = spark.createDataFrame(rowRDD2, schema);</span><br><span class="line">Dataset&lt;Row&gt; indexed2 = model.transform(df2);</span><br><span class="line">indexed2.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unseen label: d.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; indexed2 = model.setHandleInvalid(&quot;skip&quot;).transform(df2);</span><br><span class="line">indexed2.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+</span><br><span class="line">|id |category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|0  |a       |0.0          |</span><br><span class="line">|1  |b       |2.0          |</span><br><span class="line">|2  |c       |1.0          |</span><br><span class="line">|3  |a       |0.0          |</span><br><span class="line">|4  |a       |0.0          |</span><br><span class="line">+---+--------+-------------+</span><br></pre></td></tr></table></figure><h1 id="三、IndexToString（索引-字符串变换）"><a href="#三、IndexToString（索引-字符串变换）" class="headerlink" title="三、IndexToString（索引-字符串变换）"></a><strong>三、IndexToString（索引-字符串变换）</strong></h1><h3 id="3-1、原理"><a href="#3-1、原理" class="headerlink" title="3.1、原理"></a><strong>3.1、原理</strong></h3><p>与StringIndexer对应，IndexToString将索引化标签还原成原始字符串。一个常用的场景是先通过StringIndexer产生索引化标签，然后使用索引化标签进行训练，最后再对预测结果使用IndexToString来获取其原始的标签字符串。</p><h3 id="3-2、代码实现"><a href="#3-2、代码实现" class="headerlink" title="3.2、代码实现"></a><strong>3.2、代码实现</strong></h3><p>首先我们用StringIndexer读取数据集中的“category”列，把字符型标签转化成标签索引，然后输出到“categoryIndex”列上，构建出一个新的DataFrame数据集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rawData =  Arrays.asList(RowFactory.create(0, &quot;a&quot;),</span><br><span class="line">                        RowFactory.create(1, &quot;b&quot;),</span><br><span class="line">                        RowFactory.create(2, &quot;c&quot;),</span><br><span class="line">                        RowFactory.create(3, &quot;a&quot;),</span><br><span class="line">                        RowFactory.create(4, &quot;a&quot;),</span><br><span class="line">                        RowFactory.create(5, &quot;c&quot;));</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df3 = spark.createDataFrame(rawData, schema);</span><br><span class="line">Dataset&lt;Row&gt; indexed3 = indexer.fit(df3).transform(df3);</span><br></pre></td></tr></table></figure><p>然后我们创建IndexToString对象，读取“categoryIndex”上的标签索引，获得原有数据集的字符型标签，然后再输出到“originalCategory”列上。最后，通过输出“originalCategory”列，就可以看到数据集中原有的字符标签了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IndexToString converter = new IndexToString().setInputCol(&quot;categoryIndex&quot;).setOutputCol(&quot;originalCategory&quot;);</span><br><span class="line">Dataset&lt;Row&gt; converted3 = converter.transform(indexed3);</span><br><span class="line">converted3.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+----------------+</span><br><span class="line">|id |category|categoryIndex|originalCategory|</span><br><span class="line">+---+--------+-------------+----------------+</span><br><span class="line">|0  |a       |0.0          |a               |</span><br><span class="line">|1  |b       |2.0          |b               |</span><br><span class="line">|2  |c       |1.0          |c               |</span><br><span class="line">|3  |a       |0.0          |a               |</span><br><span class="line">|4  |a       |0.0          |a               |</span><br><span class="line">|5  |c       |1.0          |c               |</span><br><span class="line">+---+--------+-------------+----------------+</span><br></pre></td></tr></table></figure><h1 id="四、OneHotEncoder（独热编码）"><a href="#四、OneHotEncoder（独热编码）" class="headerlink" title="四、OneHotEncoder（独热编码）"></a><strong>四、OneHotEncoder（独热编码）</strong></h1><h3 id="4-1、原理"><a href="#4-1、原理" class="headerlink" title="4.1、原理"></a><strong>4.1、原理</strong></h3><p>独热编码（One-hot encoding）将类别特征映射为二进制向量，其中只有一个有效值（为1，其余为0）。这样在诸如Logistic回归这样需要连续数值值作为特征输入的分类器中也可以使用类别（离散）特征。</p><p>One-Hot编码适合一些期望类别特征为连续特征的算法，比如说逻辑斯蒂回归等。</p><h3 id="4-2、代码实现"><a href="#4-2、代码实现" class="headerlink" title="4.2、代码实现"></a><strong>4.2、代码实现</strong></h3><p>首先创建一个DataFrame，其包含一列类别性特征，需要注意的是，在使用OneHotEncoder进行转换前，DataFrame需要先使用StringIndexer将原始标签数值化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rawData4 = Arrays.asList(RowFactory.create(0.0, 1.0),</span><br><span class="line">          RowFactory.create(1.0, 0.0),</span><br><span class="line">          RowFactory.create(2.0, 1.0),</span><br><span class="line">          RowFactory.create(0.0, 2.0),</span><br><span class="line">          RowFactory.create(0.0, 1.0),</span><br><span class="line">          RowFactory.create(2.0, 0.0));</span><br><span class="line"></span><br><span class="line">StructType schema4 = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;id&quot;,DataTypes.DoubleType,false,Metadata.empty()),</span><br><span class="line">        new StructField(&quot;category&quot;,DataTypes.DoubleType,false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df4 = spark.createDataFrame(rawData4, schema4);</span><br></pre></td></tr></table></figure><p>我们创建OneHotEncoder对象对处理后的DataFrame进行编码，可以看见，编码后的二进制特征呈稀疏向量形式，与StringIndexer编码的顺序相同，需注意的是最后一个Category（”b”）被编码为全0向量，若希望”b”也占有一个二进制特征，则可在创建OneHotEncoder时指定setDropLast(false)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">OneHotEncoderEstimator encoder = new OneHotEncoderEstimator()</span><br><span class="line">                               .setInputCols(new String[] &#123;&quot;id&quot;,&quot;category&quot;&#125;)</span><br><span class="line">                               .setOutputCols(new String[] &#123;&quot;categoryVec1&quot;,&quot;categoryVec2&quot;&#125;);</span><br><span class="line">Dataset&lt;Row&gt; encoded4 = encoder.fit(df4).transform(df4);</span><br><span class="line">encoded4.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+-------------+</span><br><span class="line">|id |category|categoryVec1 |categoryVec2 |</span><br><span class="line">+---+--------+-------------+-------------+</span><br><span class="line">|0.0|1.0     |(2,[0],[1.0])|(2,[1],[1.0])|</span><br><span class="line">|1.0|0.0     |(2,[1],[1.0])|(2,[0],[1.0])|</span><br><span class="line">|2.0|1.0     |(2,[],[])    |(2,[1],[1.0])|</span><br><span class="line">|0.0|2.0     |(2,[0],[1.0])|(2,[],[])    |</span><br><span class="line">|0.0|1.0     |(2,[0],[1.0])|(2,[1],[1.0])|</span><br><span class="line">|2.0|0.0     |(2,[],[])    |(2,[0],[1.0])|</span><br><span class="line">+---+--------+-------------+-------------+</span><br></pre></td></tr></table></figure><h1 id="五、VectorIndexer-向量类型索引化"><a href="#五、VectorIndexer-向量类型索引化" class="headerlink" title="五、VectorIndexer(向量类型索引化)"></a><strong>五、VectorIndexer(向量类型索引化)</strong></h1><h3 id="5-1、原理"><a href="#5-1、原理" class="headerlink" title="5.1、原理"></a><strong>5.1、原理</strong></h3><p>VectorIndexer帮助索引Vector数据集中的分类特征。 它既可以自动确定哪些特征是分类的，又可以将原始值转换为分类索引。 具体来说，它执行以下操作：</p><p>1、设置类型为Vector的输入列和参数maxCategories。<br>2、根据不同值的数量确定应分类的要素，其中最多具有maxCategories的要素被声明为分类。<br>3、为每个分类特征计算从0开始的分类索引。<br>4、为分类特征建立索引，并将原始特征值转换为索引。</p><p>索引分类特征允许诸如决策树和树组合之类的算法适当地处理分类特征，从而提高性能。</p><h3 id="5-2、代码实现"><a href="#5-2、代码实现" class="headerlink" title="5.2、代码实现"></a><strong>5.2、代码实现</strong></h3><p>首先，我们读入一个数据集DataFrame，然后使用VectorIndexer训练出模型，来决定哪些特征需要被作为类别特征，将类别特征转换为索引，这里设置maxCategories为2，即只有种类小于2的特征才被认为是类别型特征，否则被认为是连续型特征：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rawData5 = Arrays.asList(RowFactory.create(Vectors.dense(-1.0, 1.0, 1.0)),</span><br><span class="line">                RowFactory.create(Vectors.dense(-1.0, 3.0, 1.0)),</span><br><span class="line">                RowFactory.create(Vectors.dense(0.0, 5.0, 1.0)));</span><br><span class="line"></span><br><span class="line">StructType schema5 = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;features&quot;,new VectorUDT(),false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df5 = spark.createDataFrame(rawData5, schema5);</span><br><span class="line">df5.show(false);</span><br><span class="line">VectorIndexerModel indexModel = new VectorIndexer()</span><br><span class="line">                                .setInputCol(&quot;features&quot;)</span><br><span class="line">                                .setOutputCol(&quot;indexed&quot;)</span><br><span class="line">                                .setMaxCategories(2).fit(df5);</span><br><span class="line">Set&lt;Object&gt; categoricalFeatures = indexModel.categoryMaps().keySet();</span><br><span class="line">System.out.println(categoricalFeatures.mkString(&quot;,&quot;));</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0,2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; indexed5 = indexModel.transform(df5);</span><br><span class="line">indexed5.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+--------------+-------------+</span><br><span class="line">|features      |indexed      |</span><br><span class="line">+--------------+-------------+</span><br><span class="line">|[-1.0,1.0,1.0]|[1.0,1.0,0.0]|</span><br><span class="line">|[-1.0,3.0,1.0]|[1.0,3.0,0.0]|</span><br><span class="line">|[0.0,5.0,1.0] |[0.0,5.0,0.0]|</span><br><span class="line">+--------------+-------------+</span><br></pre></td></tr></table></figure></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ml-ml15" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-20T15:28:56.000Z"><a href="/machinelearning/ml-ml15/">2019-11-20</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml15/">机器学习（十五） 特征选择-卡方选择器</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/ml15.png" alt="卡方选择器"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml15/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml14" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-19T14:35:05.000Z"><a href="/machinelearning/ml-ml14/">2019-11-19</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml14/">机器学习（十四） 特征抽取–Word2Vec</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml14.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml14/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml13" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-17T14:35:05.000Z"><a href="/machinelearning/ml-ml13/">2019-11-17</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml13/">机器学习（十三） 特征抽取–CountVectorizer</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml13-1.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml13/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml12" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-16T14:35:05.000Z"><a href="/machinelearning/ml-ml12/">2019-11-16</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml12/">机器学习（十二） 特征提取 TF-IDF</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml12-0.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml12/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml11" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-15T14:35:05.000Z"><a href="/machinelearning/ml-ml11/">2019-11-15</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml11/">机器学习（十一） 机器学习工作流</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml11-0.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml11/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml10" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-14T11:35:05.000Z"><a href="/machinelearning/ml-ml10/">2019-11-14</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml10/">机器学习（十） 聚类</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml10.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml10/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml9" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-11T10:42:05.000Z"><a href="/machinelearning/ml-ml9/">2019-11-11</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml9/">机器学习（九） 协同过滤算法</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml9.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml9/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ebooks-bftzd" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-10-28T03:15:06.000Z"><a href="/read/ebooks-bftzd/">2019-10-28</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-bftzd/">《巴菲特之道（原书第3版）》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/bftzd.png" alt="巴菲特之道（原书第3版）"></p><p>中国一批知名价值投资者均受教于此书，获得财务自由，本书也被公认为总结巴菲特投资思想的较好的书。</p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file/21704187-408259751" target="view_window">《巴菲特之道（原书第3版）》</a> -<a href="https://545c.com/file/21704187-408259751" target="view_window"> 点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a><strong>内容介绍</strong></h1><p>在《巴菲除之道》第3版中，作者不仅更精彩地描述了巴菲特60载栉风沐雨的投资之旅，还将其获取亿万财富的方法分解为12个普通人都可以学习的投资准则。作者将巴菲特的方法放到更广阔的历史背景上，揭示了20世纪投资巨人们的主要观点，从格雷厄姆到菲利普·费雪、查理·芒格，更涉猎投资数学、投资心理学等范畴。新版再现了巴菲特做出重要投资决策的全过程，包括近两年的新案例，如收购亨氏食品、投资IBM，这些决策令其投资表现无可匹敌。彼得·林奇评价本书包含了一个成功投资家的思考与哲学，它提到的方法值得任何财务等级的人践行。</p><h1 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a><strong>作者介绍</strong></h1><p>罗伯特·哈格斯特朗，美盛投资顾问公司首席投资策略师、董事总经理。他是美国知名的以善于写作巴菲特及芒格思想理念而著称的作家，已出版九本投资图书，包括《纽约时报》畅销书《巴菲特之道》（原书第3版）（本书第1版即销售120万册）、《巴菲特的投资组合》（芒格推荐书目）、《查理·芒格的智慧：投资的格栅理论》，以及《NASCAR：驱动体育的商业》、《侦探与投资者：来自传奇特工的未曾揭示的投资技巧》 等。</p><p>罗伯特本科与硕士均毕业于维拉诺瓦大学，并持有CFA证书。他与家人居住于美国宾夕法尼亚州维拉诺瓦。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zycx_min.png" alt="卓有成效的管理者"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/tzlc_min.png" alt="《投资理财从入门到精通》"><br><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》</a> &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/bft_min.png" alt="世界首富沃伦·巴菲特传"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/xzb_min.png" alt="《小资本赚钱100招》"><br><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; <a href="http://www.wangpengcufe.com/read/ebooks-bft/">《世界首富沃伦·巴菲特传》</a>&nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-xzb/">《小资本赚钱100招》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-wzqgz/">《白领理财日记之玩转钱规则》</a></p><p>下一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-bft/">《一个美国资本家的成长—世界首富沃伦·巴菲特传》</a></p><p>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-bk" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-10-27T03:15:06.000Z"><a href="/read/ebooks-bk/">2019-10-27</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-bk/">《爆款：如何打造超级IP》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/bk.png" alt="爆款：如何打造超级IP"></p><p>哈佛商学院IP运营与产品管理方法论第一书，翻转长尾理论的重要著作</p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file/21704187-408259756" target="view_window">《爆款：如何打造超级IP》</a> -<a href="https://545c.com/file/21704187-408259756" target="view_window"> 点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a><strong>内容介绍</strong></h1><p>哈佛商学院最受欢迎的教授安妮塔•埃尔伯斯，就互联网时代下的IP运营公司进行了前瞻性的研究。她在这本开创性的著作中指出，在竞争激烈的商业社会，产品脱颖而出的关键是采取看似高风险的“爆款策略”。比如打造超级IP，制作成本与营销费用极度高昂的电影、电视节目、歌曲、书籍——其实是最能确保公司成功的方法。</p><p>埃尔伯斯提出，IP即品牌，爆款的逻辑是赢者通吃。执行高风险、高报酬的计划，其实是最佳的、长期制胜的商业模式。我们需要对长尾理论进行修正，即使尾巴很重要，但要成功，还需精心打造头部。互联网时代，一切行业皆娱乐业，如何用数字技术打造超级IP的逻辑推动商机，如何在所处产业推出爆款，《爆款：如何打造超级IP》将是第一本必读的重要著作。</p><h1 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a><strong>作者介绍</strong></h1><p>【美】安妮塔•埃尔伯斯（Anita Elberse）</p><p>哈佛商学院林肯•法林工商管理教授（Lincoln Filene Professor of Business Administration）。她是校史上最年轻得到终身教席的女性教授，作品散见于《纽约时报》《华尔街日报》《财富》杂志。</p><p>安妮塔•埃尔伯斯深入华纳兄弟、二十一世纪福克斯公司、大中央出版社、索尼、YouTube等IP产业运营公司的内部，取得大量真实的故事与案例，公开第一手资料，揭秘爆款背后的秘密。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zycx_min.png" alt="卓有成效的管理者"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/tzlc_min.png" alt="《投资理财从入门到精通》"><br><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》</a> &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gkwsm_min.png" alt="顾客为什么购买"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/qmgx_min.png" alt="《亲密关系》"><br><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; <a href="http://www.wangpengcufe.com/read/ebooks-gkwsm/">《顾客为什么购买》</a>&nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gkwsm/">《顾客为什么购买》</a></p><p>下一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-ddcb/">《一本书读懂财报》</a></p><p>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-gf" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-10-27T01:15:06.000Z"><a href="/read/ebooks-gf/">2019-10-27</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-gf/">《稻盛和夫_干法》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/gf.png" alt="稻盛和夫_干法"></p><p>工作可以起到锻炼心志、提升人性的作用，长时间不懈的工作可以磨砺心志</p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file/21704187-408259759" target="view_window">《稻盛和夫_干法》 </a>-<a href="https://545c.com/file/21704187-408259759" target="view_window"> 点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><h1 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a><strong>作者简介</strong></h1><p>稻盛和夫，1932年出生于日本鹿儿岛，鹿儿岛大学工学部毕业。27岁创办京都陶瓷株式会社（现名京瓷Kyocera），52岁创办第二电电（现名KDDI，仅次于NTT的日本第二大通讯公司），这两家公司均为世界500强企业。2010年执掌日航，令濒临破产的日航迅速扭亏为盈。同时还担任京都商工会议所名誉会头，瑞典皇家科学院海外特别会员，卡耐基协会名誉理事，美国科学院海外会员。1984年获得日本政府颁发的紫绶褒章。此外，还分别获得美国阿尔弗雷德大学、丹佛大学、圣地亚哥大学，英国克莱菲尔德大学名誉博士学位。并于2010年任青岛市经济顾问，2006年荣膺“中日友好使者”，获首届“光明公益奖”最佳个人奖，任东莞市荣誉市民，贵阳市荣誉市民，东莞市、天津市高级顾问，任南京大学顾问教授等。</p><p>1984年创立“稻盛集团”，同时创设“京都奖”，每年表扬对人类社会发展具有卓越贡献的人士。此外，出任以年轻一辈经营者为招收对象的“盛和塾”塾长，其经营哲学被日本企业界奉为圭臬。</p><h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a><strong>内容介绍</strong></h1><p>除非喜欢自己所做的工作，否则永远无法成功。”在我们周围，听到的多是报怨和无奈，谈得最少的是工作的开心和乐趣。其实不管自己处在何种境遇，都要抱着积极的心态朝前看，任何时候都要拼命工作，持续努力，这才是最重要的。并且对每天做过的事情，老老实实地进行反省，就可以避免工作上的失败，并能够不断地完善自我。无论遇到什么事情都要感谢，既然选择了这份工作这个岗位，就必须无条件地接受它的全部，包括好的和不好的，持有随时感谢心态非常重要。很多时候，我们在工作的时候，总是说尽力而为；遇到困难的时候，总是说尽力而为。我们可以原谅这种说法，但不能原谅这种做法。做任何事情不能本着尽力而为，竭尽全力是我们工作所要达到的境界。古语云：世上无难事，只怕有心人。我们要在工作上树立必胜的信心，在战略上藐视困难，在战术上重视困难，付出自己百分之百的努力，成功的机会是不会错过的。想到困难，正视困难，克服困难，解决困难，到完成工作任务的过程，其实就是一个自我的心理斗争过程，就像稻盛先生所说：当你竭尽全力时神灵将会现身。只有付出”不亚于任何人的努力”才是人生和事业成功的最强动力。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/ambjy_min.png" alt="稻盛和夫_阿米巴经营"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="http://wangpengcufe.com/jyxyzx_min.png" alt="《稻盛和夫-经营为什么需要哲学》"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/tzlc_min.png" alt="《投资理财从入门到精通》"><br><a href="http://www.wangpengcufe.com/read/ebooks-ambjy/">《稻盛和夫_阿米巴经营》</a> &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-jyxyzx/">《稻盛和夫-经营为什么需要哲学》</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/yzsk_min.png" alt="优质思考的力量"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/qmgx_min.png" alt="《亲密关系》"><br><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; <a href="http://www.wangpengcufe.com/read/ebooks-yzsk/">《优质思考的力量》</a>&nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-ambjy/">《稻盛和夫_阿米巴经营》</a></p><p>下一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-jyxzyx/">《稻盛和夫-经营为什么需要哲学》</a></p><p>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-ambjy" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-10-26T03:15:06.000Z"><a href="/read/ebooks-ambjy/">2019-10-26</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-ambjy/">《稻盛和夫_阿米巴经营》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/ambjy.png" alt="稻盛和夫_阿米巴经营"></p><p>阿米巴经营模式被誉为“京瓷经营成功的两大支柱之一”</p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file/21704187-408259758" target="view_window">《稻盛和夫_阿米巴经营》</a> -<a href="https://545c.com/file/21704187-408259758" target="view_window"> 点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a><strong>内容介绍</strong></h1><p>阿米巴经营模式被誉为“京瓷经营成功的两大支柱之一”。本书详细阐述了阿米巴经营的操作方法，总结了稻盛和夫的经营观点和实践经验，是学习阿米巴经营的必读书。</p><p>阿米巴经营基于牢固的经营哲学和精细的部门独立核算管理，将企业划分为“小集体”，像自由自在的重复进行细胞分裂的阿米巴——以各个阿米巴为核心，自行制订计划，独立核算，持续自主成长，让每一位员工成为主角，全员参与经营，打造激情燃烧的集体，依靠全体智慧和努力完成企业经营目标，实现企业的飞速发展。</p><p>全球超过500家的企业引进了阿米巴经营模式，业绩得以大幅提升。</p><h1 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a><strong>作者介绍</strong></h1><p>稻盛和夫，被誉为日本的“经营之圣、人生之师”，日本企业家兼哲学家第一人。</p><p>1932年出生于日本鹿儿岛市，毕业于鹿儿岛大学工学部。1959年创办京都陶瓷株式会社（现日本京瓷株式会社）。1984年创办第二电电株式会社（现名KDDI，目前在日本是仅次于NTT的第二大通信公司）。这两家企业都进入过世界500强。2010年出任日本航空（JAL）株式会社的会长，当年就让破产重建的日航大幅度扭亏为盈，并创造了日航历*高的利润。这个利润也是当年全世界航空企业中的最高利润。现任京瓷和日航名誉会长，KDDI最高顾问。1983年创办盛和塾，向企业家塾生义务传授经营哲学。现在全世界的盛和塾塾生已超过10000人。1984年创立稻盛财团，同年创设“京都奖”，被誉为“亚洲诺贝尔奖”。</p><p>稻盛和夫曾出版过10多本介绍企业经营理念和人生哲学的图书，《阿米巴经营》是他最重要的代表作。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zycx_min.png" alt="卓有成效的管理者"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/tzlc_min.png" alt="《投资理财从入门到精通》"><br><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》</a> &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gf_min.png" alt="稻盛和夫_干法"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/jyxyzx_min.png" alt="《稻盛和夫-经营为什么需要哲学》"><br><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; <a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a>&nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-jyxyzx/">《稻盛和夫-经营为什么需要哲学》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gxddybs/">《如何高效读懂一本书》</a></p><p>下一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a></p><p>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><nav id="pagination"><a href="/page/2/" class="alignright next">下一页</a><div class="clearfix"></div></nav></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>63</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>21</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>85</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>21</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔（简书）</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">机器学习（csdn博客）</a></li><li><a href="https://zhuanlan.zhihu.com/c_1182309165824901120" target="_blank">机器学习（知乎）</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li><li><a href="https://baippt.com/" target="_blank">ppt模板免费下载</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2020 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>