<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta name="baidu-site-verification" content="kZmc8XTvrB"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?458cfc1440d057b6b8799400c6b2e2bf";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta charset="utf-8"><title>机器学习 and 数据科学</title><meta name="author" content="王小鹏  京ICP备19037345号-1"><meta name="keywords" content="PDF电子书下载,电子书，PDF下载"><meta name="description" content="PDF电子书下载,电子书，PDF下载"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:site_name" content="机器学习 and 数据科学"><meta property="og:image" content><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="机器学习 and 数据科学" type="application/atom+xml"><link rel="stylesheet" href="/css/style.css" media="screen" type="text/css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--></head><body><header id="header" class="inner"><div class="alignleft"><h1><a href="/">机器学习 and 数据科学</a></h1><h2><a href="/">明天幸福今天修</a></h2></div><nav id="main-nav" class="alignright"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/about">关于</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div id="content" class="inner"><div id="main-col" class="alignleft"><div id="wrapper"><article id="post-main" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T06:42:05.000Z"><a href="/navigate/main/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/navigate/main/">文章汇总</a></h1></header><div class="e-content entry" itemprop="articleBody"><ul><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml1/">机器学习 (一) Spark MLlib介绍</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml2/">机器学习 (二) 基本数据类型</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml3/">机器学习 (三) 基本的统计工具</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml4/">机器学习 (四) 分类</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml5/">机器学习 (五) 逻辑斯蒂回归</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml6/">机器学习 (六) 决策树</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml7/">机器学习 (七) 奇异值分解-SVD</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml8/">机器学习 (八) 主成分分析-PCA</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml9/">机器学习 (九) 协同过滤算法</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml10/">机器学习 (十) 聚类</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml11/">机器学习 (十一) 机器学习工作流</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml12/">机器学习 (十二) 特征提取 TF-IDF</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml13/">机器学习 (十三) 特征抽取–CountVectorizer</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml14/">机器学习 (十四) 特征抽取–Word2Vec</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml15/">机器学习 (十五) 特征选择-卡方选择器</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/ml-ml16/">机器学习 (十六) 特征变换-标签和索引的转化</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/python-python1/">案例（一） 利用RFM模型做用户价值分析</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/python-python2/">案例（二） 如何把python项目部署到linux服务器上</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml1/">python 机器学习（一）机器学习概述与特征工程</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml2/">python 机器学习（二）分类算法-k近邻算法</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml3/">python 机器学习（三）分类算法-朴素贝叶斯</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml4/">python 机器学习（四）分类算法-决策树</a></p></li><li><p><a href="http://www.wangpengcufe.com/machinelearning/pythonml-pythonml5/">python机器学习（五）回归算法-线性回归</a></p></li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-15T14:47:27.000Z"><a href="/read/ebooks/">2019-11-15</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks/">好书精选</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>掌握已知，探索未知，为日益精进而阅读！</p><p><a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=68bf1f82b97f7e775c9a2bf79650d7dfae359b2ea236dc4298408bcf41f055fb"><img border="0" src="//pub.idqqimg.com/wpa/images/group.png" alt="电子书分享" title="电子书分享"></a></p><p><strong>发现自己</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-dls/">《断离舍》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cdswldq/">《拆掉思维里的墙》</a></li></ul><p><strong>卓越情商与社交力</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-qsghsh/">《所谓情商高，就是会说话》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gtdys/">《沟通的艺术》</a></li></ul><p><strong>管理</strong></p><ul><li><a href="https://545c.com/file/21704187-397370436" target="view_window">《反脆弱：从不确定性中获益》（美）纳西姆·尼古拉斯·塔勒布</a></li><li><a href="https://545c.com/file/21704187-397841620" target="view_window">《舍与得的人生经营课》 赵丽荣</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-hfsxy/">《哈佛商学院管理全书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》（美）德鲁克</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gxq/">《滚雪球：巴菲特和他的财富人生》-艾丽斯·施罗德</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-lj/">《雷军：从金山软件到小米手机》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-sycs/">《一本书读懂商业常识》 (董智轩)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zkl/">《自控力》凯利·麦格尼格尔</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-jycy/">《精益创业》(美)埃里克·莱斯</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bsjdpy/">《把时间当作朋友》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-glsj/">《聪明人是怎样管理时间的》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-jjl/">《拒绝力》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cjjy/">《超级记忆》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zqjd/">《如何做出正确决定》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yzsk/">《优质思考的力量》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gxddybs/">《如何高效读懂一本书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-ambjy/">《稻盛和夫_阿米巴经营》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-jyxyzx/">《稻盛和夫-经营为什么需要哲学》</a></li></ul><p><strong>理财</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-xzf/">《薛兆丰经济学讲义》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qbbfbb/">《穷爸爸富爸爸》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qclbd/">《穷查理宝典 查理·芒格智慧箴言录》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》(墨知行)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qmz/">《给穷忙族看的理财书》(比尔李)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-qrqkd/">《穷人穷口袋，富人富脑袋》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-ffrs/">《富足人生的八大支柱 别为金钱焦虑丛书》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-glmyfq/">《如何有效管理每一分钱：用会计思维增值你的财富》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-fldz/">《复利打造百万富翁：持续被动收入，通往财务自由》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-lcrms/">《我的第一本理财入门书 畅销珍藏版》(武庆新)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cfdfp/">《财富的分配》（美）克拉克</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bblfh/">《巴比伦富豪 理财的十二条黄金定律》（美）乔治.克拉森</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wzqgz/">《白领理财日记之玩转钱规则》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bftzd/">《巴菲特之道（原书第3版）》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bft/">《一个美国资本家的成长—世界首富沃伦·巴菲特传》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-xzb/">《小资本赚钱100招》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-xgqq/">《小狗钱钱》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cwzyzl/">《财务自由之路：7年内赚到你的第一个1000万》</a></li></ul><p><strong>营销</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-gkwsm/">《顾客为什么购买》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-bk/">《爆款：如何打造超级IP》</a></li></ul><p><strong>金融</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-ddcb/">《一本书读懂财报》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-xdjjx/">《小岛经济学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wdgsrs/">《荣辱二十年 我的股市人生》(阚治东)</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-hdjjx/">《海盗经济学》（美）彼得·里森</a></li></ul><p><strong>家庭</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》（美）罗兰·米勒</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-fblgt/">《非暴力沟通》（美）卢森堡</a></li><li><a href="https://545c.com/file/21704187-397349671" target="view_window">《正面管教》（美）简·尼尔森</a></li></ul><p><strong>心理</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-xwx/">《行为心理学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-whzz/">《乌合之众：大众心理研究》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-hzxl/">《如何把握孩子心理》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-flyd/">《弗洛伊德，性学与爱情心理学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-cjgt/">《超级沟通心理学》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-yxl/">《影响力》</a></li></ul><p><strong>科技</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-rljs/">《人类简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wwjs/">《万物简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-wljs/">《未来简史》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-sjjs/">《时间简史》</a></li></ul><p><strong>计算机</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-tjjqxx/">《图解机器学习》</a></li></ul><p><strong>历史</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks-sj/">《史记》</a></li><li><a href="http://www.wangpengcufe.com/read/ebooks-zgf/">《曾国藩：又笨又慢平天下》</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-download" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T08:00:27.000Z"><a href="/datadownload/download/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/datadownload/download/">电子书</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><strong>阅读Top10</strong></p><ul><li>1、<a href="http://www.wangpengcufe.com/read/ebooks-rljs/">《人类简史》</a></li><li>2、<a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》</a></li><li>3、<a href="https://545c.com/file/21704187-397349671" target="view_window"> 《正面管教》</a></li><li>4、<a href="https://545c.com/file/21704187-398029061" target="view_window"> 《沉思录》</a></li><li>5、<a href="http://www.wangpengcufe.com/read/ebooks-xdjjx/">《小岛经济学》</a></li><li>6、<a href="http://www.wangpengcufe.com/read/ebooks-ddcb/">《一本书读懂财报》</a></li><li>7、<a href="http://www.wangpengcufe.com/read/ebooks-sjjs/">《时间简史》</a></li><li>8、<a href="http://www.wangpengcufe.com/read/ebooks-qbbfbb/">《穷爸爸富爸爸》</a></li><li>9、<a href="http://www.wangpengcufe.com/read/ebooks-bsjdpy/">《把时间当作朋友》</a></li><li>10、<a href="http://www.wangpengcufe.com/read/ebooks-xwx/">《行为心理学》</a></li></ul><p><strong>更多阅读</strong></p><ul><li><a href="http://www.wangpengcufe.com/read/ebooks/">好书精选</a></li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml5" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-04-03T04:45:16.000Z"><a href="/machinelearning/pythonml-pythonml5/">2020-04-03</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml5/">python机器学习（五）回归算法-线性回归</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="https://upload-images.jianshu.io/upload_images/7289495-d1d3209b329261b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线性回归"></p><h1 id="一、线性回归的概念"><a href="#一、线性回归的概念" class="headerlink" title="一、线性回归的概念"></a>一、线性回归的概念</h1><h5 id="1-1、定义"><a href="#1-1、定义" class="headerlink" title="1.1、定义"></a>1.1、定义</h5><p>线性回归通过一个或者多个自变量与因变量之间之间进行建模的回归分析。其中特点为一个或多个称为回归系数的模型参数的线性组合。</p><p><strong>优点：</strong><br>结果易于理解，计算不复杂。</p><p><strong>缺点：</strong><br>对非线性的数据拟合不好。</p><p><strong>适用数据类型：</strong><br>数值型和标称型。</p><h5 id="1-2、分类"><a href="#1-2、分类" class="headerlink" title="1.2、分类"></a>1.2、分类</h5><p><strong>一元线性回归：</strong><br>涉及到的变量只有一个。</p><p><strong>多元线性回归：</strong><br>涉及到的变量两个或两个以上。</p><h5 id="1-3、公式"><a href="#1-3、公式" class="headerlink" title="1.3、公式"></a>1.3、公式</h5><p><img src="https://upload-images.jianshu.io/upload_images/7289495-b6bdc1c15b09727c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中𝑤,𝑥为矩阵：<img src="https://upload-images.jianshu.io/upload_images/7289495-f414dd162c70aa41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="𝑤,𝑥为矩阵"></p><h1 id="二、线性回归的实例"><a href="#二、线性回归的实例" class="headerlink" title="二、线性回归的实例"></a>二、线性回归的实例</h1><h5 id="2-1、单变量实例"><a href="#2-1、单变量实例" class="headerlink" title="2.1、单变量实例"></a>2.1、单变量实例</h5><p>房子价格与房子面积<br><img src="https://upload-images.jianshu.io/upload_images/7289495-b067cd091dbd501e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="单变量实例"></p><h5 id="2-2、多变量实例"><a href="#2-2、多变量实例" class="headerlink" title="2.2、多变量实例"></a>2.2、多变量实例</h5><p>期末成绩：0.7×考试成绩+0.3×平时成绩<br>西瓜好坏：0.2×色泽+0.5×根蒂+0.3×敲声<br><img src="https://upload-images.jianshu.io/upload_images/7289495-6ebb21fabea01890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="多变量实例"></p><h1 id="三、损失函数"><a href="#三、损失函数" class="headerlink" title="三、损失函数"></a>三、损失函数</h1><p>损失函数是一个贯穿整个机器学习重要的一个概念，大部分机器学习算法都会有误差，我们得通过显性的公式来描述这个误差，并且将这个误差优化到最小值。</p><h5 id="3-1、损失原因"><a href="#3-1、损失原因" class="headerlink" title="3.1、损失原因"></a>3.1、损失原因</h5><p>预测结果与真实值是有一定的误差。<br><img src="https://upload-images.jianshu.io/upload_images/7289495-a3aff1dc51b19aef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="损失函数"></p><h5 id="3-2、损失函数定义"><a href="#3-2、损失函数定义" class="headerlink" title="3.2、损失函数定义"></a>3.2、损失函数定义</h5><p>损失函数代表了误差的大小，用公式表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-b0a531bf9ded4567.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="损失函数公式"><br>又称最小二乘法。</p><p>对于线性回归模型，将模型与数据点之间的距离差之和做为衡量匹配好坏的标准，误差越小,匹配程度越大。我们要找的模型就是需要将f(x)和我们的真实值之间最相似的状态。</p><p>损失函数由W决定，那么如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）<br><img src="https://upload-images.jianshu.io/upload_images/7289495-f014a751a711bb4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="减少损失函数"></p><h5 id="3-3、减小损失函数的2种方式"><a href="#3-3、减小损失函数的2种方式" class="headerlink" title="3.3、减小损失函数的2种方式"></a>3.3、减小损失函数的2种方式</h5><h5 id="方式一：最小二乘法之正规方程"><a href="#方式一：最小二乘法之正规方程" class="headerlink" title="方式一：最小二乘法之正规方程"></a>方式一：最小二乘法之正规方程</h5><p>求解：<img src="https://upload-images.jianshu.io/upload_images/7289495-08e115ff92b3edac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="最小二乘法之正规方程"><br>𝑋为特征值矩阵，𝑦为目标值矩阵。<br><img src="https://upload-images.jianshu.io/upload_images/7289495-2e5f7ac946294576.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="损失函数直观图"></p><p>缺点：当特征过于复杂，求解速度太慢。<br>对于复杂的算法，不能使用正规方程求解(逻辑回归等)</p><h5 id="方式二：最小二乘法之梯度下降"><a href="#方式二：最小二乘法之梯度下降" class="headerlink" title="方式二：最小二乘法之梯度下降"></a>方式二：最小二乘法之梯度下降</h5><p><img src="https://upload-images.jianshu.io/upload_images/7289495-a5b09f063fb5bc58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>𝛼为学习速率，需要手动指定，其中<img src="https://upload-images.jianshu.io/upload_images/7289495-020e33036a96e5ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="方向">表示方向<br>理解：沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后<br>更新W值<br><img src="https://upload-images.jianshu.io/upload_images/7289495-b69d8b234bc83dff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="最小二乘法之梯度下降"><br><img src="https://upload-images.jianshu.io/upload_images/7289495-aff3601d6c5c14ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="最小二乘法之梯度下降"></p><h5 id="3-4、两种方式对比："><a href="#3-4、两种方式对比：" class="headerlink" title="3.4、两种方式对比："></a>3.4、两种方式对比：</h5><p><img src="https://upload-images.jianshu.io/upload_images/7289495-dd802975efddca68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="两种方式对比"></p><h1 id="四、线性回归API"><a href="#四、线性回归API" class="headerlink" title="四、线性回归API"></a>四、线性回归API</h1><h5 id="4-1、普通最小二乘法线性回归"><a href="#4-1、普通最小二乘法线性回归" class="headerlink" title="4.1、普通最小二乘法线性回归"></a>4.1、普通最小二乘法线性回归</h5><p>sklearn.linear_model.LinearRegression()<br>coef_：回归系数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">reg = LinearRegression()</span><br><span class="line"># 方法</span><br><span class="line">reg.fit(X,y,sample_weight = None)  #使用X作为训练数据拟合模型，y作为X的类别值。X，y为数组或者矩阵</span><br><span class="line"></span><br><span class="line">reg.predict([[X,y]])  # 预测提供的数据对应的结果</span><br><span class="line"> </span><br><span class="line">#属性</span><br><span class="line">reg.coef_   #表示回归系数w=(w1,w2....)</span><br></pre></td></tr></table></figure><h5 id="4-2、通过使用SGD最小线性模型"><a href="#4-2、通过使用SGD最小线性模型" class="headerlink" title="4.2、通过使用SGD最小线性模型"></a>4.2、通过使用SGD最小线性模型</h5><p>sklearn.linear_model.SGDRegressor( )<br>coef_：回归系数</p><h5 id="4-3、带有正则化的线性回归"><a href="#4-3、带有正则化的线性回归" class="headerlink" title="4.3、带有正则化的线性回归"></a>4.3、带有正则化的线性回归</h5><p>sklearn.linear_model.Ridge<br>具有l2正则化的线性最小二乘法<br>alpha:正则化力度<br>coef_:回归系数</p><h1 id="五、实现案例"><a href="#五、实现案例" class="headerlink" title="五、实现案例"></a>五、实现案例</h1><h5 id="波士顿房价数据分析流程"><a href="#波士顿房价数据分析流程" class="headerlink" title="波士顿房价数据分析流程:"></a>波士顿房价数据分析流程:</h5><h5 id="5-1、数据获取"><a href="#5-1、数据获取" class="headerlink" title="5.1、数据获取"></a>5.1、数据获取</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">from sklearn.linear_model import LinearRegression, SGDRegressor,  Ridge, LogisticRegression</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.metrics import mean_squared_error, classification_report</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"># 获取数据</span><br><span class="line">lb = load_boston()</span><br></pre></td></tr></table></figure><h5 id="5-2、数据分割"><a href="#5-2、数据分割" class="headerlink" title="5.2、数据分割"></a>5.2、数据分割</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 分割数据集到训练集和测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=0.25)</span><br></pre></td></tr></table></figure><h5 id="5-3、训练与测试数据标准化处理"><a href="#5-3、训练与测试数据标准化处理" class="headerlink" title="5.3、训练与测试数据标准化处理"></a>5.3、训练与测试数据标准化处理</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 特征值和目标值是都必须进行标准化处理, 实例化两个标准化API</span><br><span class="line">std_x = StandardScaler()</span><br><span class="line"></span><br><span class="line">x_train = std_x.fit_transform(x_train)</span><br><span class="line">x_test = std_x.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 目标值</span><br><span class="line">std_y = StandardScaler()</span><br><span class="line"></span><br><span class="line">y_train = std_y.fit_transform(y_train)</span><br><span class="line">y_test = std_y.transform(y_test)</span><br></pre></td></tr></table></figure><h5 id="5-4、线性回归模型和梯度下降估计对房价进行预测"><a href="#5-4、线性回归模型和梯度下降估计对房价进行预测" class="headerlink" title="5.4、线性回归模型和梯度下降估计对房价进行预测"></a>5.4、线性回归模型和梯度下降估计对房价进行预测</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 正规方程求解方式预测结果</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line">print(lr.coef_)</span><br><span class="line">保存训练好的模型</span><br><span class="line">joblib.dump(lr, &quot;test.pkl&quot;)</span><br><span class="line"># 预测测试集的房子价格</span><br><span class="line">y_lr_predict = std_y.inverse_transform(lr.predict(x_test))</span><br><span class="line">print(&quot;正规方程测试集里面每个房子的预测价格：&quot;, y_lr_predict)</span><br><span class="line">print(&quot;正规方程的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_lr_predict))</span><br><span class="line"></span><br><span class="line"># 梯度下降进行房价预测</span><br><span class="line">sgd = SGDRegressor()</span><br><span class="line">sgd.fit(x_train, y_train)</span><br><span class="line">print(sgd.coef_)</span><br><span class="line"># 预测测试集的房子价格</span><br><span class="line">y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test))</span><br><span class="line">print(&quot;梯度下降测试集里面每个房子的预测价格：&quot;, y_sgd_predict)</span><br><span class="line">print(&quot;梯度下降的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line"></span><br><span class="line"># 岭回归进行房价预测</span><br><span class="line">rd = Ridge(alpha=1.0)</span><br><span class="line">rd.fit(x_train, y_train)</span><br><span class="line">print(rd.coef_)</span><br><span class="line"># 预测测试集的房子价格</span><br><span class="line">y_rd_predict = std_y.inverse_transform(rd.predict(x_test))</span><br><span class="line">print(&quot;梯度下降测试集里面每个房子的预测价格：&quot;, y_rd_predict)</span><br><span class="line">print(&quot;梯度下降的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_rd_predict))</span><br></pre></td></tr></table></figure><h1 id="六、回归的性能评估"><a href="#六、回归的性能评估" class="headerlink" title="六、回归的性能评估"></a>六、回归的性能评估</h1><h5 id="6-1、均方差误差评估机制-（MSE）："><a href="#6-1、均方差误差评估机制-（MSE）：" class="headerlink" title="6.1、均方差误差评估机制 （MSE）："></a>6.1、均方差误差评估机制 （MSE）：</h5><p><img src="https://upload-images.jianshu.io/upload_images/7289495-aa60bf2ee9693f1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="均方差误差评估机制"></p><p>注：𝑦^𝑖为预测值，¯𝑦为真实值。</p><h5 id="6-2、回归评估API："><a href="#6-2、回归评估API：" class="headerlink" title="6.2、回归评估API："></a>6.2、回归评估API：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error</span><br><span class="line">mean_squared_error(y_true, y_pred)</span><br><span class="line">均方误差回归损失</span><br><span class="line">y_true:真实值</span><br><span class="line">y_pred:预测值</span><br><span class="line">return:浮点数结果</span><br></pre></td></tr></table></figure><p>注：真实值，预测值为标准化之前的值。</p><h1 id="七、线性回归的可能问题"><a href="#七、线性回归的可能问题" class="headerlink" title="七、线性回归的可能问题"></a>七、线性回归的可能问题</h1><p>训练数据训练的很好啊，误差也不大，为什么在测试集上面有问题呢？机器学习可能存在过拟合和欠拟合的问题。如下图：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-5fbb11953b73903f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="过拟合和欠拟合"></p><p><img src="https://upload-images.jianshu.io/upload_images/7289495-3f12dd3f1d9552ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="过拟合和欠拟合"></p><h5 id="7-1、过拟合"><a href="#7-1、过拟合" class="headerlink" title="7.1、过拟合"></a>7.1、过拟合</h5><p>一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在训练数据外的数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)，如下图：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-6fd703cd43910634.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="过拟合"></p><h6 id="产生原因："><a href="#产生原因：" class="headerlink" title="产生原因："></a>产生原因：</h6><p>原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点。</p><h6 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h6><ul><li>进行特征选择，消除关联性大的特征（很难做）</li><li>交叉验证（建议使用）</li><li>正则化 （了解即可）</li></ul><h4 id="7-2、欠拟合"><a href="#7-2、欠拟合" class="headerlink" title="7.2、欠拟合"></a>7.2、欠拟合</h4><p>一个假设在训练数据上不能获得更好的拟合， 但是在训练数据外的数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单)。如下图：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-6407b29f79f29e1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="欠拟合"></p><h6 id="产生原因：-1"><a href="#产生原因：-1" class="headerlink" title="产生原因："></a>产生原因：</h6><p>学习到数据的特征过少。</p><h6 id="解决办法：-1"><a href="#解决办法：-1" class="headerlink" title="解决办法："></a>解决办法：</h6><p>增加数据的特征数量。</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-qsghsh" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-03-11T05:08:27.000Z"><a href="/read/ebooks-qsghsh/">2020-03-11</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-qsghsh/">《所谓情商高，就是会说话》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/qsghsh.jpg" alt="所谓情商高就是会说话"></p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file296/21704187-428373057" target="view_window">《所谓情商高就是会说话》 </a>- <a href="https://545c.com/file296/21704187-428373057" target="view_window">点击下载</a></li></ul><h1 id="正版购买"><a href="#正版购买" class="headerlink" title="正版购买"></a><strong>正版购买</strong></h1><ul><li><a href="https://u.jd.com/YlyvBs" target="view_window">京东购买 ￥36.80 </a>- <a href="https://u.jd.com/YlyvBs" target="view_window">点击下载</a></li><li><a href="http://u.xc6.ink/AA2doy" target="view_window">淘宝购买 ￥23.00 </a>- <a href="http://u.xc6.ink/AA2doy" target="view_window">点击下载</a></li><li><a href="https://p.pinduoduo.com/mk3Q1KXH" target="view_window">拼多多购买 ￥19.60 </a>- <a href="https://p.pinduoduo.com/mk3Q1KXH" target="view_window">点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><hr><h1 id="作品简介"><a href="#作品简介" class="headerlink" title="作品简介"></a><strong>作品简介</strong></h1><p>说话不只是一种“感觉”，还是一门“技术”！就像烹饪有食谱一样，说话也有它的“秘方”。古往今来，但凡不通说话之道者，都难成大事，而能成事者，一定在语言方面具有其独特的能力。一句恰到好处的话，甚至可以改变一个人的命运。人际沟通中怎样措辞？怎样把握说话的场合和时机？如何领略说话精髓，悟透说话之道，很快在众人中脱颖而出？</p><p>作者通过自己多年来掌握的语言措辞技巧告诉大家该如何修炼说话能力，如何提高自己的说话水平。在本书中，你可以通过熟悉“7个突破口”和“8个技巧”，并阅读大量实践故事，更好地掌握并运用语言措辞的技巧，使自己成为说话高手。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zkl_min.png" alt="自控力．实操篇 - 凯利·麦格尼格尔"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gxq_min.png" alt="《滚雪球：巴菲特和他的财富人生（畅销版）（共两册）-艾丽斯·施罗德》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-zkl/">《自控力》 凯利·麦格尼格尔</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-gxq/">《滚雪球：巴菲特和他的财富人生》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/xdjjx_min.png" alt="小岛经济学"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/sycs_min.png" alt="《一本书读懂商业常识_董智轩》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-xdjjx/">《小岛经济学》</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-sycs/">《一本书读懂商业常识_董智轩》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-dls/">《断离舍》</a><br>下一篇：- <a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》</a><br>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-gtdys" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-03-11T04:49:27.000Z"><a href="/read/ebooks-gtdys/">2020-03-11</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-gtdys/">《沟通的艺术》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/gtdys.jpg" alt="沟通的艺术"></p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file296/21704187-428370528" target="view_window">《沟通的艺术》 </a>- <a href="https://545c.com/file296/21704187-428370528" target="view_window">点击下载</a></li></ul><h1 id="正版购买"><a href="#正版购买" class="headerlink" title="正版购买"></a><strong>正版购买</strong></h1><ul><li><a href="https://u.jd.com/70397b" target="view_window">京东购买 ￥32.00 </a>- <a href="https://u.jd.com/70397b" target="view_window">点击下载</a></li><li><a href="http://u.xc1.ink/AA2do3" target="view_window">淘宝购买 ￥32.00 </a>- <a href="http://u.xc1.ink/AA2do3" target="view_window">点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><hr><h1 id="作品简介"><a href="#作品简介" class="headerlink" title="作品简介"></a><strong>作品简介</strong></h1><p>本书分为“看入人里”、“看出人外”和“看人之间”三部分。“看入人里”聚焦于探讨与自己有关的沟通因素，简要介绍人际关系的本质，强调自我在沟通中的角色，并分析知觉与情绪在沟通中的重要性；“看出人外”聚焦于探讨与沟通对象有关的因素，分析语言和非口语的特性，强调倾听的重要性；“看人之间”聚焦于讨论关系动力，强调关系的重要性与关系中的亲密和距离，如何增进沟通气氛及人际冲突的形态与因应之道。本书作者投注了多年的专业经验，将本书打造成为有关人际关系理论与实践的最佳读本。本书兼具深入性、广泛性与完整性，特别针对性别与文化观点进行了贯穿全书的探讨。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zkl_min.png" alt="自控力．实操篇 - 凯利·麦格尼格尔"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gxq_min.png" alt="《滚雪球：巴菲特和他的财富人生（畅销版）（共两册）-艾丽斯·施罗德》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-zkl/">《自控力》 凯利·麦格尼格尔</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-gxq/">《滚雪球：巴菲特和他的财富人生》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/qmgx_min.png" alt="亲密关系"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/sycs_min.png" alt="《一本书读懂商业常识_董智轩》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-sycs/">《一本书读懂商业常识_董智轩》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-hfsxy/">《哈佛商学院管理全书》</a><br>下一篇：- <a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》</a><br>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-cdswldq" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-03-10T06:11:27.000Z"><a href="/read/ebooks-cdswldq/">2020-03-10</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-cdswldq/">《拆掉思维里的墙》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/cdswldq.jpg" alt="拆掉思维里的墙"></p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file296/21704187-427714717" target="view_window">《卓有成效的管理者》 </a>- <a href="https://545c.com/file296/21704187-427714717" target="view_window">点击下载</a></li></ul><h1 id="正版购买"><a href="#正版购买" class="headerlink" title="正版购买"></a><strong>正版购买</strong></h1><ul><li><a href="https://u.jd.com/JvlPSg" target="view_window">京东购买 ￥18.60 </a>- <a href="https://u.jd.com/JvlPSg" target="view_window">点击下载</a></li><li><a href="http://u.xc1.ink/AA2C4Q" target="view_window">淘宝购买 ￥21.50 </a>- <a href="http://u.xc1.ink/AA2C4Q" target="view_window">点击下载</a></li><li><a href="https://p.pinduoduo.com/iDvQWx1j" target="view_window">拼多多购买 ￥8.80 </a>- <a href="https://p.pinduoduo.com/iDvQWx1j" target="view_window">点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><hr><h1 id="作品简介"><a href="#作品简介" class="headerlink" title="作品简介"></a><strong>作品简介</strong></h1><p>新一代人生设计大师古典倾情打造“人生开窍手册”：心理学+职业规划+人生哲学。<br>你是否缺少安全感？你会经常觉得累吗？结婚一定要买房吗？坚持一定会成功吗？努力一定有结果？只有有钱才能够幸福？老板可以掌控吗？小人物就无法对抗不公平吗？你对如今的生活不满意，但却因为父母、老婆或者孩子不得不这样过下去吗？每天都在混日子，却幻想有一天找到自己真正喜欢的事业就一定会全心投入？……如果有一个回答是“YES”，那么这本书就是你想要的。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zkl_min.png" alt="自控力．实操篇 - 凯利·麦格尼格尔"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gxq_min.png" alt="《滚雪球：巴菲特和他的财富人生（畅销版）（共两册）-艾丽斯·施罗德》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-zkl/">《自控力》 凯利·麦格尼格尔</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-gxq/">《滚雪球：巴菲特和他的财富人生》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/xdjjx_min.png" alt="小岛经济学"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/sycs_min.png" alt="《一本书读懂商业常识_董智轩》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-xdjjx/">《小岛经济学》</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-sycs/">《一本书读懂商业常识_董智轩》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-dls/">《断离舍》</a><br>下一篇：- <a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》</a><br>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-dls" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-03-10T05:57:27.000Z"><a href="/read/ebooks-dls/">2020-03-10</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-dls/">《断离舍》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/dls.jpg" alt="断离舍"></p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file296/21704187-427714678" target="view_window">《断离舍》 </a>- <a href="https://545c.com/file296/21704187-427714678" target="view_window">点击下载</a></li></ul><h1 id="正版购买"><a href="#正版购买" class="headerlink" title="正版购买"></a><strong>正版购买</strong></h1><ul><li><a href="https://u.jd.com/KiNUJP" target="view_window">京东购买 ￥25.00 </a>- <a href="https://u.jd.com/KiNUJP" target="view_window">点击下载</a></li><li><a href="http://u.xc8.ink/AA2CM3" target="view_window">淘宝购买 ￥22.50 </a>- <a href="http://u.xc8.ink/AA2CM3" target="view_window">点击下载</a></li><li><a href="https://p.pinduoduo.com/mINQaIlT" target="view_window">拼多多购买 ￥16.80 </a>- <a href="https://p.pinduoduo.com/mINQaIlT" target="view_window">点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><hr><h1 id="作品简介"><a href="#作品简介" class="headerlink" title="作品简介"></a><strong>作品简介</strong></h1><p>该书叙述了“断舍离”的含义，让读者了解并做到“断离舍”。该书还记录了如何做到断舍离的具体过程——从断离舍的思考模式到领悟断离舍的思想到真正进行断舍离。</p><p>山下英子出生在东京，家中成员有父母和一个大她6岁的姐姐。因为时代所限制，山下英子从两位主妇的身上看到了时代的影响。一个是她的母亲另一个是她婆婆。妈妈非常讨厌主妇的工作，婆婆不一样，她非常执着于主妇的工作，但除此之外她并没有找到其他任何的人生意义。”山下英子对于这两种生活状态的不认同，给了山下英子写下《断舍离》的契机。 [3] 山下英子在22岁的时候就开始学瑜珈的道场，被传授了“断舍离”的理念。但在十年之后才开始意识到“断舍离”确实可以运用在现实的生活当中，于是作者在自己的生活中实施“断舍离”，在不断的失败和体验中，总结出来“断舍离”理念，就开始写博客，最后被出版商看到。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zkl_min.png" alt="自控力．实操篇 - 凯利·麦格尼格尔"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gxq_min.png" alt="《滚雪球：巴菲特和他的财富人生（畅销版）（共两册）-艾丽斯·施罗德》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-zkl/">《自控力》 凯利·麦格尼格尔</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-gxq/">《滚雪球：巴菲特和他的财富人生》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/qmgx_min.png" alt="亲密关系"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/sycs_min.png" alt="《一本书读懂商业常识_董智轩》"></p><p><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-qmgx/">《亲密关系》</a>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-sycs/">《一本书读懂商业常识_董智轩》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-hfsxy/">《哈佛商学院管理全书》</a><br>下一篇：- <a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》</a><br>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-cwzyzl" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-02-25T08:49:06.000Z"><a href="/read/ebooks-cwzyzl/">2020-02-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-cwzyzl/">《财务自由之路》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/cwzyzl.png" alt="财务自由之路"></p><p>极简理财术，7年内实现财务自由！</p><p><strong>下载链接</strong></p><ul><li><a href="https://545c.com/file/21704187-424220776" target="view_window">《财务自由之路》</a> -<a href="https://545c.com/file/21704187-424220776" target="view_window"> 点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a><strong>内容介绍</strong></h1><p>《小狗钱钱》讲述的是一个童话故事：有一天，吉娅发现一只受伤的猎狗，并把它带回了家。可是，有谁会想到，这只普通的四脚动物却是一个真正的理财天才呢？吉娅和小狗成为了朋友，并从它那里得知，原来所有的愿望都是可以实现的。</p><p>从这个童话故事里可以了解一些金钱的秘密和真相，以及投资、理财的办法。这个故事描述了在实施这些方法的过程中可能遇到的挑战，并且说明了一些令人难以置信的结果。。</p><h1 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a><strong>作者介绍</strong></h1><p>博多·舍费尔（Bodo Schafer）是位畅销书作家。他萃取个人经历中的精华，总结出一套投资理念。借着这套理念的帮助，欧洲成千上万的人在个人财务问题上取得了巨大的进步。</p><p>在获得财务自由之前博多也曾经历过长期的奋斗。他16岁时远赴美国求学，高中毕业后进入大学主修法律，之后在不同公司中担任各种重要职务。26岁时博多陷入严重的个人财务危机，然而凭借坚强的意志和正确的投资理念，他最终摆脱了债务，获得了成功。</p><p>博多决心把他的理财知识传播给更多的人。他的著作被翻译成十几种语言在全球各地广为传播，创下了110周稳居德国图书排行榜首位的记录。他每年在欧洲各地的巡回演讲吸引数十万人热情参与。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zycx_min.png" alt="卓有成效的管理者"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/tzlc_min.png" alt="《投资理财从入门到精通》"><br><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》</a> &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gf_min.png" alt="稻盛和夫_干法"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/jyxyzx_min.png" alt="《稻盛和夫-经营为什么需要哲学》"><br><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; <a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a>&nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-jyxyzx/">《稻盛和夫-经营为什么需要哲学》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gxddybs/">《如何高效读懂一本书》</a></p><p>下一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a></p><p>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ebooks-xgqq" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-02-25T08:45:06.000Z"><a href="/read/ebooks-xgqq/">2020-02-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/read/ebooks-xgqq/">《小狗钱钱》</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/xgqq.png" alt="小狗钱钱"></p><p>本书讲述的是一个童话故事，从这个童话故事里可以了解一些金钱的秘密和真相，以及投资、理财的办法。</p><h1 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a><strong>下载链接</strong></h1><ul><li><a href="https://545c.com/file/21704187-424220777" target="view_window">《小狗钱钱》</a> -<a href="https://545c.com/file/21704187-424220777" target="view_window"> 点击下载</a></li></ul><p>该书籍由网友制作上传“免费PDF电子书”整理发布，版权归原作者所有，仅供学习交流之用，请在下载后24小时内自行删除！<br>注:请尊重原作者和出版社的版权,如果确实喜欢,请购买正版, 多向您的朋友介绍这里,谢谢支持。所有书籍均是完全版,请放心下载.如出现残缺版本,欢迎留言指正.</p><h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a><strong>内容介绍</strong></h1><p>《小狗钱钱》讲述的是一个童话故事：有一天，吉娅发现一只受伤的猎狗，并把它带回了家。可是，有谁会想到，这只普通的四脚动物却是一个真正的理财天才呢？吉娅和小狗成为了朋友，并从它那里得知，原来所有的愿望都是可以实现的。</p><p>从这个童话故事里可以了解一些金钱的秘密和真相，以及投资、理财的办法。这个故事描述了在实施这些方法的过程中可能遇到的挑战，并且说明了一些令人难以置信的结果。</p><h1 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a><strong>作者介绍</strong></h1><p>博多·舍费尔（Bodo Schafer）是位畅销书作家。他萃取个人经历中的精华，总结出一套投资理念。借着这套理念的帮助，欧洲成千上万的人在个人财务问题上取得了巨大的进步。</p><p>在获得财务自由之前博多也曾经历过长期的奋斗。他16岁时远赴美国求学，高中毕业后进入大学主修法律，之后在不同公司中担任各种重要职务。26岁时博多陷入严重的个人财务危机，然而凭借坚强的意志和正确的投资理念，他最终摆脱了债务，获得了成功。</p><p>博多决心把他的理财知识传播给更多的人。他的著作被翻译成十几种语言在全球各地广为传播，创下了110周稳居德国图书排行榜首位的记录。他每年在欧洲各地的巡回演讲吸引数十万人热情参与。</p><p><strong>喜欢这篇文章的人还喜欢：</strong></p><p><img src="http://wangpengcufe.com/zycx_min.png" alt="卓有成效的管理者"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="http://wangpengcufe.com/yz_min.png" alt="《原则》 瑞·达利欧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/tzlc_min.png" alt="《投资理财从入门到精通》"><br><a href="http://www.wangpengcufe.com/read/ebooks-zycx/">《卓有成效的管理者》</a> &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-yz/">《原则》 瑞·达利欧</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-tzlc/">《投资理财从入门到精通》</a></p><p><img src="http://wangpengcufe.com/knj_min.png" alt="卡耐基沟通的艺术与处世智慧"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/gf_min.png" alt="稻盛和夫_干法"> &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; <img src="http://wangpengcufe.com/jyxyzx_min.png" alt="《稻盛和夫-经营为什么需要哲学》"><br><a href="http://www.wangpengcufe.com/read/ebooks-knj/">《卡耐基沟通的艺术与处世智慧》</a> &nbsp; <a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a>&nbsp; &nbsp;&nbsp;<a href="http://www.wangpengcufe.com/read/ebooks-jyxyzx/">《稻盛和夫-经营为什么需要哲学》</a></p><p>上一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gxddybs/">《如何高效读懂一本书》</a></p><p>下一篇： - <a href="http://www.wangpengcufe.com/read/ebooks-gf/">《稻盛和夫_干法》</a></p><p>返回： - <a href="http://www.wangpengcufe.com/read/ebooks/">经管首页</a></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml4" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-02-17T16:00:16.000Z"><a href="/machinelearning/pythonml-pythonml4/">2020-02-18</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml4/">python机器学习（四）分类算法-决策树</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="https://upload-images.jianshu.io/upload_images/7289495-9ca2829e21b456a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="决策树"></p><h1 id="一、决策树的原理"><a href="#一、决策树的原理" class="headerlink" title="一、决策树的原理"></a>一、决策树的原理</h1><p>决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法 。</p><h1 id="二、决策树的现实案例"><a href="#二、决策树的现实案例" class="headerlink" title="二、决策树的现实案例"></a>二、决策树的现实案例</h1><h3 id="相亲"><a href="#相亲" class="headerlink" title="相亲"></a><strong>相亲</strong></h3><p><img src="https://upload-images.jianshu.io/upload_images/7289495-2aa0dc94a9ff3a05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="相亲决策树"></p><p>女儿：多大年纪了？<br>母亲：26。<br>女儿：长的帅不帅？<br>母亲：挺帅的。<br>女儿：收入高不？<br>母亲：不算很高，中等情况。<br>女儿：是公务员不？<br>母亲：是，在税务局上班呢。<br>女儿：那好，我去见见。</p><h3 id="银行是否发放贷款"><a href="#银行是否发放贷款" class="headerlink" title="银行是否发放贷款"></a><strong>银行是否发放贷款</strong></h3><p>行长：是否有自己的房子？<br>职员：有。<br>行长：可以考虑放贷。<br>职员：如果没有自己的房子呢？<br>行长：是否有稳定工作？<br>职员：有。<br>行长：可以考虑放贷。<br>职员：那如果没有呢？<br>行长：既没有自己的房子，也没有稳定工作，那咱还放啥贷款？<br>职员：懂了。<br><img src="https://upload-images.jianshu.io/upload_images/7289495-8ac7388572cc5c9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="贷款决策树"></p><h3 id="预测足球队是否夺冠"><a href="#预测足球队是否夺冠" class="headerlink" title="预测足球队是否夺冠"></a><strong>预测足球队是否夺冠</strong></h3><p><img src="https://upload-images.jianshu.io/upload_images/7289495-df1be6afa40a2762.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="预测决策树"></p><h1 id="三、信息论基础"><a href="#三、信息论基础" class="headerlink" title="三、信息论基础"></a>三、信息论基础</h1><h3 id="信息熵："><a href="#信息熵：" class="headerlink" title="信息熵："></a><strong>信息熵：</strong></h3><p>假如我们竞猜32只足球队谁是冠军？我可以把球编上号，从1到32，然后提问：冠 军在1-16号吗？依次进行二分法询问，只需要五次，就可以知道结果。<br>32支球队，问询了5次，信息量定义为5比特，log32=5比特。比特就是表示信息的单位。<br>假如有64支球队的话，那么我们需要二分法问询6次，信息量就是6比特，log64=6比特。<br>问询了多少次，专业术语称之为信息熵，单位为比特。<br>公式为：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-1ca326c87c1f5fd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="信息熵"></p><p>信息熵的作用：<br>决策树生成的过程中，信息熵大的作为根节点，信息熵小的作为叶子节点，按照信息熵的从大到小原则，生成决策树。</p><h3 id="条件熵："><a href="#条件熵：" class="headerlink" title="条件熵："></a><strong>条件熵：</strong></h3><p>条件熵H（D|A）表示在已知随机变量A的条件下随机变量D的不确定性。<br>公式为：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-cce09a9275128c78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="条件熵"></p><p>通俗来讲就是，知道A情况下，D的信息量。</p><h3 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a><strong>信息增益：</strong></h3><p>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。<br>公式为：<br><img src="https://upload-images.jianshu.io/upload_images/7289495-356c6e1db31cb5f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="信息增益"></p><p>怎么理解信息增益呢？信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。简单讲，就是知道的增多，使得不知道的（不确定的）就减少。</p><h1 id="四、-决策树API"><a href="#四、-决策树API" class="headerlink" title="四、 决策树API"></a>四、 决策树API</h1><p>sklearn.tree.DecisionTreeClassifier</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)</span><br><span class="line">决策树分类器</span><br><span class="line">criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’</span><br><span class="line">max_depth:树的深度大小</span><br><span class="line">random_state:随机数种子</span><br><span class="line"></span><br><span class="line">method:</span><br><span class="line">dec.fit(X,y): 根据数据集(X,y)建立决策树分类器</span><br><span class="line">dec.apply(X): 返回每个样本被预测为的叶子的索引。</span><br><span class="line">dec.cost_complexity_pruning_path(X,y): 在最小成本复杂性修剪期间计算修剪路径。</span><br><span class="line">dec.decision_path(X): 返回树中的决策路径</span><br><span class="line">dec.get_depth(): 返回树的深度</span><br><span class="line">dec.get_n_leaves(): 返回决策树的叶子节点</span><br><span class="line">dec.get_params(): 返回评估器的参数</span><br><span class="line">dec.predict(X): 预测X的类或回归值</span><br><span class="line">dec.predict_log_proba(X): 预测X的类的log值</span><br><span class="line">dec.predict_proba(X): 预测X分类的概率值</span><br><span class="line">dec.score(X,y): 测试数据X和标签值y之间的平均准确率</span><br><span class="line">dec.set_params(min_samples_split=3): 设置评估器的参数</span><br><span class="line">X 表示训练集，y表示特征值</span><br></pre></td></tr></table></figure><h1 id="五、-决策树的生成与本地保存"><a href="#五、-决策树的生成与本地保存" class="headerlink" title="五、 决策树的生成与本地保存"></a>五、 决策树的生成与本地保存</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">li = load_iris()</span><br><span class="line">dec = DecisionTreeClassifier()</span><br><span class="line"># 根据训练集(X,y)建立决策树分类器</span><br><span class="line">dec.fit(li.data,li.target)</span><br><span class="line"># 预测X的类或回归值</span><br><span class="line">dec.predict(li.data)</span><br><span class="line"># 测试数据X和标签值y之间的平均准确率</span><br><span class="line">dec.score(li.data,li.target)</span><br><span class="line"># 保存树文件 tree.dot</span><br><span class="line">tree.export_graphviz(dec,out_file=&apos;tree.dot&apos;)</span><br></pre></td></tr></table></figure><p>tree.dot 保存结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">digraph Tree &#123;</span><br><span class="line">node [shape=box] ;</span><br><span class="line">0 [label=&quot;X[2] &lt;= 2.45\ngini = 0.667\nsamples = 150\nvalue = [50, 50, 50]&quot;] ;</span><br><span class="line">1 [label=&quot;gini = 0.0\nsamples = 50\nvalue = [50, 0, 0]&quot;] ;</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><h1 id="六、-决策树的优缺点"><a href="#六、-决策树的优缺点" class="headerlink" title="六、 决策树的优缺点"></a>六、 决策树的优缺点</h1><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a><strong>优点</strong></h3><ul><li>简单的理解和解释，树木可视化。</li><li>需要很少的数据准备，其他技术通常需要数据归一化。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a><strong>缺点</strong></h3><ul><li>决策树学习者可以创建不能很好地推广数据的过于复杂的树，被称为过拟合。</li><li>决策树可能不稳定，因为数据的小变化可能会导致完全不同的树<br>被生成。</li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a><strong>改进</strong></h3><ul><li>减枝cart算法</li><li>随机森林</li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml3" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-02-04T16:22:16.000Z"><a href="/machinelearning/pythonml-pythonml3/">2020-02-05</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml3/">python机器学习（三）分类算法-朴素贝叶斯</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/pythonml3.1.png" alt="朴素贝叶斯"></p><h1 id="一、概率基础"><a href="#一、概率基础" class="headerlink" title="一、概率基础"></a><strong>一、概率基础</strong></h1><p><strong>概率定义：</strong><br>概率定义为一件事情发生的可能性，例如，随机抛硬币，正面朝上的概率。</p><p><strong>联合概率：</strong><br>包含多个条件，且所有条件同时成立的概率，记作：𝑃(𝐴,𝐵) 。</p><p><strong>条件概率：</strong><br>事件A在另外一个事件B已经发生条件下的发生概率，记作：𝑃(𝐴|𝐵) 。P(A1,A2|B) = P(A1|B)P(A2|B)，需要注意的是：此条件概率的成立，是由于A1,A2相互独立的结果。</p><h1 id="二、朴素贝叶斯介绍"><a href="#二、朴素贝叶斯介绍" class="headerlink" title="二、朴素贝叶斯介绍"></a><strong>二、朴素贝叶斯介绍</strong></h1><p><strong>公式：</strong><br><img src="http://wangpengcufe.com/pythonml3.2.png" alt="朴素贝叶斯公式"><br>其中，w为给定文档的特征值(频数统计,预测文档提供)，c为文档类别。<br>公式可以理解为：</p><p><img src="http://wangpengcufe.com/pythonml3.3.png" alt="朴素贝叶斯公式的理解"><br>其中c可以是不同类别。</p><p>公式分为三个部分：</p><p><strong>𝑃(𝐶)</strong>：每个文档类别的概率(某文档类别词数／总文档词数)<br><strong>𝑃(𝑊│𝐶)</strong>：给定类别下特征（被预测文档中出现的词）的概率<br>计算方法：𝑃(𝐹1│𝐶)=𝑁𝑖/𝑁 （训练文档中去计算）<br>𝑁𝑖为该𝐹1词在C类别所有文档中出现的次数<br>N为所属类别C下的文档所有词出现的次数和<br><strong>𝑃(𝐹1,𝐹2,…)</strong>: 预测文档中每个词的概率</p><p><strong>举个栗子：</strong></p><p>现有一篇被预测文档：出现了都江宴，武汉，武松，计算属于历史，地理的类别概率？<br><img src="http://wangpengcufe.com/pythonml3.4.png" alt="image"><br>历史：𝑃(都江宴，武汉，武松│历史)∗P(历史)=（10/108）∗（22/108）∗（65/108）∗(108/235) =0.00563435<br>地理：𝑃(都江宴，武汉，武松│地理)∗P(地理)=（58/127）∗（17/127）∗（0/127）∗(127/235)=0</p><p><strong>拉普拉斯平滑：</strong><br>思考：属于某个类别为0，合适吗？<br>从上面的例子我们得到地理概率为0，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零。<br>解决方法：拉普拉斯平滑系数。<br><img src="http://wangpengcufe.com/pythonml3.5.png" alt="image"><br>𝛼为指定的系数一般为1，m为训练文档中统计出的特征词个数</p><p><strong>sklearn朴素贝叶斯实现API：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</span><br><span class="line">alpha：拉普拉斯平滑系数</span><br></pre></td></tr></table></figure><p><strong>案例：新闻分类</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=&apos;all&apos;)</span><br><span class="line"># 进行数据分割</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)</span><br><span class="line"># 对数据集进行特征抽取</span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line"># 以训练集当中的词的列表进行每篇文章重要性统计[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;]</span><br><span class="line">x_train = tf.fit_transform(x_train)</span><br><span class="line">x_test = tf.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 进行朴素贝叶斯算法的预测</span><br><span class="line">mlt = MultinomialNB(alpha=1.0)</span><br><span class="line">print(x_train)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(0, 120993)	0.0838226531816039</span><br><span class="line">(0, 36277)	0.028728297074726128</span><br><span class="line">(0, 118261)	0.051733692584494416</span><br><span class="line">(0, 118605)	0.08660213360333731</span><br><span class="line">(0, 78914)	0.10725171098177662</span><br><span class="line">(0, 120174)	0.07226288195761017</span><br><span class="line">(0, 146730)	0.03649798864200877</span><br><span class="line">(0, 49960)	0.09535813190987927</span><br><span class="line">(0, 108029)	0.10406938034117505</span><br><span class="line">(0, 151947)	0.1081016719923428</span><br><span class="line">(0, 120110)	0.13513684031456163</span><br><span class="line">(0, 34588)	0.06453595223748614</span><br><span class="line">(0, 133893)	0.04993313285348771</span><br><span class="line">(0, 31218)	0.07845873103784344</span><br><span class="line">(0, 108032)	0.08430822316250115</span><br><span class="line">(0, 30921)	0.11806736198114927</span><br><span class="line">(0, 33267)	0.030864914635712264</span><br><span class="line">(0, 36137)	0.0714722249527062</span><br><span class="line">(0, 57776)	0.07110907374703304</span><br><span class="line">(0, 77937)	0.026514922107534245</span><br><span class="line">(0, 90944)	0.09746338158610199</span><br><span class="line">(0, 135824)	0.09394365947415394</span><br><span class="line">(0, 49956)	0.09183375914922258</span><br><span class="line">(0, 151957)	0.07203295034824395</span><br><span class="line">(0, 33356)	0.07203295034824395</span><br><span class="line">:	:</span><br><span class="line">(14133, 45099)	0.030803124311834594</span><br><span class="line">(14133, 135309)	0.02305588722190138</span><br><span class="line">(14133, 135472)	0.06570104508511963</span><br><span class="line">(14133, 52014)	0.05222321951090842</span><br><span class="line">(14133, 108029)	0.05584161408783517</span><br><span class="line">(14133, 36137)	0.07670122356304401</span><br><span class="line">(14133, 34063)	0.12187079805145053</span><br><span class="line">(14133, 106978)	0.0851182715752145</span><br><span class="line">(14133, 106534)	0.03378056586331488</span><br><span class="line">(14133, 105921)	0.09707364301640503</span><br><span class="line">(14133, 103839)	0.07144955527096918</span><br><span class="line">(14133, 136535)	0.03801377630817533</span><br><span class="line">(14133, 42966)	0.028558472354146207</span><br><span class="line">(14133, 81075)	0.02180715538325887</span><br><span class="line">(14133, 135641)	0.025875408277197205</span><br><span class="line">(14133, 148185)	0.028450089379106706</span><br><span class="line">(14133, 78894)	0.020030955308174968</span><br><span class="line">(14133, 147914)	0.047259202253661425</span><br><span class="line">(14133, 90152)	0.017166154294786778</span><br><span class="line">(14133, 45598)	0.05645818387150284</span><br><span class="line">(14133, 135325)	0.03667700550640032</span><br><span class="line">(14133, 118218)	0.02343357701502816</span><br><span class="line">(14133, 131632)	0.01710795977554328</span><br><span class="line">(14133, 59957)	0.0485327006460036</span><br><span class="line">(14133, 67480)	0.01710795977554328</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mlt.fit(x_train, y_train)   #MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span><br><span class="line">y_predict = mlt.predict(x_test)</span><br><span class="line">print(&quot;预测的文章类别为：&quot;, y_predict)</span><br><span class="line">#预测的文章类别为： [ 3 16  5 ...  0  5  8]</span><br><span class="line"># 得出准确率</span><br><span class="line">print(&quot;准确率为：&quot;, mlt.score(x_test, y_test))</span><br><span class="line">#准确率为： 0.8414685908319185</span><br><span class="line">print(&quot;每个类别的精确率和召回率：&quot;, classification_report(y_test, y_predict, target_names=news.target_names))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">每个类别的精确率和召回率：                           precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.89      0.75      0.81       210</span><br><span class="line">           comp.graphics       0.87      0.81      0.84       225</span><br><span class="line"> comp.os.ms-windows.misc       0.77      0.90      0.83       209</span><br><span class="line">comp.sys.ibm.pc.hardware       0.77      0.78      0.78       258</span><br><span class="line">   comp.sys.mac.hardware       0.86      0.88      0.87       223</span><br><span class="line">          comp.windows.x       0.97      0.76      0.85       260</span><br><span class="line">            misc.forsale       0.92      0.68      0.78       233</span><br><span class="line">               rec.autos       0.91      0.89      0.90       263</span><br><span class="line">         rec.motorcycles       0.94      0.96      0.95       260</span><br><span class="line">      rec.sport.baseball       0.93      0.92      0.92       230</span><br><span class="line">        rec.sport.hockey       0.89      0.97      0.93       234</span><br><span class="line">               sci.crypt       0.64      0.99      0.78       235</span><br><span class="line">         sci.electronics       0.94      0.68      0.79       275</span><br><span class="line">                 sci.med       0.96      0.89      0.93       241</span><br><span class="line">               sci.space       0.89      0.97      0.93       246</span><br><span class="line">  soc.religion.christian       0.56      0.99      0.72       257</span><br><span class="line">      talk.politics.guns       0.84      0.94      0.89       256</span><br><span class="line">   talk.politics.mideast       0.92      0.98      0.94       245</span><br><span class="line">      talk.politics.misc       0.98      0.67      0.80       182</span><br><span class="line">      talk.religion.misc       1.00      0.17      0.29       170</span><br><span class="line"></span><br><span class="line">                accuracy                           0.84      4712</span><br><span class="line">               macro avg       0.87      0.83      0.83      4712</span><br><span class="line">            weighted avg       0.87      0.84      0.84      4712</span><br></pre></td></tr></table></figure><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a><strong>三、总结</strong></h1><p><strong>优点：</strong></p><ul><li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li><li>分类准确度高，速度快</li></ul><p><strong>缺点：</strong></p><ul><li>需要知道先验概率P(F1,F2,…|C)，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。</li></ul></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml2" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-21T09:02:06.000Z"><a href="/machinelearning/pythonml-pythonml2/">2020-01-21</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml2/">python 机器学习（二）分类算法-k近邻算法</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/pythonml2.1.png" alt="分类算法-K近邻算法"></p><h1 id="一、什么是K近邻算法？"><a href="#一、什么是K近邻算法？" class="headerlink" title="一、什么是K近邻算法？"></a><strong>一、什么是K近邻算法</strong>？</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义:"></a><strong>定义:</strong></h2><p>如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><h2 id="来源"><a href="#来源" class="headerlink" title="来源:"></a><strong>来源:</strong></h2><p>KNN算法最早是由Cover和Hart提出的一种分类算法.</p><h2 id="计算距离公式"><a href="#计算距离公式" class="headerlink" title="计算距离公式:"></a><strong>计算距离公式:</strong></h2><p>两个样本的距离可以通过如下公式计算，又叫欧式距离。<br>比如说，a(a1,a2,a3),b(b1,b2,b3)<br><img src="http://wangpengcufe.com/pythonml2.2.png" alt="欧式距离"></p><h1 id="二、K近邻算法的实现"><a href="#二、K近邻算法的实现" class="headerlink" title="二、K近邻算法的实现"></a><strong>二、K近邻算法的实现</strong></h1><h2 id="sk-learn近邻算法API"><a href="#sk-learn近邻算法API" class="headerlink" title="sk-learn近邻算法API"></a><strong>sk-learn近邻算法API</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=&apos;auto&apos;)</span><br><span class="line">n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数 </span><br><span class="line">algorithm：&#123;‘auto’，‘ball_tree’，‘kd_tree’，‘brute’&#125;，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)</span><br></pre></td></tr></table></figure><h2 id="近邻算法实例"><a href="#近邻算法实例" class="headerlink" title="近邻算法实例"></a><strong>近邻算法实例</strong></h2><p>案例背景：（kaggle地址：<a href="https://www.kaggle.com/c/facebook-v-predicting-check-ins/overview）" target="_blank" rel="noopener">https://www.kaggle.com/c/facebook-v-predicting-check-ins/overview）</a><br><img src="http://wangpengcufe.com/pythonml2.3.png" alt="预测入住"></p><p>数据下载地址：<a href="https://storage.googleapis.com/kagglesdsdata/competitions/5186/37497/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1579794270&Signature=dUw4i0K1VJJ7UfiSVvXmPiMRhx9v2aPh1eJagNdOQIpdM%2F7CjfhO7K3VLg5Oxd8%2BD%2B9XJqggolwF63CsmOLEPEyBb5BL7g6YRriltjCf1gwJUFx3u2ax6dfjfsyr%2FY4x5hKrrRSpqFPwd4SN9TzUrwcMf7erFxreIpWrO8peG7T%2Fw1EyxNkbH0NGBgYEZ20n0TgYstGGS30fjdoB8mus%2B747tNaWsudQXv5MCr9XRdC8IT95klZ8R5pqNb5tMoIKYaZRwmM5N2clianWQ1knAALW%2Fmpa536gQM%2BbLDzGCUX48rLbwHiZYGE45EEAEGtNdnwM0CUOBojJ2c7UfoeK8g%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv.zip" target="_blank" rel="noopener">train.csv</a></p><p>数据格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">            row_id       x       y  accuracy    time    place_id</span><br><span class="line">0                0  0.7941  9.0809        54  470702  8523065625</span><br><span class="line">1                1  5.9567  4.7968        13  186555  1757726713</span><br><span class="line">2                2  8.3078  7.0407        74  322648  1137537235</span><br><span class="line">3                3  7.3665  2.5165        65  704587  6567393236</span><br><span class="line">4                4  4.0961  1.1307        31  472130  7440663949</span><br><span class="line">...            ...     ...     ...       ...     ...         ...</span><br><span class="line">29118016  29118016  6.5133  1.1435        67  399740  8671361106</span><br><span class="line">29118017  29118017  5.9186  4.4134        67  125480  9077887898</span><br><span class="line">29118018  29118018  2.9993  6.3680        67  737758  2838334300</span><br><span class="line">29118019  29118019  4.0637  8.0061        70  764975  1007355847</span><br><span class="line">29118020  29118020  7.4523  2.0871        17  102842  7028698129</span><br><span class="line"></span><br><span class="line">[29118021 rows x 6 columns]</span><br></pre></td></tr></table></figure><p>实现思路：<br>1、数据集的处理(缩小数据集范围，处理日期数据，增加分割的日期数据，删除没用的日期数据，将签到位置少于n个用户的删除)<br>2、分割数据集<br>3、对数据集进行标准化<br>4、estimator流程进行分类预测</p><p>具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"># 读取数据</span><br><span class="line">data = pd.read_csv(&quot;./data/FBlocation/train.csv&quot;)</span><br><span class="line"># 处理数据</span><br><span class="line"># 1、缩小数据,查询数据集范围</span><br><span class="line">data = data.query(&quot;x &gt; 1.0 &amp;  x &lt; 1.25 &amp; y &gt; 2.5 &amp; y &lt; 2.75&quot;)</span><br><span class="line"># 处理时间的数据</span><br><span class="line">time_value = pd.to_datetime(data[&apos;time&apos;], unit=&apos;s&apos;)</span><br><span class="line"> # 把日期格式转换成 字典格式</span><br><span class="line">time_value = pd.DatetimeIndex(time_value)</span><br><span class="line"># 构造一些特征</span><br><span class="line">data[&apos;day&apos;] = time_value.day</span><br><span class="line">data[&apos;hour&apos;] = time_value.hour</span><br><span class="line">data[&apos;weekday&apos;] = time_value.weekday</span><br><span class="line"># 把时间戳特征删除</span><br><span class="line">data = data.drop([&apos;time&apos;], axis=1)</span><br><span class="line"># 把签到数量少于n个目标位置删除</span><br><span class="line">place_count = data.groupby(&apos;place_id&apos;).count()</span><br><span class="line">tf = place_count[place_count.row_id &gt; 3].reset_index()</span><br><span class="line">data = data[data[&apos;place_id&apos;].isin(tf.place_id)]</span><br><span class="line"></span><br><span class="line"># 取出数据当中的特征值和目标值</span><br><span class="line">y = data[&apos;place_id&apos;]</span><br><span class="line">x = data.drop([&apos;place_id&apos;], axis=1)</span><br><span class="line"># 进行数据的分割训练集合测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)</span><br><span class="line"># 特征工程（标准化）</span><br><span class="line">std = StandardScaler()</span><br><span class="line"># 对测试集和训练集的特征值进行标准化</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test)</span><br><span class="line"># 进行算法流程 # 超参数</span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line">knn.fit(x_train, y_train)</span><br><span class="line">y_predict = knn.predict(x_test)</span><br><span class="line">print(&quot;预测的目标签到位置为：&quot;, y_predict)</span><br><span class="line">print(&quot;预测的准确率:&quot;, knn.score(x_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">预测的目标签到位置为： [8258328058 2355236719 6683426742 ... 5606572086 4932578245 9237487147]</span><br><span class="line">预测的准确率: 0.3959810874704492</span><br></pre></td></tr></table></figure><h2 id="思考问题"><a href="#思考问题" class="headerlink" title="思考问题"></a><strong>思考问题</strong></h2><p>1、k值取多大？有什么影响？<br>2、性能问题？</p><h1 id="三、K近邻算法总结"><a href="#三、K近邻算法总结" class="headerlink" title="三、K近邻算法总结"></a><strong>三、K近邻算法总结</strong></h1><h2 id="K近邻算法优缺点"><a href="#K近邻算法优缺点" class="headerlink" title="K近邻算法优缺点"></a><strong>K近邻算法优缺点</strong></h2><p><strong>优点</strong>：<br>简单，易于理解，易于实现，无需估计参数，无需训练</p><p><strong>缺点</strong></p><ul><li>懒惰算法，对测试样本分类时的计算量大，内存开销大</li><li>必须指定K值，K值选择不当则分类精度不能保证</li></ul><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h2><p>小数据场景，几千～几万样本，具体场景具体业务去测试</p><h1 id="四、分类模型的评估"><a href="#四、分类模型的评估" class="headerlink" title="四、分类模型的评估"></a><strong>四、分类模型的评估</strong></h1><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a><strong>评估方法</strong></h2><p>estimator.score()<br>一般最常见使用的是准确率，即预测结果正确的百分比</p><p><strong>混淆矩阵</strong><br>在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)<br><img src="http://wangpengcufe.com/pythonml2.4.png" alt="混淆矩阵"><br><strong>精确率（Precision）</strong>：预测结果为正例样本中真实为正例的比例（查得准）<br><img src="http://wangpengcufe.com/pythonml2.5.png" alt="精确率（Precision）"><br><strong>召回率(Recall)</strong>：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）<br><img src="http://wangpengcufe.com/pythonml2.6.png" alt="召回率(Recall)"><br>其他分类标准，<strong>F1-score</strong>，反映了模型的稳健型<br><img src="http://wangpengcufe.com/pythonml2.7.png" alt="F1-score"></p><h2 id="分类模型评估API"><a href="#分类模型评估API" class="headerlink" title="分类模型评估API"></a><strong>分类模型评估API</strong></h2><p>评估API ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report</span><br></pre></td></tr></table></figure><p>用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report(y_true, y_pred, target_names=None)</span><br><span class="line"></span><br><span class="line">y_true：真实目标值</span><br><span class="line"></span><br><span class="line">y_pred：估计器预测目标值</span><br><span class="line"></span><br><span class="line">target_names：目标类别名称</span><br><span class="line"></span><br><span class="line">return：每个类别精确率与召回率</span><br></pre></td></tr></table></figure></div><footer><div class="clearfix"></div></footer></div></article><article id="post-pythonml-pythonml1" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-19T02:06:06.000Z"><a href="/machinelearning/pythonml-pythonml1/">2020-01-19</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/pythonml-pythonml1/">python 机器学习（一）机器学习概述与特征工程</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80.png" alt="机器学习概述与特征工程"></p><h1 id="一、机器学习概述"><a href="#一、机器学习概述" class="headerlink" title="一、机器学习概述"></a>一、机器学习概述</h1><h5 id="1-1、什么是机器学习？"><a href="#1-1、什么是机器学习？" class="headerlink" title="1.1、什么是机器学习？"></a><strong>1.1、什么是机器学习</strong>？</h5><p>机器学习是从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测</p><h5 id="1-2、为什么需要机器学习？"><a href="#1-2、为什么需要机器学习？" class="headerlink" title="1.2、为什么需要机器学习？"></a><strong>1.2、为什么需要机器学习</strong>？</h5><ul><li>解放生产力，智能客服，可以不知疲倦的24小时作业</li><li>解决专业问题，ET医疗，帮助看病</li><li>提供社会便利，例如杭州的城市大脑<h5 id="1-3、机器学习应用场景"><a href="#1-3、机器学习应用场景" class="headerlink" title="1.3、机器学习应用场景"></a><strong>1.3、机器学习应用场景</strong></h5></li><li>自然语言处理</li><li>无人驾驶</li><li>计算机视觉</li><li>推荐系统<h1 id="二、数据来源与类型"><a href="#二、数据来源与类型" class="headerlink" title="二、数据来源与类型"></a>二、数据来源与类型</h1><h5 id="2-1、数据的来源"><a href="#2-1、数据的来源" class="headerlink" title="2.1、数据的来源"></a><strong>2.1、数据的来源</strong></h5></li><li>企业日益积累的大量数据（互联网公司更为显著）</li><li>政府掌握的各种数据</li><li>科研机构的实验数据<h5 id="2-2、数据的类型"><a href="#2-2、数据的类型" class="headerlink" title="2.2、数据的类型"></a><strong>2.2、数据的类型</strong></h5>数据的类型将是机器学习模型不同问题不同处理的依据。数据的类型包括：</li></ul><p><strong>离散型数据</strong>：由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。</p><p><strong>连续型数据</strong>：变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。</p><p><strong>注意</strong>：</p><ul><li>只要记住一点，离散型是区间内不可分，连续型是区间内可分</li></ul><h5 id="2-3、可用的数据集"><a href="#2-3、可用的数据集" class="headerlink" title="2.3、可用的数据集"></a><strong>2.3、可用的数据集</strong></h5><p><strong>scikit-learn</strong>：数据量较小 ，方便学习。<br><strong>UCI</strong>：收录了360个数据集，覆盖科学、生活、经济等领域 ，数据量几十万。<br><strong>Kaggle</strong>：大数据竞赛平台，80万科学家，真实数据，数据量巨大。</p><p>常用数据集数据的结构组成：特征值+目标值，如下图：<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.1.png" alt="常用数据集结构"></p><h1 id="三、数据的特征工程"><a href="#三、数据的特征工程" class="headerlink" title="三、数据的特征工程"></a>三、数据的特征工程</h1><h5 id="3-1、特征工程是什么？"><a href="#3-1、特征工程是什么？" class="headerlink" title="3.1、特征工程是什么？"></a><strong>3.1、特征工程是什么</strong>？</h5><p>特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。</p><h5 id="3-2、特征工程的意义"><a href="#3-2、特征工程的意义" class="headerlink" title="3.2、特征工程的意义"></a><strong>3.2、特征工程的意义</strong></h5><p><strong>意义</strong>：直接影响模型的预测结果。</p><h5 id="3-3、scikit-learn"><a href="#3-3、scikit-learn" class="headerlink" title="3.3、scikit-learn"></a><strong>3.3、scikit-learn</strong></h5><ul><li>Python语言的机器学习工具</li><li>Scikit-learn包括许多知名的机器学习算法的实现</li><li>Scikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎。</li><li>目前稳定版本0.18</li><li>安装：pip3 install Scikit-learn</li><li>引用：import sklearn</li></ul><h5 id="3-4、数据的特征抽取"><a href="#3-4、数据的特征抽取" class="headerlink" title="3.4、数据的特征抽取"></a><strong>3.4、数据的特征抽取</strong></h5><h6 id="3-4-1、特点："><a href="#3-4-1、特点：" class="headerlink" title="3.4.1、特点："></a><strong>3.4.1、特点</strong>：</h6><ul><li>特征抽取针对非连续型数据</li><li>特征抽取对文本等进行特征值化</li></ul><h6 id="3-4-2、sklearn特征抽取API"><a href="#3-4-2、sklearn特征抽取API" class="headerlink" title="3.4.2、sklearn特征抽取API :"></a><strong>3.4.2、sklearn特征抽取API</strong> :</h6><p>sklearn.feature_extraction</p><h6 id="3-4-3、字典特征抽取"><a href="#3-4-3、字典特征抽取" class="headerlink" title="3.4.3、字典特征抽取 :"></a><strong>3.4.3、字典特征抽取</strong> :</h6><p><strong>作用</strong>：对字典数据进行特征值化<br><strong>类</strong>：sklearn.feature_extraction.DictVectorizer<br><strong>DictVectorizer语法</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DictVectorizer(sparse=True,…)</span><br><span class="line"></span><br><span class="line">DictVectorizer.fit_transform(X)       </span><br><span class="line">    X:字典或者包含字典的迭代器</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">DictVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">DictVectorizer.get_feature_names()</span><br><span class="line">    返回类别名称</span><br><span class="line"></span><br><span class="line">DictVectorizer.transform(X)</span><br><span class="line">    按照原先的标准转换</span><br></pre></td></tr></table></figure><p><strong>流程</strong>：<br>1、实例化类DictVectorizer<br>2、调用fit_transform方法输入数据并转换</p><p><strong>举一个栗子</strong>：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction import DictVectorizer</span><br><span class="line">dict = DictVectorizer(sparse=False)</span><br><span class="line">data = dict.fit_transform([&#123;&apos;name&apos;: &apos;张飞&apos;,&apos;score&apos;: 70&#125;, &#123;&apos;name&apos;: &apos;赵云&apos;,&apos;score&apos;:100&#125;, &#123;&apos;name&apos;: &apos;刘备&apos;,&apos;score&apos;: 98&#125;])</span><br><span class="line">print(dict.get_feature_names())</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p></p><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[&apos;name=刘备&apos;, &apos;name=张飞&apos;, &apos;name=赵云&apos;, &apos;score&apos;]</span><br><span class="line">[[  0.   1.   0.  70.]</span><br><span class="line"> [  0.   0.   1. 100.]</span><br><span class="line"> [  1.   0.   0.  98.]]</span><br></pre></td></tr></table></figure><p>从中，我们可以看出：对于字典 [{‘name’: ‘张飞’,’score’: 70}, {‘name’: ‘赵云’,’score’:100}, {‘name’: ‘刘备’,’score’: 98}] ，DictVectorizer类将汉字（张飞，赵云，刘备）转成了one-hot编码（0,1,0），而数值类型的数据（70,100,98）是不做处理的。</p><p>什么是one-hot编码？<br>One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。</p><h6 id="3-4-4、文本特征抽取"><a href="#3-4-4、文本特征抽取" class="headerlink" title="3.4.4、文本特征抽取"></a><strong>3.4.4、文本特征抽取</strong></h6><p><strong>作用</strong>：对文本数据进行特征值化<br><strong>类</strong>：sklearn.feature_extraction.text.CountVectorizer<br><strong>CountVectorizer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CountVectorizer(max_df=1.0,min_df=1,…)</span><br><span class="line">    返回词频矩阵</span><br><span class="line"></span><br><span class="line">CountVectorizer.fit_transform(X,y)       </span><br><span class="line">    X:文本或者包含文本字符串的可迭代对象</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">CountVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">CountVectorizer.get_feature_names()</span><br><span class="line">    返回值:单词列表</span><br></pre></td></tr></table></figure><p><strong>流程</strong>:<br>1、实例化类CountVectorizer<br>2、调用fit_transform方法输入数据并转换</p><p><strong>举一个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;我爱学习，学习使我进步&quot;, &quot;我爱work，work 使我快乐&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;work&apos;, &apos;使我快乐&apos;, &apos;学习使我进步&apos;, &apos;我爱work&apos;, &apos;我爱学习&apos;]</span><br><span class="line">[[0 0 1 0 1]</span><br><span class="line"> [1 1 0 1 0]]</span><br></pre></td></tr></table></figure><p>可以看到，API中的CountVectorizer类将中文转换成了单个词语，并给每个词语的出现个数进行了统计。有一点要注意的是，程序并不会给中文分词，所以，例子中，‘学习使我进步’程序认为是一个词语，这种情况下，可以用空格，或者逗号，将中文进行分割。还有一点要注意的是，<strong>如果是英文的话，是不会统计单个字母的，因为字母的统计是没有意义的，同理，CountVectorizer也不支持单个中文字</strong>。</p><p>我们可以验证一下栗子：<br><strong>英文栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;I like study , study makes me happy&quot;, &quot;I am a good student&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;am&apos;, &apos;good&apos;, &apos;happy&apos;, &apos;like&apos;, &apos;makes&apos;, &apos;me&apos;, &apos;student&apos;, &apos;study&apos;]</span><br><span class="line">[[0 0 1 1 1 1 0 2]</span><br><span class="line"> [1 1 0 0 0 0 1 0]]</span><br></pre></td></tr></table></figure><p><strong>中文栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([&quot;我 热爱 学习， 学习 使我 进步&quot;, &quot;我 是 一个 好学生&quot;])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&apos;一个&apos;, &apos;使我&apos;, &apos;好学生&apos;, &apos;学习&apos;, &apos;热爱&apos;, &apos;进步&apos;]</span><br><span class="line">[[0 1 0 2 1 1]</span><br><span class="line"> [1 0 1 0 0 0]]</span><br></pre></td></tr></table></figure><p>从中文栗子和英文栗子中，我们可以看到单个英文和单个中文是不会统计数量的，因为统计单个中文或者英文是没有意义的。</p><p>手动加空格，或者加逗号分隔始终是指标不治本，如果给我们一篇文章，让我们去处理的话，那要累到手瘫了。那么，有没有好的办法呢？是有的，那就是用 python 里提供的 jieba 分词类库。<br><strong>我们再来举一个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">con1 = jieba.cut(&quot;我热爱学习，学习使我感到进步。&quot;)</span><br><span class="line">con2 = jieba.cut(&quot;我热爱工作，工作可以让我感到快乐。&quot;)</span><br><span class="line">con3 = jieba.cut(&quot;如果不让我学习，也不让我工作，我会觉得浑身不舒服。&quot;)</span><br><span class="line"></span><br><span class="line"># 转换成列表</span><br><span class="line">content1 = list(con1)</span><br><span class="line">content2 = list(con2)</span><br><span class="line">content3 = list(con3)</span><br><span class="line"></span><br><span class="line"># 把列表转换成字符串</span><br><span class="line">c1 = &apos; &apos;.join(content1)</span><br><span class="line">c2 = &apos; &apos;.join(content2)</span><br><span class="line">c3 = &apos; &apos;.join(content3)</span><br><span class="line"></span><br><span class="line">print(c1, c2, c3)</span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">data = cv.fit_transform([c1, c2, c3])</span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我 热爱 学习 ， 学习 使 我 感到 进步 。 我 热爱工作 ， 工作 可以 让 我 感到 快乐 。 如果 不让 我 学习 ， 也 不让 我 工作 ， 我会 觉得 浑身 不 舒服 。</span><br><span class="line">[&apos;不让&apos;, &apos;可以&apos;, &apos;如果&apos;, &apos;学习&apos;, &apos;工作&apos;, &apos;快乐&apos;, &apos;感到&apos;, &apos;我会&apos;, &apos;浑身&apos;, &apos;热爱&apos;, &apos;热爱工作&apos;, &apos;舒服&apos;, &apos;觉得&apos;, &apos;进步&apos;]</span><br><span class="line">[[0 0 0 2 0 0 1 0 0 1 0 0 0 1]</span><br><span class="line"> [0 1 0 0 1 1 1 0 0 0 1 0 0 0]</span><br><span class="line"> [2 0 1 1 1 0 0 1 1 0 0 1 1 0]]</span><br></pre></td></tr></table></figure><p>从栗子中可以看到，jieba分词包把句子进行了分词，然后对每个词语的个数进行了统计，但是对于 ‘我’、 ‘也’ 这样的单个中文，并没有统计个数，因为这样的单个中文统计没有意义。</p><p>至此，我们学会了统计文章中英文和中文的词语的个数，那么，单纯统计一个词语出现的个数越多就表示这个词语在文章中越重要吗？那比如，“我们”，“你们”，“他们”，“你的”，这样的指示代词出现的频率应该是最高的，能说明代词是文章的重点吗？显然不是。怎么过滤掉这种出现很多，但是并不是重点的词语呢？我们就要开始学习一种 TF-IDF 的处理方法了。</p><h6 id="3-4-5、TF-IDF"><a href="#3-4-5、TF-IDF" class="headerlink" title="3.4.5、TF-IDF"></a><strong>3.4.5、TF-IDF</strong></h6><p><strong>主要思想</strong>：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。<br><strong>作用</strong>：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。<br><strong>类</strong>：sklearn.feature_extraction.text.TfidfVectorizer<br><strong>TfidfVectorizer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TfidfVectorizer(stop_words=None,…)</span><br><span class="line">    返回词的权重矩阵</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.fit_transform(X,y)       </span><br><span class="line">    X:文本或者包含文本字符串的可迭代对象</span><br><span class="line">    返回值：返回sparse矩阵</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.inverse_transform(X)</span><br><span class="line">    X:array数组或者sparse矩阵</span><br><span class="line">    返回值:转换之前数据格式</span><br><span class="line"></span><br><span class="line">TfidfVectorizer.get_feature_names()</span><br><span class="line">    返回值:单词列表</span><br></pre></td></tr></table></figure><p>我们开始举个栗子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</span><br><span class="line"></span><br><span class="line">con1 = jieba.cut(&quot;我们热爱学习，学习使我们感到进步。&quot;)</span><br><span class="line">con2 = jieba.cut(&quot;我们热爱工作，工作可以让我们感到快乐。&quot;)</span><br><span class="line">con3 = jieba.cut(&quot;如果不让我们学习，不让我们工作，就会感到浑身不舒服。&quot;)</span><br><span class="line"></span><br><span class="line"># 转换成列表</span><br><span class="line">content1 = list(con1)</span><br><span class="line">content2 = list(con2)</span><br><span class="line">content3 = list(con3)</span><br><span class="line"></span><br><span class="line"># 把列表转换成字符串</span><br><span class="line">c1 = &apos; &apos;.join(content1)</span><br><span class="line">c2 = &apos; &apos;.join(content2)</span><br><span class="line">c3 = &apos; &apos;.join(content3)</span><br><span class="line"></span><br><span class="line">print(c1, c2, c3)</span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line">data = tf.fit_transform([c1, c2, c3])</span><br><span class="line">print(tf.get_feature_names())</span><br><span class="line">print(data.toarray())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">我们 热爱 学习 ， 学习 使 我们 感到 进步 。 我们 热爱工作 ， 工作 可以 让 我们 感到 快乐 。 如果 不让 我们 学习 ， 不让 我们 工作 ， 就 会 感到 浑身 不 舒服 。</span><br><span class="line">[&apos;不让&apos;, &apos;可以&apos;, &apos;如果&apos;, &apos;学习&apos;, &apos;工作&apos;, &apos;快乐&apos;, &apos;感到&apos;, &apos;我们&apos;, &apos;浑身&apos;, &apos;热爱&apos;, &apos;热爱工作&apos;, &apos;舒服&apos;, &apos;进步&apos;]</span><br><span class="line">[[0.         0.         0.         0.61800047 0.         0.</span><br><span class="line">  0.23996625 0.4799325  0.         0.40629818 0.         0.</span><br><span class="line">  0.40629818]</span><br><span class="line"> [0.         0.43345167 0.         0.         0.32965117 0.43345167</span><br><span class="line">  0.25600354 0.51200708 0.         0.         0.43345167 0.</span><br><span class="line">  0.        ]</span><br><span class="line"> [0.63561168 0.         0.31780584 0.24169953 0.24169953 0.</span><br><span class="line">  0.18770125 0.3754025  0.31780584 0.         0.         0.31780584</span><br><span class="line">  0.        ]]</span><br></pre></td></tr></table></figure><p>我们可以看到，通过 TF-IDF 的处理，把每句话的重点单词找出来了，第一句话 “我们” 和 “学习” 都出现了2次 ，但 “学习”是0.61800047，“我们” 是0.4799325， 重点是 “学习” ， 第三句话“我们” 和 “ 不让 ” 都出现了2次 ，但 “不让”是0.63561168 ，“我们” 是0.3754025 ， 第三句话重点强调 “不让” ，因为 “我们” 在三句话中都频繁出现，并不是每一句话的 “专属” ， 也就不是一句话的重点了。</p><h5 id="3-5、数据的特征处理"><a href="#3-5、数据的特征处理" class="headerlink" title="3.5、数据的特征处理"></a><strong>3.5、数据的特征处理</strong></h5><h6 id="3-5-1、特征处理是什么？"><a href="#3-5-1、特征处理是什么？" class="headerlink" title="3.5.1、特征处理是什么？"></a><strong>3.5.1、特征处理是什么</strong>？</h6><p>通过特定的统计方法（数学方法）将数据转换成算法要求的数据。</p><h6 id="3-5-2、为什么需要特征处理？"><a href="#3-5-2、为什么需要特征处理？" class="headerlink" title="3.5.2、为什么需要特征处理？"></a><strong>3.5.2、为什么需要特征处理</strong>？</h6><p>每个特征的单位不一样，比如相亲的时候，有乘坐飞机的里程数，人的身高，玩游戏的时间，里程数的数值很大，身高相对里程数值很小，那么在做分析的时候，里程数的数值就会起决定性作用。事实在，在统计分析的时候，分析人员认为每个特征同样重要。所以我们需要把不同单位的数值进行特征处理，不因为数值的相差巨大而造成特征的差别。</p><h6 id="3-5-3、特征处理的方法"><a href="#3-5-3、特征处理的方法" class="headerlink" title="3.5.3、特征处理的方法"></a><strong>3.5.3、特征处理的方法</strong></h6><p>数值型数据：（标准缩放）<br>1、<strong>归一化</strong><br>2、<strong>标准化</strong><br>3、<strong>缺失值</strong><br>类别型数据：one-hot编码<br>时间类型：时间的切分</p><h6 id="3-5-4、归一化："><a href="#3-5-4、归一化：" class="headerlink" title="3.5.4、归一化："></a><strong>3.5.4、归一化</strong>：</h6><p><strong>特点</strong>：通过对原始数据进行变换把数据映射到(默认为[0,1])之间。<br><strong>公式</strong>： &nbsp;&nbsp;𝑋′= (𝑥−𝑚𝑖𝑛)/(𝑚𝑎𝑥−𝑚𝑖𝑛) &nbsp;&nbsp;&nbsp;&nbsp; 𝑋′′=𝑋′∗(𝑚𝑥−𝑚𝑖)+𝑚𝑖<br>其中：作用于每一列，max为一列的最大值，min为一列的最小值，那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0。<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.2.png" alt="归一化计算过程"><br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.3.png" alt="归一化计算过程"></p><p><strong>sklearn归一化API</strong> : sklearn.preprocessing.MinMaxScaler<br><strong>MinMaxScaler语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MinMaxScalar(feature_range=(0,1)…)</span><br><span class="line">    每个特征缩放到给定范围(默认[0,1])</span><br><span class="line"></span><br><span class="line">MinMaxScalar.fit_transform(X)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br></pre></td></tr></table></figure><p><strong>归一化步骤</strong>：<br>1、实例化MinMaxScalar<br>2、通过fit_transform转换</p><p><strong>举一个栗子</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">mm = MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,15,46]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[1.         0.         0.         0.        ]</span><br><span class="line"> [0.         1.         1.         0.83333333]</span><br><span class="line"> [0.5        0.5        1.         1.        ]]</span><br></pre></td></tr></table></figure><p>我们可以看到，之前的数据，特征一（ 90,60,75） 是比特征二（2,4,3）在数值上大很多的，那么，如果不做特征处理，直接带入模型处理的话，特征一显然就占决定性作用了，就没有特征二什么事情了。而进行归一化转换之后，特征一和特征二在数值上就在同一量级了，他们就变得“同等重要”了。</p><p><strong>归一化总结</strong>：注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。</p><p>对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变。那么，我们有没有好的解决办法呢？有的，那就是标准化。</p><h6 id="3-5-5、标准化"><a href="#3-5-5、标准化" class="headerlink" title="3.5.5、标准化"></a><strong>3.5.5、标准化</strong></h6><p><strong>特点</strong>：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内<br><strong>公式</strong> ：𝑋′= (𝑥−mean)/𝜎<br>其中，mean为平均值，𝜎为标准差(考量数据的稳定性)<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.5.png" alt="标准差"></p><p>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。</p><p><strong>sklearn特征处理API</strong> : scikit-learn.preprocessing.StandardScaler</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StandardScaler(…)</span><br><span class="line">    处理之后每列来说所有数据都聚集在均值0附近方差为1</span><br><span class="line"></span><br><span class="line">StandardScaler.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br><span class="line"></span><br><span class="line">StandardScaler.mean_</span><br><span class="line">    原始数据中每列特征的平均值</span><br><span class="line"></span><br><span class="line">StandardScaler.std_</span><br><span class="line">    原始数据每列特征的方差</span><br></pre></td></tr></table></figure><p><strong>标准化步骤</strong>：<br>1、实例化StandardScaler<br>2、通过fit_transform转换</p><p><strong>举个栗子</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">std = StandardScaler()</span><br><span class="line">data = std.fit_transform([[ 1., -1., 3.],[ 2., 4., 2.],[ 4., 6., -1.]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[-1.06904497 -1.35873244  0.98058068]</span><br><span class="line"> [-0.26726124  0.33968311  0.39223227]</span><br><span class="line"> [ 1.33630621  1.01904933 -1.37281295]]</span><br></pre></td></tr></table></figure><p><strong>标准化总结</strong>：标准化可以避免最大值，最小值发生异常值的干扰。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</p><p><strong>缺失值处理方法</strong><br><strong>删除</strong>：如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列。</p><p><strong>插补</strong>：可以通过缺失值每行或者每列的平均值、中位数来填充。（主要方法）</p><p><strong>sklearn缺失值API</strong>: sklearn.preprocessing.Imputer<br><strong>Imputer语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, axis=0)</span><br><span class="line">    完成缺失值插补</span><br><span class="line"></span><br><span class="line">Imputer.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后的形状相同的array</span><br></pre></td></tr></table></figure><p><strong>Imputer流程</strong>：<br>1、初始化Imputer,指定”缺失值”，指定填补策略，指定行或列<br>2、调用fit_transform</p><p>关于np.nan(np.NaN)<br>1、 numpy的数组中可以使用np.nan/np.NaN来代替缺失值，属于float类型。<br>2、如果是文件中的一些缺失值，可以替换成nan，通过np.array转化成float型的数组即可。</p><h5 id="3-6、数据的特征选择"><a href="#3-6、数据的特征选择" class="headerlink" title="3.6、数据的特征选择"></a><strong>3.6、数据的特征选择</strong></h5><h6 id="3-6-1、特征选择是什么？"><a href="#3-6-1、特征选择是什么？" class="headerlink" title="3.6.1、特征选择是什么？"></a><strong>3.6.1、特征选择是什么</strong>？</h6><p>特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。</p><h6 id="3-6-2、为什么要做特征选择？"><a href="#3-6-2、为什么要做特征选择？" class="headerlink" title="3.6.2、为什么要做特征选择？"></a><strong>3.6.2、为什么要做特征选择</strong>？</h6><p>冗余：部分特征的相关度高，容易消耗计算性能<br>噪声：部分特征对预测结果有负影响</p><h6 id="3-6-3、特征选择主要方法："><a href="#3-6-3、特征选择主要方法：" class="headerlink" title="3.6.3、特征选择主要方法："></a><strong>3.6.3、特征选择主要方法</strong>：</h6><p>Filter(过滤式):VarianceThreshold<br>Embedded(嵌入式)：正则化、决策树</p><h6 id="3-6-4、sklearn特征选择API"><a href="#3-6-4、sklearn特征选择API" class="headerlink" title="3.6.4、sklearn特征选择API"></a><strong>3.6.4、sklearn特征选择API</strong></h6><p>sklearn.feature_selection.VarianceThreshold<br>VarianceThreshold语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">VarianceThreshold(threshold = 0.0)</span><br><span class="line">    删除所有低方差特征</span><br><span class="line"></span><br><span class="line">Variance.fit_transform(X,y)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：训练集差异低于threshold的特征将被删除。</span><br><span class="line">    默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。</span><br></pre></td></tr></table></figure><h6 id="3-6-5、VarianceThreshold流程："><a href="#3-6-5、VarianceThreshold流程：" class="headerlink" title="3.6.5、VarianceThreshold流程："></a><strong>3.6.5、VarianceThreshold流程</strong>：</h6><p>1、初始化VarianceThreshold,指定阀值方差<br>2、调用fit_transform</p><h6 id="3-6-6、举个栗子："><a href="#3-6-6、举个栗子：" class="headerlink" title="3.6.6、举个栗子："></a><strong>3.6.6、举个栗子</strong>：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">var = VarianceThreshold(threshold=1.0)</span><br><span class="line">data = var.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0]</span><br><span class="line"> [4]</span><br><span class="line"> [1]]</span><br></pre></td></tr></table></figure><p>从栗子中，可以看到，把方差是0的第一个特征值（0,0,0），第4个特征值（3,3,3），和方差小于1的第2个特征值（2,1,2）都给删除了，只剩下（0,4,1）这个方差大于1的特征值。默认情况下，threshold 等于1.0 。</p><h5 id="3-7、降维-（PCA）"><a href="#3-7、降维-（PCA）" class="headerlink" title="3.7、降维 （PCA）"></a><strong>3.7、降维 （PCA）</strong></h5><h6 id="3-7-1、sklearn降维API-："><a href="#3-7-1、sklearn降维API-：" class="headerlink" title="3.7.1、sklearn降维API ："></a><strong>3.7.1、sklearn降维API</strong> ：</h6><p>sklearn. decomposition</p><h6 id="3-7-2、本质："><a href="#3-7-2、本质：" class="headerlink" title="3.7.2、本质："></a><strong>3.7.2、本质</strong>：</h6><p>PCA是一种分析、简化数据集的技术。</p><h6 id="3-7-3、目的："><a href="#3-7-3、目的：" class="headerlink" title="3.7.3、目的："></a><strong>3.7.3、目的</strong>：</h6><p>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</p><h6 id="3-7-4、作用："><a href="#3-7-4、作用：" class="headerlink" title="3.7.4、作用："></a><strong>3.7.4、作用</strong>：</h6><p>可以削减回归分析或者聚类分析中特征的数量。</p><h6 id="3-7-5、PCA语法："><a href="#3-7-5、PCA语法：" class="headerlink" title="3.7.5、PCA语法："></a><strong>3.7.5、PCA语法</strong>：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PCA(n_components=None)</span><br><span class="line">    将数据分解为较低维数空间</span><br><span class="line"></span><br><span class="line">PCA.fit_transform(X)       </span><br><span class="line">    X:numpy array格式的数据[n_samples,n_features]</span><br><span class="line">    返回值：转换后指定维度的array</span><br></pre></td></tr></table></figure><h6 id="3-7-6、PCA流程："><a href="#3-7-6、PCA流程：" class="headerlink" title="3.7.6、PCA流程："></a><strong>3.7.6、PCA流程</strong>：</h6><p>1、初始化PCA,指定减少后的维度<br>2、调用fit_transform</p><h6 id="3-7-7、举个栗子"><a href="#3-7-7、举个栗子" class="headerlink" title="3.7.7、举个栗子"></a><strong>3.7.7、举个栗子</strong></h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">pca = PCA(n_components=0.9)</span><br><span class="line">data = pca.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,15,46]])</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 15.77507261]</span><br><span class="line"> [-15.11129418]</span><br><span class="line"> [ -0.66377843]]</span><br></pre></td></tr></table></figure><p>该栗子中，原本有4个特征的数据，变成了一个特征，并且该特征保留了原来90%的信息，n_components=0.9 。</p><h1 id="四、机器学习基础"><a href="#四、机器学习基础" class="headerlink" title="四、机器学习基础"></a>四、机器学习基础</h1><h5 id="4-1、机器学习开发流程"><a href="#4-1、机器学习开发流程" class="headerlink" title="4.1、机器学习开发流程"></a><strong>4.1、机器学习开发流程</strong></h5><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.6.png" alt="机器学习开发流程"><br>首先要根据原始数据明确问题做什么，建立模型： 根据数据类型划分应用种类；然后做数据的基本处理：（缺失值，合并表等）和特征工程（特征进行处理） （重要）；其次，找到合适的算法进行预测： 最后，对模型评估，根据模型的准确率，判定效果，如果合格 ，则上线使用，以API形式提供，如果不合格，则要换算法 或者重新提取特征工程，如此循环往复继续下去，直到得到满意的模型。</p><h5 id="4-2、机器学习模型是什么？"><a href="#4-2、机器学习模型是什么？" class="headerlink" title="4.2、机器学习模型是什么？"></a><strong>4.2、机器学习模型是什么</strong>？</h5><p><strong>定义</strong>：通过一种映射关系将输入值到输出值。<br>简单来讲，模型 = 算法 + 数据。<br><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.7.png" alt="模型"></p><h5 id="4-3、机器学习算法分类"><a href="#4-3、机器学习算法分类" class="headerlink" title="4.3、机器学习算法分类"></a><strong>4.3、机器学习算法分类</strong></h5><p><img src="http://wangpengcufe.com/python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.8.png" alt="算法分类"></p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-python-python2" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2020-01-02T15:28:56.000Z"><a href="/machinelearning/python-python2/">2020-01-02</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/python-python2/">案例（二）如何把python项目部署到linux服务器上</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2linux%E6%9C%8D%E5%8A%A1%E5%99%A8.png" alt="Python项目部署"></p><h1 id="一、案例背景"><a href="#一、案例背景" class="headerlink" title="一、案例背景"></a><strong>一、案例背景</strong></h1><p>用Python写了个脚本，需要部署到Linux环境的服务器上，由于服务器linux系统（centos,redhat等）自带的是python2，现在的python萌新都是从python3开始学习，所以需要给服务器配置一个python3环境，一番摸索过后，踩过一些坑，也总结了一下经验，故留此文。</p><h1 id="二、主要应用技术"><a href="#二、主要应用技术" class="headerlink" title="二、主要应用技术"></a><strong>二、主要应用技术</strong></h1><h2 id="2-1、linux-命令"><a href="#2-1、linux-命令" class="headerlink" title="2.1、linux 命令"></a><strong>2.1、linux 命令</strong></h2><p><img src="http://wangpengcufe.com/python2-1.png" alt="linux命令"></p><h1 id="三、实施步骤"><a href="#三、实施步骤" class="headerlink" title="*三、实施步骤 *"></a>*<em>三、实施步骤 *</em></h1><h3 id="3-1、安装python3"><a href="#3-1、安装python3" class="headerlink" title="3.1、安装python3"></a><strong>3.1、安装python3</strong></h3><p>开始安装之前先看一下机器的环境，主要看一下操作系他的环境和python版本。<br>查看环境<br>操作系统:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]#  cat /etc/redhat-release</span><br><span class="line">CentOS release 6.5 (Final)</span><br></pre></td></tr></table></figure><p>查看python版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# python -V</span><br><span class="line">Python 2.6.6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@reportweb]# python3 -V</span><br><span class="line">-bash: python3: command not found</span><br></pre></td></tr></table></figure><p>可以看到操作系统是CentOS 6.5 ，python此时只有一个python2，没有python 3 。</p><p>下面开始进入正式安装：<br>第1步：下载python安装包，准备编译环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make</span><br></pre></td></tr></table></figure><p>第2步：下载python源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# wget https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz</span><br></pre></td></tr></table></figure><p>第3步：解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# tar -zxvf Python-3.6.6.tgz</span><br></pre></td></tr></table></figure><p>第4步：安装，编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# cd Python-3.6.2</span><br><span class="line">[root@reportweb Python-3.6.2] # mkdir /usr/local/python3.6</span><br><span class="line">[root@reportweb Python-3.6.2] # ./configure --prefix=/usr/local/python3.6</span><br><span class="line">[root@reportweb Python-3.6.2] # make</span><br><span class="line">[root@reportweb Python-3.6.2] # make install</span><br></pre></td></tr></table></figure><p>第5步：建立软链</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb Python-3.6.2] # ln -s /usr/local/python3.6/bin/python3.6  /usr/bin/python3</span><br></pre></td></tr></table></figure><p>第6步：查询python版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# python3 -V</span><br><span class="line">Python 3.6.6</span><br><span class="line"></span><br><span class="line">[root@reportweb]# python -V</span><br><span class="line">Python 2.6.6</span><br></pre></td></tr></table></figure><p>至此，python3安装完毕。</p><h3 id="3-2、安装pip3"><a href="#3-2、安装pip3" class="headerlink" title="3.2、安装pip3"></a><strong>3.2、安装pip3</strong></h3><p>下面开始安装pip3的步骤。<br>第1步：安装setuptools</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# wget --no-check-certificate  https://pypi.python.org/packages/source/s/setuptools/setuptools-19.6.tar.gz#md5=c607dd118eae682c44ed146367a17e26</span><br></pre></td></tr></table></figure><p>第2步：解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src]# tar -zxvf setuptools-19.6.tar.gz</span><br></pre></td></tr></table></figure><p>第3步：编译,安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb src] # cd setuptools-19.6/</span><br><span class="line">[root@reportweb setuptools-19.6] # python3 setup.py build</span><br><span class="line">[root@reportweb setuptools-19.6] # python3 setup.py install</span><br></pre></td></tr></table></figure><p>第4步：建立软链</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb setuptools-19.6] #  ln -s /usr/local/python3.6/bin/pip3 /usr/bin/pip3</span><br></pre></td></tr></table></figure><p>第5步：查看pip3版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb] # pip3 -V</span><br><span class="line">pip 8.0.2 from /usr/local/python3.6/lib/python3.6/site-packages/pip-8.0.2-py3.6.egg (python 3.6)</span><br></pre></td></tr></table></figure><p>第6步：更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb] # pip3 install --upgrade pip</span><br><span class="line">Collecting pip</span><br><span class="line">  Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB)</span><br><span class="line">    100% |████████████████████████████████| 1.4MB 172kB/s </span><br><span class="line">Installing collected packages: pip</span><br><span class="line">  Found existing installation: pip 8.0.2</span><br><span class="line">    Uninstalling pip-8.0.2:</span><br><span class="line">      Successfully uninstalled pip-8.0.2</span><br><span class="line">Successfully installed pip-19.3.1</span><br><span class="line">[root@reportweb]# pip3 -V</span><br><span class="line">pip 19.3.1 from /usr/local/python3.6/lib/python3.6/site-packages/pip (python 3.6)</span><br></pre></td></tr></table></figure><p>至此，pip3安装完毕。</p><h3 id="3-3、启动服务"><a href="#3-3、启动服务" class="headerlink" title="3.3、启动服务"></a><strong>3.3、启动服务</strong></h3><p>启动服务分为前台启动和后台启动。</p><p>前台启动，就是应用直接由窗口运行，能在窗口直接打印出日志信息，如果手动 ctrl + C 打断时，应用停止。当使用前台启动时，如果我们退出终端，服务就会停止。<br>后台启动，就是用nohup等命令，执行应用，在窗口关闭后，或者 执行其他命令时，该应用仍然可以再后台运行。</p><p>使用前台启动python项目：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# python3 test.py</span><br><span class="line">2020-01-02 16:00:47 读取原日志信息：00:00:01.560 [http-nio-8605-exec-5] [DEBUG] Bound request context to thread: org.apache.catalina.connector.RequestFacade@2dc071e6 org.springframework.boot.web.filter.OrderedRequestContextFilter [RID:] [TID:] [MID:]</span><br><span class="line">2020-01-02 16:00:47 转换格式前信息：00:00:01.560 [http-nio-8605-exec-5] [DEBUG] Bound request context to thread: org.apache.catalina.connector.RequestFacade@2dc071e6 org.springframework.boot.web.filter.OrderedRequestContextFilter [RID:] [TID:] [MID:]</span><br><span class="line">^CTraceback (most recent call last):</span><br><span class="line">  File &quot;test.py&quot;, line 174, in &lt;module&gt;</span><br><span class="line">    read_rawlog()</span><br><span class="line">  File &quot;test.py&quot;, line 79, in read_rawlog</span><br><span class="line">    line_json = log_data_to_json(line_temp)</span><br><span class="line">  File &quot;test.py&quot;, line 120, in log_data_to_json</span><br><span class="line">    logger = re.findall(logger_rule, line)</span><br><span class="line">  File &quot;/usr/python3/lib/python3.6/re.py&quot;, line 222, in findall</span><br><span class="line">    return _compile(pattern, flags).findall(string)</span><br><span class="line">KeyboardInterrupt</span><br><span class="line">[root@reportweb data]#</span><br></pre></td></tr></table></figure><p>前台会输出正常的日志信息，直到你按下 ctrl + c 打断它，就会报KeyboardInterrupt。<br>下面演示后台启动python文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb data]# nohup python3 test.py &amp;</span><br></pre></td></tr></table></figure><p>简单介绍一下nohup 命令</p><p>用途：不挂断地运行命令。</p><p>语法：nohup Command [ Arg … ] [　&amp; ]</p><p>描述：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思( no hang up)。nohup 执行 默认会自动生成一个 nohup.out 记录文件。</p><h3 id="3-4、停止服务"><a href="#3-4、停止服务" class="headerlink" title="3.4、停止服务"></a><strong>3.4、停止服务</strong></h3><p>前台启动的服务，直接 ctrl + c 停止服务，或者退出终端，服务就会停止。</p><p>后台启动的服务，用命令查看pid，然后 kill -9 pid 杀死任务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@reportweb]# ps -ef|grep python</span><br><span class="line">root      9233  2598 95 16:22 pts/0    00:00:16 python3 test.py</span><br><span class="line">root      9244  2598  0 16:22 pts/0    00:00:00 grep python</span><br><span class="line">[root@reportweb]# kill -9 9233</span><br><span class="line">[root@reportweb]# ps -ef|grep python</span><br><span class="line">[root@reportweb]# ps -ef|grep python</span><br><span class="line">root      9257  2598  0 16:23 pts/0    00:00:00 grep python</span><br><span class="line">[1]+  已杀死               nohup python3 test.py</span><br></pre></td></tr></table></figure><h3 id="3-5、查看python执行的日志"><a href="#3-5、查看python执行的日志" class="headerlink" title="3.5、查看python执行的日志"></a><strong>3.5、查看python执行的日志</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">查看前10行命令：</span><br><span class="line">[root@reportweb]# head -n 10 nohup.out</span><br><span class="line"></span><br><span class="line">查看后10行命令：</span><br><span class="line">[root@reportweb]# tail -n 10 nohup.out</span><br><span class="line"></span><br><span class="line">显示文件 nohup.log 的内容，从第 20 行至文件末尾:</span><br><span class="line">[root@reportweb]# tail +20 nohup.out</span><br><span class="line"></span><br><span class="line">要跟踪名为 nohup.log 的文件的增长情况，请输入以下命令：</span><br><span class="line">[root@reportweb]# tail -f nohup.out</span><br></pre></td></tr></table></figure><p>tail -f filename 会把 filename 文件里的最尾部的内容显示在屏幕上，并且不断刷新，只要 filename 更新就可以看到最新的文件内容。</p><p>命令格式：<br>tail [参数] [文件]<br>参数：</p><p>-f 循环读取<br>-q 不显示处理信息<br>-v 显示详细的处理信息<br>-c&lt;数目&gt; 显示的字节数<br>-n&lt;行数&gt; 显示文件的尾部 n 行内容<br>–pid=PID 与-f合用,表示在进程ID,PID死掉之后结束<br>-q, –quiet, –silent 从不输出给出文件名的首部<br>-s, –sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ml-ml16" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T15:28:56.000Z"><a href="/machinelearning/ml-ml16/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml16/">机器学习（十六） 特征变换-标签和索引的转化</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/ml16.png" alt="卡方选择器"></p><h1 id="一、原理"><a href="#一、原理" class="headerlink" title="一、原理"></a><strong>一、原理</strong></h1><p>在机器学习处理过程中,为了方便相关算法的实现,经常需要把标签数据(一般是字符串)转化成整数索引,或是在计算结束后将整数索引还原为相应的标签.<br>Spark ML 包中提供了几个相关的转换器:<br>StringIndexer,IndexToString,OneHotEncoder,VectorIndexer,他们提供了十分方便的特征转换功能,这些转换器都位于org.apache.spark.ml.feature包下。</p><p>值得注意的是,用于特征转换的转换器和其他的机器学习算法一样,也属于Ml Pipeline模型的一部分,可以用来构成机器学习流水线,以StringIndexer为例。</p><p>StringIndexer（字符串-索引变换）将字符串的标签编码成标签索引。标签索引序列的取值范围是[0，numLabels（字符串中所有出现的单词去掉重复的词后的总和）]，按照标签出现频率排序，出现最多的标签索引为0。如果输入是数值型，我们先将数值映射到字符串，再对字符串进行索引化。如果下游的pipeline（例如：Estimator或者Transformer）需要用到索引化后的标签序列，则需要将这个pipeline的输入列名字指定为索引化序列的名字。大部分情况下，通过setInputCol设置输入的列名。</p><p>下面来具体介绍StringIndexer、IndexToString、OneHotEncoder、VectorIndexer。</p><h1 id="二、StringIndexer（字符串-索引变换）"><a href="#二、StringIndexer（字符串-索引变换）" class="headerlink" title="二、StringIndexer（字符串-索引变换）"></a><strong>二、StringIndexer（字符串-索引变换）</strong></h1><h3 id="2-1、原理"><a href="#2-1、原理" class="headerlink" title="2.1、原理"></a><strong>2.1、原理</strong></h3><p>StringIndexer将标签的字符串列编码为标签索引的列。 索引位于[0，numLabels）中，并支持四个排序选项：“frequencyDesc”：按标签频率的降序（最频繁的标签分配为0），“frequencyAsc”：按标签频率的升序（最不频繁的标签分配为0） ，“alphabetDesc”：降序字母顺序和“alphabetAsc”：升序字母顺序（默认=“frequencyDesc”）。 如果用户选择保留，则看不见的标签将放置在索引numLabels处。 如果输入列为数字，则将其强制转换为字符串并为字符串值编制索引。 当下游管道组件（例如Estimator或Transformer）使用此字符串索引标签时，必须将组件的输入列设置为此字符串索引列名称。 在许多情况下，可以使用setInputCol设置输入列。</p><h3 id="2-2、代码实现"><a href="#2-2、代码实现" class="headerlink" title="2.2、代码实现"></a><strong>2.2、代码实现</strong></h3><p>首先引入需要用的包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line">import org.apache.spark.ml.feature.IndexToString;</span><br><span class="line">import org.apache.spark.ml.feature.OneHotEncoderEstimator;</span><br><span class="line">import org.apache.spark.ml.feature.StringIndexer;</span><br><span class="line">import org.apache.spark.ml.feature.StringIndexerModel;</span><br><span class="line">import org.apache.spark.ml.feature.VectorIndexer;</span><br><span class="line">import org.apache.spark.ml.feature.VectorIndexerModel;</span><br><span class="line">import org.apache.spark.ml.linalg.VectorUDT;</span><br><span class="line">import org.apache.spark.ml.linalg.Vectors;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.Metadata;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line">import scala.collection.immutable.Set;</span><br></pre></td></tr></table></figure><p>获取spark:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = SparkSession.builder().appName(&quot;StringIndexerTest&quot;).master(&quot;local&quot;).getOrCreate();</span><br></pre></td></tr></table></figure><p>构造一些简单数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rowRDD = Arrays.asList(RowFactory.create(0,&quot;a&quot;),</span><br><span class="line">                        RowFactory.create(1,&quot;b&quot;),</span><br><span class="line">                        RowFactory.create(2,&quot;c&quot;),</span><br><span class="line">                        RowFactory.create(3,&quot;a&quot;),</span><br><span class="line">                        RowFactory.create(4,&quot;a&quot;),</span><br><span class="line">                        RowFactory.create(5,&quot;c&quot;));</span><br><span class="line"></span><br><span class="line">StructType schema = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;id&quot;,DataTypes.IntegerType,false,Metadata.empty()),</span><br><span class="line">        new StructField(&quot;category&quot;,DataTypes.StringType,false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.createDataFrame(rowRDD, schema);</span><br><span class="line">df.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+</span><br><span class="line">|id |category|</span><br><span class="line">+---+--------+</span><br><span class="line">|0  |a       |</span><br><span class="line">|1  |b       |</span><br><span class="line">|2  |c       |</span><br><span class="line">|3  |a       |</span><br><span class="line">|4  |a       |</span><br><span class="line">|5  |c       |</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure><p>然后构建StringIndexer模型，我们创建一个StringIndexer对象，设定输入输出列名，其余参数采用默认值，并对这个DataFrame进行训练，产生StringIndexerModel对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StringIndexer indexer = new StringIndexer().setInputCol(&quot;category&quot;).setOutputCol(&quot;categoryIndex&quot;);</span><br><span class="line">StringIndexerModel model = indexer.fit(df);</span><br></pre></td></tr></table></figure><p>之后我们即可利用StringIndexerModel对象对DataFrame数据进行转换操作，可以看到，默认情况下，StringIndexerModel依次按照出现频率的高低，把字符标签进行了排序，即出现最多的“a”被编号成0，“c”为1，出现最少的“b”为0。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; indexed1 = model.transform(df);</span><br><span class="line">indexed1.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+</span><br><span class="line">|id |category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|0  |a       |0.0          |</span><br><span class="line">|1  |b       |2.0          |</span><br><span class="line">|2  |c       |1.0          |</span><br><span class="line">|3  |a       |0.0          |</span><br><span class="line">|4  |a       |0.0          |</span><br><span class="line">|5  |c       |1.0          |</span><br><span class="line">+---+--------+-------------+</span><br></pre></td></tr></table></figure><p>如果我们使用已有的数据构建了一个StringIndexerModel，然后再构建一个新的DataFrame，这个DataFrame中有着模型内未曾出现的标签“d”，用已有的模型去转换这一DataFrame会有什么效果？<br>实际上，如果直接转换的话，Spark会抛出异常，报出“Unseen label: d”的错误。<br>为了处理这种情况，在模型训练后，可以通过设置setHandleInvalid(“skip”)来忽略掉那些未出现的标签，这样，带有未出现标签的行将直接被过滤掉，所下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rowRDD2 = Arrays.asList(RowFactory.create(0,&quot;a&quot;),</span><br><span class="line">        RowFactory.create(1,&quot;b&quot;),</span><br><span class="line">        RowFactory.create(2,&quot;c&quot;),</span><br><span class="line">        RowFactory.create(3,&quot;a&quot;),</span><br><span class="line">        RowFactory.create(4,&quot;a&quot;),</span><br><span class="line">        RowFactory.create(5,&quot;d&quot;));</span><br><span class="line">Dataset&lt;Row&gt; df2 = spark.createDataFrame(rowRDD2, schema);</span><br><span class="line">Dataset&lt;Row&gt; indexed2 = model.transform(df2);</span><br><span class="line">indexed2.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unseen label: d.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; indexed2 = model.setHandleInvalid(&quot;skip&quot;).transform(df2);</span><br><span class="line">indexed2.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+</span><br><span class="line">|id |category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|0  |a       |0.0          |</span><br><span class="line">|1  |b       |2.0          |</span><br><span class="line">|2  |c       |1.0          |</span><br><span class="line">|3  |a       |0.0          |</span><br><span class="line">|4  |a       |0.0          |</span><br><span class="line">+---+--------+-------------+</span><br></pre></td></tr></table></figure><h1 id="三、IndexToString（索引-字符串变换）"><a href="#三、IndexToString（索引-字符串变换）" class="headerlink" title="三、IndexToString（索引-字符串变换）"></a><strong>三、IndexToString（索引-字符串变换）</strong></h1><h3 id="3-1、原理"><a href="#3-1、原理" class="headerlink" title="3.1、原理"></a><strong>3.1、原理</strong></h3><p>与StringIndexer对应，IndexToString将索引化标签还原成原始字符串。一个常用的场景是先通过StringIndexer产生索引化标签，然后使用索引化标签进行训练，最后再对预测结果使用IndexToString来获取其原始的标签字符串。</p><h3 id="3-2、代码实现"><a href="#3-2、代码实现" class="headerlink" title="3.2、代码实现"></a><strong>3.2、代码实现</strong></h3><p>首先我们用StringIndexer读取数据集中的“category”列，把字符型标签转化成标签索引，然后输出到“categoryIndex”列上，构建出一个新的DataFrame数据集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rawData =  Arrays.asList(RowFactory.create(0, &quot;a&quot;),</span><br><span class="line">                        RowFactory.create(1, &quot;b&quot;),</span><br><span class="line">                        RowFactory.create(2, &quot;c&quot;),</span><br><span class="line">                        RowFactory.create(3, &quot;a&quot;),</span><br><span class="line">                        RowFactory.create(4, &quot;a&quot;),</span><br><span class="line">                        RowFactory.create(5, &quot;c&quot;));</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df3 = spark.createDataFrame(rawData, schema);</span><br><span class="line">Dataset&lt;Row&gt; indexed3 = indexer.fit(df3).transform(df3);</span><br></pre></td></tr></table></figure><p>然后我们创建IndexToString对象，读取“categoryIndex”上的标签索引，获得原有数据集的字符型标签，然后再输出到“originalCategory”列上。最后，通过输出“originalCategory”列，就可以看到数据集中原有的字符标签了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IndexToString converter = new IndexToString().setInputCol(&quot;categoryIndex&quot;).setOutputCol(&quot;originalCategory&quot;);</span><br><span class="line">Dataset&lt;Row&gt; converted3 = converter.transform(indexed3);</span><br><span class="line">converted3.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+----------------+</span><br><span class="line">|id |category|categoryIndex|originalCategory|</span><br><span class="line">+---+--------+-------------+----------------+</span><br><span class="line">|0  |a       |0.0          |a               |</span><br><span class="line">|1  |b       |2.0          |b               |</span><br><span class="line">|2  |c       |1.0          |c               |</span><br><span class="line">|3  |a       |0.0          |a               |</span><br><span class="line">|4  |a       |0.0          |a               |</span><br><span class="line">|5  |c       |1.0          |c               |</span><br><span class="line">+---+--------+-------------+----------------+</span><br></pre></td></tr></table></figure><h1 id="四、OneHotEncoder（独热编码）"><a href="#四、OneHotEncoder（独热编码）" class="headerlink" title="四、OneHotEncoder（独热编码）"></a><strong>四、OneHotEncoder（独热编码）</strong></h1><h3 id="4-1、原理"><a href="#4-1、原理" class="headerlink" title="4.1、原理"></a><strong>4.1、原理</strong></h3><p>独热编码（One-hot encoding）将类别特征映射为二进制向量，其中只有一个有效值（为1，其余为0）。这样在诸如Logistic回归这样需要连续数值值作为特征输入的分类器中也可以使用类别（离散）特征。</p><p>One-Hot编码适合一些期望类别特征为连续特征的算法，比如说逻辑斯蒂回归等。</p><h3 id="4-2、代码实现"><a href="#4-2、代码实现" class="headerlink" title="4.2、代码实现"></a><strong>4.2、代码实现</strong></h3><p>首先创建一个DataFrame，其包含一列类别性特征，需要注意的是，在使用OneHotEncoder进行转换前，DataFrame需要先使用StringIndexer将原始标签数值化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rawData4 = Arrays.asList(RowFactory.create(0.0, 1.0),</span><br><span class="line">          RowFactory.create(1.0, 0.0),</span><br><span class="line">          RowFactory.create(2.0, 1.0),</span><br><span class="line">          RowFactory.create(0.0, 2.0),</span><br><span class="line">          RowFactory.create(0.0, 1.0),</span><br><span class="line">          RowFactory.create(2.0, 0.0));</span><br><span class="line"></span><br><span class="line">StructType schema4 = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;id&quot;,DataTypes.DoubleType,false,Metadata.empty()),</span><br><span class="line">        new StructField(&quot;category&quot;,DataTypes.DoubleType,false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df4 = spark.createDataFrame(rawData4, schema4);</span><br></pre></td></tr></table></figure><p>我们创建OneHotEncoder对象对处理后的DataFrame进行编码，可以看见，编码后的二进制特征呈稀疏向量形式，与StringIndexer编码的顺序相同，需注意的是最后一个Category（”b”）被编码为全0向量，若希望”b”也占有一个二进制特征，则可在创建OneHotEncoder时指定setDropLast(false)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">OneHotEncoderEstimator encoder = new OneHotEncoderEstimator()</span><br><span class="line">                               .setInputCols(new String[] &#123;&quot;id&quot;,&quot;category&quot;&#125;)</span><br><span class="line">                               .setOutputCols(new String[] &#123;&quot;categoryVec1&quot;,&quot;categoryVec2&quot;&#125;);</span><br><span class="line">Dataset&lt;Row&gt; encoded4 = encoder.fit(df4).transform(df4);</span><br><span class="line">encoded4.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+-------------+-------------+</span><br><span class="line">|id |category|categoryVec1 |categoryVec2 |</span><br><span class="line">+---+--------+-------------+-------------+</span><br><span class="line">|0.0|1.0     |(2,[0],[1.0])|(2,[1],[1.0])|</span><br><span class="line">|1.0|0.0     |(2,[1],[1.0])|(2,[0],[1.0])|</span><br><span class="line">|2.0|1.0     |(2,[],[])    |(2,[1],[1.0])|</span><br><span class="line">|0.0|2.0     |(2,[0],[1.0])|(2,[],[])    |</span><br><span class="line">|0.0|1.0     |(2,[0],[1.0])|(2,[1],[1.0])|</span><br><span class="line">|2.0|0.0     |(2,[],[])    |(2,[0],[1.0])|</span><br><span class="line">+---+--------+-------------+-------------+</span><br></pre></td></tr></table></figure><h1 id="五、VectorIndexer-向量类型索引化"><a href="#五、VectorIndexer-向量类型索引化" class="headerlink" title="五、VectorIndexer(向量类型索引化)"></a><strong>五、VectorIndexer(向量类型索引化)</strong></h1><h3 id="5-1、原理"><a href="#5-1、原理" class="headerlink" title="5.1、原理"></a><strong>5.1、原理</strong></h3><p>VectorIndexer帮助索引Vector数据集中的分类特征。 它既可以自动确定哪些特征是分类的，又可以将原始值转换为分类索引。 具体来说，它执行以下操作：</p><p>1、设置类型为Vector的输入列和参数maxCategories。<br>2、根据不同值的数量确定应分类的要素，其中最多具有maxCategories的要素被声明为分类。<br>3、为每个分类特征计算从0开始的分类索引。<br>4、为分类特征建立索引，并将原始特征值转换为索引。</p><p>索引分类特征允许诸如决策树和树组合之类的算法适当地处理分类特征，从而提高性能。</p><h3 id="5-2、代码实现"><a href="#5-2、代码实现" class="headerlink" title="5.2、代码实现"></a><strong>5.2、代码实现</strong></h3><p>首先，我们读入一个数据集DataFrame，然后使用VectorIndexer训练出模型，来决定哪些特征需要被作为类别特征，将类别特征转换为索引，这里设置maxCategories为2，即只有种类小于2的特征才被认为是类别型特征，否则被认为是连续型特征：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Row&gt; rawData5 = Arrays.asList(RowFactory.create(Vectors.dense(-1.0, 1.0, 1.0)),</span><br><span class="line">                RowFactory.create(Vectors.dense(-1.0, 3.0, 1.0)),</span><br><span class="line">                RowFactory.create(Vectors.dense(0.0, 5.0, 1.0)));</span><br><span class="line"></span><br><span class="line">StructType schema5 = new StructType(new StructField[] &#123;</span><br><span class="line">        new StructField(&quot;features&quot;,new VectorUDT(),false,Metadata.empty())</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df5 = spark.createDataFrame(rawData5, schema5);</span><br><span class="line">df5.show(false);</span><br><span class="line">VectorIndexerModel indexModel = new VectorIndexer()</span><br><span class="line">                                .setInputCol(&quot;features&quot;)</span><br><span class="line">                                .setOutputCol(&quot;indexed&quot;)</span><br><span class="line">                                .setMaxCategories(2).fit(df5);</span><br><span class="line">Set&lt;Object&gt; categoricalFeatures = indexModel.categoryMaps().keySet();</span><br><span class="line">System.out.println(categoricalFeatures.mkString(&quot;,&quot;));</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0,2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; indexed5 = indexModel.transform(df5);</span><br><span class="line">indexed5.show(false);</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+--------------+-------------+</span><br><span class="line">|features      |indexed      |</span><br><span class="line">+--------------+-------------+</span><br><span class="line">|[-1.0,1.0,1.0]|[1.0,1.0,0.0]|</span><br><span class="line">|[-1.0,3.0,1.0]|[1.0,3.0,0.0]|</span><br><span class="line">|[0.0,5.0,1.0] |[0.0,5.0,0.0]|</span><br><span class="line">+--------------+-------------+</span><br></pre></td></tr></table></figure></div><footer><div class="clearfix"></div></footer></div></article><article id="post-python-python1" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-25T15:28:56.000Z"><a href="/machinelearning/python-python1/">2019-11-25</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/python-python1/">案例（一） 利用RFM模型做用户价值分析</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/python1-1.png" alt="RFM模型"></p><h1 id="一、案例背景"><a href="#一、案例背景" class="headerlink" title="一、案例背景"></a><strong>一、案例背景</strong></h1><p>在产品迭代过程中，通常需要根据用户的属性进行归类，也就是通过分析数据，对用户进行归类，以便于在推送及转化过程中获得更大的收益。</p><p>本案例是基于某互联网公司的实际用户购票数据为研究对象，对用户购票的时间，购买的金额进行了采集，每个用户用手机号来区别唯一性。数据分析人员根据用户购买的时间和金额，通过建立RFM模型，来计算出用户最近最近一次购买的打分，用户购买频率的打分，用户购买金额的打分，然后根据三个分数进行一个加权打分，和综合打分。业务人员可以根据用户的打分情况，对不同的用户进行个性化营销和精准营销，例如给不同的用户推送定制的营销短信，不同优惠额度的打折券等等。</p><p>通过RFM方法，可以根据用户的属性数据分析，对用户进行了归类。在推送、转化等很多过程中，可以更加精准化，不至于出现用户反感的情景，更重要的是，对产品转化等商业价值也有很大的帮助。</p><h1 id="二、RFM概念"><a href="#二、RFM概念" class="headerlink" title="二、RFM概念"></a><strong>二、RFM概念</strong></h1><p>RFM模型是衡量客户价值和客户创利能力的重要工具和手段。在众多的客户关系管理(CRM)的分析模式中，RFM模型是被广泛提到的。该机械模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱3项指标来描述该客户的价值状况。</p><p><strong>RFM分析</strong> 就是根据客户活跃程度和交易金额的贡献，进行客户价值细分的一种方法。其中：</p><p><strong>R（Recency）</strong>：客户最近一次交易时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。</p><p><strong>F（Frequency）</strong>：客户在最近一段时间内交易的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。</p><p><strong>M（Monetary）</strong>：客户在最近一段时间内交易的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。</p><p><img src="http://wangpengcufe.com/python1-2.png" alt="客户价值"></p><p><strong>R打分：</strong>基于最近一次交易日期计算的得分，距离当前日期越近，得分越高。例如5分制。</p><p><strong>F打分：</strong>基于交易频率计算的得分，交易频率越高，得分越高。如5分制。</p><p><strong>M打分：</strong>基于交易金额计算的得分，交易金额越高，得分越高。如5分制。</p><p><strong>RFM总分值</strong>：RFM=Rx100+Fx10+Mx1</p><p><strong>RFM分析的主要作用：</strong></p><ul><li><p>识别优质客户。可以指定个性化的沟通和营销服务，为更多的营销决策提供有力支持。</p></li><li><p>能够衡量客户价值和客户利润创收能力。</p></li></ul><h1 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a><strong>三、代码实现</strong></h1><h3 id="3-1、引包"><a href="#3-1、引包" class="headerlink" title="3.1、引包"></a><strong>3.1、引包</strong></h3><p>首先我们引入需要用的包，数据分析常用的numpy包，pandas包，等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import mysql.connector</span><br></pre></td></tr></table></figure><h3 id="3-2、读取数据"><a href="#3-2、读取数据" class="headerlink" title="3.2、读取数据"></a><strong>3.2、读取数据</strong></h3><p>接下来我们开始用pd.read_csv方法读取用户的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+&apos;：读取数据...&apos;)</span><br><span class="line"></span><br><span class="line">config = &#123;</span><br><span class="line">    &apos;host&apos; : &apos;127.0.0.1&apos;,</span><br><span class="line">    &apos;user&apos; : &apos;root&apos;,</span><br><span class="line">    &apos;password&apos; : &apos;test123&apos;,</span><br><span class="line">    &apos;port&apos; : 3306,</span><br><span class="line">    &apos;database&apos; : &apos;user&apos;,</span><br><span class="line">    &apos;charset&apos; : &apos;gb2312&apos;</span><br><span class="line">&#125;</span><br><span class="line">cnn = mysql.connector.connect(**config) # 建立MySQL连接</span><br><span class="line">cursor = cnn.cursor() # 获得游标</span><br><span class="line">sql = &quot;SELECT  phoneNo AS PHONENO,create_date AS ORDERDATE,order_no AS ORDERNO,ROUND(pay_amount/100,2) AS PAYAMOUNT &quot; \</span><br><span class="line">      &quot;FROM user.`event_record_order`&quot; # SQL语句</span><br><span class="line">raw_data = pd.read_sql(sql,cnn,index_col=&apos;PHONENO&apos;)</span><br><span class="line">cursor.close() # 关闭游标</span><br><span class="line">cnn.close() # 关闭连接</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+&apos;：读取数据完毕！&apos;)</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+&apos;：开始建立RFM模型...&apos;)</span><br></pre></td></tr></table></figure><p>介绍一下config 里的参数信息：host是数据库的ip信息，本案例用的是本地数据库，实际部署生产服务器时，改成生产的ip地址即可。user 是数据库的用户名，password是密码，port是数据库的端口号，database是连接的数据库名 （schema），charset是字符集编码。</p><p>购票时间（ORDERDATE)，订单号（ORDERID）是object类型，订单金额（AMOUNTINFO）是浮点类型。index_col指定了数据中用户的唯一性用 USERID来表示。</p><p>time.strftime(‘%Y-%m-%d %H:%M:%S’,time.localtime(time.time())打印了当前的系统时间，用来记录日志信息。</p><h3 id="3-3、数据审查"><a href="#3-3、数据审查" class="headerlink" title="3.3、数据审查"></a><strong>3.3、数据审查</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;Data Overview :&apos;)</span><br><span class="line">print(raw_data.head(4)) #打印原始数据前4条</span><br><span class="line">print(&apos;-&apos; * 30)</span><br><span class="line">print(&apos;Data DESC:&apos;)</span><br><span class="line">print(raw_data.describe())  #打印原始数据基本描述性信息</span><br></pre></td></tr></table></figure><p>我们用raw_data.head(n)来指定取出数据的前几条，’-‘*30是用来输出打印分隔线，下文再出现时不再重复解释，用raw_data.describe()来获得数据的基本描述性信息。输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Data Overview:</span><br><span class="line">                      ORDERDATE               ORDERNO  PAYAMOUNT</span><br><span class="line">PHONENO                                                         </span><br><span class="line">135****0930 2019-10-02 13:37:36  01201910021336227979        7.0</span><br><span class="line">183****1153 2019-09-30 06:22:29  0120190930062149F9AF        4.5</span><br><span class="line">150****6073 2019-10-30 18:21:45  01201910301821065CFD        2.0</span><br><span class="line">173****7295 2019-10-21 15:13:23  01201910211512498153        7.0</span><br><span class="line">------------------------------</span><br><span class="line">Data DESC:</span><br><span class="line">          PAYAMOUNT</span><br><span class="line">count  96323.000000</span><br><span class="line">mean       4.212409</span><br><span class="line">std        3.049499</span><br><span class="line">min        0.000000</span><br><span class="line">25%        2.600000</span><br><span class="line">50%        3.600000</span><br><span class="line">75%        5.000000</span><br><span class="line">max       80.000000</span><br></pre></td></tr></table></figure><p>我们看到结果中的 count表示总共的记录条数，mean表示了均值，std表示标准差，min表示最小值，25%表示下四分位，也叫第一四分位，50%表示中位值，也叫第二四分位，75%表示上四分位，也叫第三四分位。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">na_cols = raw_data.isnull().any(axis=0) #查看每一列是否具有缺失值</span><br><span class="line">print(&apos;NA Cols:&apos;)</span><br><span class="line">print(na_cols)</span><br><span class="line">print(&apos;-&apos; * 30)</span><br><span class="line">na_lines = raw_data.isnull().any(axis=1) #查看每一行是否具有缺失值</span><br><span class="line">print(&apos;NA Records:&apos;)</span><br><span class="line">print(&apos;Total number of NA lines is :&#123;0&#125;&apos;.format(na_lines.sum()))  #查看具有缺失值的行总记录数</span><br><span class="line">print(raw_data[na_lines])  #只查看具有缺失值的行信息</span><br></pre></td></tr></table></figure><p>我们用raw_data.isnull()来判断是否有缺失值，其中参数axis=0表示的是列，axis=1表示的是行，用:{0}’.format()的方式在字符串中传入参数。输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NA Cols:</span><br><span class="line">ORDERDATE    False</span><br><span class="line">ORDERNO      False</span><br><span class="line">PAYAMOUNT    False</span><br><span class="line">dtype: bool</span><br><span class="line">------------------------------</span><br><span class="line">NA Records:</span><br><span class="line">Total number of NA lines is :0</span><br><span class="line">Empty DataFrame</span><br><span class="line">Columns: [ORDERDATE, ORDERNO, PAYAMOUNT]</span><br><span class="line">Index: []</span><br></pre></td></tr></table></figure><p>通过结果可以看到，实际的交易用户数据还是比较完整的，没有缺失数据的情况，可能这批数据被技术人员采集过来已经处理过了，不讨论了。如果数据有缺失的情况怎么办？那就要对缺失的数据进行一个预处理。</p><h3 id="3-4、数据预处理"><a href="#3-4、数据预处理" class="headerlink" title="3.4、数据预处理"></a><strong>3.4、数据预处理</strong></h3><p>数据预处理，包括数据异常，格式转换，单位转化（如果有单位不统一的情况）等。</p><p>我们先来看<strong>异常值处理：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales_data = raw_data.dropna() #丢弃带有缺失值的行记录</span><br><span class="line">sales_data = sales_data[sales_data[&apos;PAYAMOUNT&apos;] &gt; 1]</span><br></pre></td></tr></table></figure><p>这里，我用代码去除了小于1元的订单，正常出行连1块钱都不用，那应该是测试数据了，现在谁出门做个公交还不得1元起步。对于用户有缺失值的记录进行了丢弃，当然也可以用其他的方法，例如平均值补全法。</p><p>然后看<strong>日期格式转换：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales_data[&apos;ORDERDATE&apos;] = pd.to_datetime(sales_data[&apos;ORDERDATE&apos;])</span><br><span class="line">print(&apos;Raw Dtype:&apos;)</span><br><span class="line">print(sales_data.dtypes)</span><br></pre></td></tr></table></figure><p>用pd.to_datetime()方法对用户的订单日期进行了格式化转换。输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Raw Dtype:</span><br><span class="line">ORDERDATE    datetime64[ns]</span><br><span class="line">ORDERNO              object</span><br><span class="line">PAYAMOUNT           float64</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><p>最后看<strong>数据转换：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recency_value = sales_data[&apos;ORDERDATE&apos;].groupby(sales_data.index).max()  #计算原始最近一次购买时间</span><br><span class="line">frequency_value = sales_data[&apos;ORDERDATE&apos;].groupby(sales_data.index).count()    #计算原始订单数</span><br><span class="line">monetray_value = sales_data[&apos;PAYAMOUNT&apos;].groupby(sales_data.index).sum()  #计算原始订单总金额</span><br></pre></td></tr></table></figure><p>这里根据订单日期的聚合运算得到了用户的最近一次购买时间，用户总的购买数，和购买金额，max()得到了购买时间，count()得到了购买数量，sum()得到了购买金额。</p><h3 id="3-5、计算RFM得分"><a href="#3-5、计算RFM得分" class="headerlink" title="3.5、计算RFM得分"></a><strong>3.5、计算RFM得分</strong></h3><p>得到了最近的购买时间，购买数，和购买金额，下面就可以开始计算RFM得分了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deadline_date = pd.datetime(2019,11,15)</span><br><span class="line">r_interval = (deadline_date - recency_value).dt.days</span><br><span class="line">r_score = pd.cut(r_interval,5,labels=[5,4,3,2,1])</span><br><span class="line">f_score = pd.cut(frequency_value,5,labels=[1,2,3,4,5])</span><br><span class="line">m_score = pd.cut(monetray_value,5,labels=[1,2,3,4,5])</span><br></pre></td></tr></table></figure><p>我们又把客户分成五等分，这个五等分分析相当于是一个“忠诚度的阶梯”(loyalty ladder)，如购买一次的客户为新客户，购买两次的客户为潜力客户，购买三次的客户为老客户，购买四次的客户为成熟客户，购买五次及以上则为忠实客户。其诀窍在于让消费者一直顺着阶梯往上爬，把销售想象成是要将两次购买的顾客往上推成三次购买的顾客，把一次购买者变成两次的。</p><p>我们用deadline_date来表示分析的截止日期，那么统计用户的时间范围就是从数据中最早开始的购买时间到deadline_date。</p><p>用pandas.series.dt.days可以对操作后的datatime直接进行取数。pandas.cut用来把一组数据分割成离散的区间。</p><p>简单介绍一下pandas.cut的用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates=&apos;raise&apos;)</span><br></pre></td></tr></table></figure><ul><li><strong>x</strong>：被切分的类数组（array-like）数据，必须是1维的（不能用DataFrame）；</li><li><strong>bins</strong>：bins是被切割后的区间（或者叫“桶”、“箱”、“面元”），有3中形式：一个int型的标量、标量序列（数组）或者pandas.IntervalIndex 。<ul><li><strong>一个int型的标量</strong>，当bins为一个int型的标量时，代表将x平分成bins份。x的范围在每侧扩展0.1%，以包括x的最大值和最小值。</li><li>标量序列，标量序列定义了被分割后每一个bin的区间边缘，此时x没有扩展。</li><li>pandas.IntervalIndex，定义要使用的精确区间。</li></ul></li><li>right：bool型参数，默认为True，表示是否包含区间右部。比如如果bins=[1,2,3]，right=True，则区间为(1,2]，(2,3]；right=False，则区间为(1,2),(2,3)。</li><li><strong>labels</strong>：给分割后的bins打标签，比如把年龄x分割成年龄段bins后，可以给年龄段打上诸如青年、中年的标签。labels的长度必须和划分后的区间长度相等，比如bins=[1,2,3]，划分后有2个区间(1,2]，(2,3]，则labels的长度必须为2。如果指定labels=False，则返回x中的数据在第几个bin中（从0开始）。</li><li>retbins：bool型的参数，表示是否将分割后的bins返回，当bins为一个int型的标量时比较有用，这样可以得到划分后的区间，默认为False。</li><li>precision：保留区间小数点的位数，默认为3.</li><li>include_lowest：bool型的参数，表示区间的左边是开还是闭的，默认为false，也就是不包含区间左部（闭）。</li><li>duplicates：是否允许重复区间。有两种选择：raise：不允许，drop：允许。</li></ul><p>重点理解我标粗的几个参数，其他参数有需要用到时查阅。</p><p><strong>RFM数据合并</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rfm_list = [r_score,f_score,m_score]  #将r、f、m三个维度组成列表</span><br><span class="line">rfm_cols = [&apos;r_score&apos;,&apos;f_score&apos;,&apos;m_score&apos;] #设置r、f、m 三个维度列名</span><br><span class="line">rfm_pd = pd.DataFrame(np.array(rfm_list).transpose(),dtype=np.int32,columns=rfm_cols,index=frequency_value.index) #建立r、f、m数据框</span><br></pre></td></tr></table></figure><p>我们把RFM的数据进行了合并，首先是将r、f、m三个维度组成一个列表，然后取了三个列名，把数据，列名组装成一个数据框DataFrame.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;RFM Score Overview:&apos;)</span><br><span class="line">print(rfm_pd.head(4))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RFM Score Overview:</span><br><span class="line">             r_score  f_score  m_score</span><br><span class="line">PHONENO                               </span><br><span class="line">13001055088        4        1        1</span><br><span class="line">13001061903        4        1        1</span><br><span class="line">13001066446        5        1        1</span><br><span class="line">13001123218        4        1        1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rfm_pd[&apos;rfm_wscore&apos;] = rfm_pd[&apos;r_score&apos;] * 0.6 + rfm_pd[&apos;f_score&apos;] * 0.3 + rfm_pd[&apos;m_score&apos;] * 0.1</span><br><span class="line">rfm_pd_tmp = rfm_pd.copy()</span><br><span class="line">rfm_pd_tmp[&apos;r_score&apos;] = rfm_pd_tmp[&apos;r_score&apos;].astype(&apos;str&apos;)</span><br><span class="line">rfm_pd_tmp[&apos;f_score&apos;] = rfm_pd_tmp[&apos;f_score&apos;].astype(&apos;str&apos;)</span><br><span class="line">rfm_pd_tmp[&apos;m_score&apos;] = rfm_pd_tmp[&apos;m_score&apos;].astype(&apos;str&apos;)</span><br><span class="line">rfm_pd[&apos;rfm_comb&apos;] = rfm_pd_tmp[&apos;r_score&apos;].str.cat(rfm_pd_tmp[&apos;f_score&apos;]).str.cat(rfm_pd_tmp[&apos;m_score&apos;])</span><br></pre></td></tr></table></figure><p>理论上，上一次消费时间越近的顾客应该是比较好的顾客，对提供即时的商品或是服务也最有可能会有反应。营销人员若想业绩有所成长，只能靠偷取竞争对手的市场占有率，而如果要密切地注意消费者的购买行为，那么最近的一次消费就是营销人员第一个要利用的工具。历史显示，如果我们能让消费者购买，他们就会持续购买。这也就是为什么，0至3个月的顾客收到营销人员的沟通信息多于3至6个月的顾客。</p><p>这里，对RFM进行了加权打分，R占60%，F占30%，M占10%，当然也可以根据业务的实际情况进行相应的权重调整。综合打分是根据RFM=R<em>100+F</em>10+M*1。</p><h3 id="3-6、保存结果"><a href="#3-6、保存结果" class="headerlink" title="3.6、保存结果"></a><strong>3.6、保存结果</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;Final RFM Score Overview:&apos;)</span><br><span class="line">print(rfm_pd.head(4))</span><br><span class="line">print(&apos;-&apos;*30)</span><br><span class="line">print(&apos;Final RFM Score DESC:&apos;)</span><br><span class="line">print(rfm_pd.describe())</span><br><span class="line"></span><br><span class="line">rfm_pd.to_csv(&apos;sales_rfm_score.csv&apos;)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Final RFM Score Overview:</span><br><span class="line">             r_score  f_score  m_score  rfm_wscore rfm_comb</span><br><span class="line">PHONENO                                                    </span><br><span class="line">13001055088        4        1        1         2.8      411</span><br><span class="line">13001061903        4        1        1         2.8      411</span><br><span class="line">13001066446        5        1        1         3.4      511</span><br><span class="line">13001123218        4        1        1         2.8      411</span><br><span class="line">------------------------------</span><br><span class="line">Final RFM Score DESC:</span><br><span class="line">            r_score       f_score       m_score    rfm_wscore</span><br><span class="line">count  53064.000000  53064.000000  53064.000000  53064.000000</span><br><span class="line">mean       3.732172      1.006407      1.002148      2.641441</span><br><span class="line">std        0.944452      0.113022      0.055212      0.570417</span><br><span class="line">min        1.000000      1.000000      1.000000      1.000000</span><br><span class="line">25%        3.000000      1.000000      1.000000      2.200000</span><br><span class="line">50%        4.000000      1.000000      1.000000      2.800000</span><br><span class="line">75%        5.000000      1.000000      1.000000      3.400000</span><br></pre></td></tr></table></figure><h3 id="3-7、写入数据库"><a href="#3-7、写入数据库" class="headerlink" title="3.7、写入数据库"></a><strong>3.7、写入数据库</strong></h3><p><strong>建立数据库连接</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">table_name = &apos;sale_rfm_score&apos;</span><br><span class="line">#数据框基本信息</span><br><span class="line">config = &#123;</span><br><span class="line">    &apos;host&apos; : &apos;172.0.0.1&apos;,</span><br><span class="line">    &apos;user&apos; : &apos;root&apos;,</span><br><span class="line">    &apos;password&apos; : &apos;test123&apos;,</span><br><span class="line">    &apos;port&apos; : 3306,</span><br><span class="line">    &apos;database&apos; : &apos;skpda&apos;,</span><br><span class="line">    &apos;charset&apos; : &apos;gb2312&apos;</span><br><span class="line">&#125;</span><br><span class="line">con = mysql.connector.connect(**config)</span><br><span class="line">cursor = con.cursor()</span><br><span class="line"></span><br><span class="line">cursor.execute(&quot;show tables&quot;)  #</span><br><span class="line">table_object = cursor.fetchall()  # 通过fetchall方法获得所有数据</span><br><span class="line">table_list = []  # 创建库列表</span><br><span class="line">for t in table_object:  # 循环读出所有库</span><br><span class="line">    table_list.append(t[0])  # 每个每个库追加到列表</span><br><span class="line">if not table_name in table_list:  # 如果目标表没有创建</span><br><span class="line">    cursor.execute(&apos;&apos;&apos;</span><br><span class="line">    CREATE TABLE %s (</span><br><span class="line">    phone_no               VARCHAR(20),</span><br><span class="line">    r_score               int(2),</span><br><span class="line">    f_score              int(2),</span><br><span class="line">    m_score              int(2),</span><br><span class="line">    rfm_wscore              DECIMAL(10,2),</span><br><span class="line">    rfm_comb              VARCHAR(10),</span><br><span class="line">    create_date              VARCHAR(20)</span><br><span class="line">    )ENGINE=InnoDB DEFAULT CHARSET=gb2312</span><br><span class="line">    &apos;&apos;&apos; % table_name)  # 创建新表</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+ &apos;：开始清除 table &#123;0&#125;的历史数据...&apos;.format(table_name)) # 输出开始清历史数据的提示信息</span><br><span class="line">delete_sql = &apos;truncate table &#123;0&#125;&apos;.format(table_name)</span><br><span class="line">cursor.execute(delete_sql)</span><br><span class="line">print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))+ &apos;：清除 table &#123;0&#125;的历史数据完毕！&apos;.format(table_name)) # 输出清除历史数据完毕的提示信息</span><br></pre></td></tr></table></figure><p>连接的参数不再介绍，上文已经介绍过。通过fetchall方法获得所有数据,读出所有的表，如果没有表则创建。用cursor.execute先执行truncate语句，把表中的信息先清除，然后重新写入数据。</p><p><strong>将数据写入数据库</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">phone_no = rfm_pd.index # 索引列</span><br><span class="line">rfm_wscore = rfm_pd[&apos;rfm_wscore&apos;]  #RFM 加权得分列</span><br><span class="line">rfm_comb = rfm_pd[&apos;rfm_comb&apos;]  #RFM组合得分列</span><br><span class="line">timestamp = time.strftime(&apos;%Y-%m-%d&apos;,time.localtime(time.time())) # 写库日期</span><br><span class="line">print(&apos;开始写入数据库表 &#123;0&#125;&apos;.format(table_name)) # 输出开始写库的提示信息</span><br><span class="line">for i in range(rfm_pd.shape[0]):</span><br><span class="line">    insert_sql = &quot;INSERT INTO `%s` VALUES (&apos;%s&apos;,%s,%s,%s,%s,%s,&apos;%s&apos;)&quot; % \</span><br><span class="line">                 (table_name, phone_no[i], r_score.iloc[i], f_score.iloc[i], m_score.iloc[i], rfm_wscore.iloc[i],</span><br><span class="line">                  rfm_comb.iloc[i], timestamp)  # 写库SQL依据</span><br><span class="line">    cursor.execute(insert_sql)</span><br><span class="line">    con.commit()</span><br><span class="line">cursor.close()</span><br><span class="line">con.close()</span><br><span class="line">print(&apos;写入数据库结束，总记录条数为: %d&apos; %(i+1))</span><br></pre></td></tr></table></figure><p>先从数据集合 rfm_pd （rfm_pd 是一个DataFrame）中获取到rfm的每个字段， ’….{0}’.format(table_name)表示的是在字符串中拼接参数，{0}代表一个字符串占位符。</p><h1 id="四、案例结果分析"><a href="#四、案例结果分析" class="headerlink" title="四、案例结果分析"></a><strong>四、案例结果分析</strong></h1><p>根据RFM模型的建立，我们在数据库里生成了数据。</p><p><img src="http://wangpengcufe.com/python1-3.png" alt="数据库表生成"></p><p>然后前段工程师根据数据库里的数据得到了用户RFM的价值打分页面，如图（后台展示页面）。</p><p>运营人员根据页面的打分情况来衡量客户价值和客户创利能力，了解客户差异。将客户分别按照R、F、M参数分组后，假设某个客户同时属于R5、F4、M3三个组，则可以得到该客户的RFM代码543。同理，我们可以推测，有一些客户刚刚成功交易、且交易频率高、总采购金额大，其RFM代码是555，还有一些客户的RFM代码是554、545……每一个RFM代码都对应着一小组客户，开展市场营销活动的时候可以从中挑选出若干组进行。<br><img src="http://wangpengcufe.com/python1-4.png" alt="后台展示页面"></p><p>用户是根据RFM的打分倒序排列，可以直接找到重点客户的信息，点开手机号，查看客户的详细信息（这一步由前端开发人员实现），针对重点客户展开各种个性化营销。</p><p><img src="http://wangpengcufe.com/python1-5.png" alt="重点客户详细信息"></p><p>RFM三个指标每个维度再细分出5份，这样就能够细分出5x5x5=125类用户，再根据每类用户精准营销……显然125类用户已超出普通人脑的计算范畴了，更别说针对125类用户量体定制营销策略。实际运用上，我们只需要把每个维度做一次两分即可，这样在3个维度上我们依然得到了8组用户。</p><p>这样，就可以得到以下解读（编号次序RFM,1代表高，0代表低）<br>重要价值客户（111）：最近消费时间近、消费频次和消费金额都很高，必须是VIP啊！<br>重要保持客户（011）：最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠诚客户，我们需要主动和他保持联系。<br>重要发展客户（101）：最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展。<br>重要挽留客户（001）：最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。</p><p><strong>案例结论：</strong></p><ul><li>表现处于一般水平以上的用户的比例太小，低于1%（R、F、M三个维度得分均在3以上的用户数），VIP客户太少。</li><li>会员中99%以上的客户消费状态都不容乐观，主要体现在消费频率低R、消费总金额低M。这可能跟公司的地铁出行的业务有关系，公司的业务分布在全国中小城市，大部分用户都是使用一次的用户。</li><li>低价值客户有262个，占总比例的 0.4%，运营人员可以导出下载这批用户。</li></ul><p>下一节，讲一下在linux服务器上部署python应用。</p></div><footer><div class="clearfix"></div></footer></div></article><article id="post-ml-ml15" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-20T15:28:56.000Z"><a href="/machinelearning/ml-ml15/">2019-11-20</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml15/">机器学习（十五） 特征选择-卡方选择器</a></h1></header><div class="e-content entry" itemprop="articleBody"><p><img src="http://wangpengcufe.com/ml15.png" alt="卡方选择器"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml15/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml14" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-19T14:35:05.000Z"><a href="/machinelearning/ml-ml14/">2019-11-19</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml14/">机器学习（十四） 特征抽取–Word2Vec</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml14.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml14/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><article id="post-ml-ml13" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting"><div class="post-content"><header><div class="icon"></div><time class="dt-published" datetime="2019-11-17T14:35:05.000Z"><a href="/machinelearning/ml-ml13/">2019-11-17</a><span id="busuanzi_container_page_pv"> &nbsp;&nbsp;&nbsp;&nbsp;阅读 <span id="busuanzi_value_page_pv"></span></span></time><h1 class="title"><a href="/machinelearning/ml-ml13/">机器学习（十三） 特征抽取–CountVectorizer</a></h1></header><div class="e-content entry" itemprop="articleBody"><p>标签： machine learning</p><p><img src="http://wangpengcufe.com/ml13-1.png" alt="目录"></p></div><footer><div class="alignleft"><a href="/machinelearning/ml-ml13/#more" class="more-link">阅读全文</a></div><div class="clearfix"></div></footer></div></article><nav id="pagination"><a href="/page/2/" class="alignright next">下一页</a><div class="clearfix"></div></nav></div></div><aside id="sidebar" class="alignright"><script language="javascript">function search(e){return e.method="get",e.action="http://www.baidu.com/baidu",document.search_form.word.value=document.search_form.word.value,!0}</script><div class="search"><form name="search_form" target="_blank" onsubmit="search(this)"><input type="search" name="word" results="0" placeholder="百度站内搜索" onblur='this.value=""'></form></div><div class="widget tag"><h3 class="title">分类</h3><ul class="entry"><li><a href="/categories/read/">read</a><small>69</small></li><li><a href="/categories/tools/">工具</a><small>2</small></li><li><a href="/categories/machinelearning/">机器学习</a><small>23</small></li><li><a href="/categories/navigate/">菜单导航</a><small>1</small></li><li><a href="/categories/datadownload/">资料下载</a><small>1</small></li></ul></div><div class="widget tag"><h3 class="title">标签</h3><ul class="entry"><li><a href="/tags/content/">content</a><small>93</small></li><li><a href="/tags/library/">library</a><small>1</small></li><li><a href="/tags/machine-learning/">machine learning</a><small>23</small></li><li><a href="/tags/navigate/">navigate</a><small>1</small></li><li><a href="/tags/tools/">tools</a><small>2</small></li></ul></div><div class="widget tag"><h3 class="title">友情链接</h3><ul class="entry"><li><a href="http://blog.didispace.com" title="程序员DD">程序员DD</a></li><li><a href="https://mangoroom.cn" title="芒果的个人博客">芒果的个人博客</a></li><li><a href="http://www.baimin.com" target="_blank">百鸣网站百科</a></li><li><a href="http://blog.sina.com.cn/u/2435344920" target="_blank">默默读书</a></li><li><a href="https://www.jianshu.com/u/510007ddad06" target="_blank">王小鹏的随笔（简书）</a></li><li><a href="https://me.csdn.net/weixin_42438712" target="_blank">机器学习（csdn博客）</a></li><li><a href="https://zhuanlan.zhihu.com/c_1182309165824901120" target="_blank">机器学习（知乎）</a></li><li><a href="http://meixiaohan.com/" target="_blank">小寒大人的blog</a></li><li><a href="https://baippt.com/" target="_blank">ppt模板免费下载</a></li><li><a href="http://www.youneedcn.com/" target="_blank">你要的资源</a></li></ul></div></aside><div class="clearfix"></div></div><footer id="footer" class="inner"><div class="alignleft">&copy; 2020 王小鹏 京ICP备19037345号-1</div><div class="clearfix"></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></footer><script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>